{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41782613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOL CALCULATION FOR PRE-DMEK PATIENTS - SETUP AND DATA LOADING\n",
    "# ================================================================\n",
    "# PURPOSE: Set up the analysis environment and load patient data\n",
    "# This notebook optimizes IOL power calculations for Fuchs' dystrophy patients\n",
    "# undergoing combined phacoemulsification and DMEK surgery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants for clinical accuracy thresholds (diopters)\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 1.00]\n",
    "TEST_SIZE = 0.2      # 20% holdout for final testing\n",
    "N_FOLDS = 10         # 10-fold cross-validation\n",
    "RANDOM_STATE = 42    # For reproducibility\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IOL CALCULATION FOR PRE-DMEK PATIENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Loading data from Fuchs' dystrophy patients\")\n",
    "print(\"‚Ä¢ These patients had combined cataract + DMEK surgery\")\n",
    "print(\"‚Ä¢ Goal: Improve IOL power calculation accuracy\")\n",
    "print(\"‚Ä¢ Challenge: Edematous corneas distort standard formulas\")\n",
    "\n",
    "# Load the patient data\n",
    "df = pd.read_excel('FacoDMEK.xlsx')\n",
    "print(f\"\\n‚úÖ Loaded {len(df)} patients from FacoDMEK.xlsx\")\n",
    "\n",
    "print(\"\\nüîç KEY MEASUREMENTS IN OUR DATA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Bio-AL: Axial length (mm)\")\n",
    "print(\"‚Ä¢ Bio-Ks/Kf: Steep and flat keratometry (D)\")\n",
    "print(\"‚Ä¢ CCT: Central corneal thickness (Œºm) - KEY for edema\")\n",
    "print(\"‚Ä¢ IOL Power: Implanted lens power (D)\")\n",
    "print(\"‚Ä¢ PostOP Spherical Equivalent: Actual outcome (D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD SRK/T2 FORMULA IMPLEMENTATION\n",
    "# ========================================\n",
    "# PURPOSE: Implement the baseline SRK/T2 formula (Sheard et al. 2010)\n",
    "# This is the current gold standard for IOL calculations\n",
    "# We'll use this as our baseline to compare improvements against\n",
    "\n",
    "def calculate_SRKT2(AL, K_avg, IOL_power, A_constant, nc=1.333, k_index=1.3375):\n",
    "    \"\"\"\n",
    "    SRK/T2 Formula (Sheard et al. 2010)\n",
    "    - Assumes NORMAL corneas (nc=1.333, k_index=1.3375)\n",
    "    - These assumptions fail in edematous Fuchs' corneas\n",
    "    \n",
    "    Parameters:\n",
    "    - AL: Axial length (mm)\n",
    "    - K_avg: Average keratometry (D)\n",
    "    - IOL_power: IOL power (D)\n",
    "    - A_constant: Lens-specific constant\n",
    "    - nc: Corneal refractive index (we'll optimize this!)\n",
    "    - k_index: Keratometric index (we'll optimize this too!)\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    na = 1.336  # Aqueous/vitreous refractive index\n",
    "    V = 12      # Vertex distance (mm)\n",
    "    ncm1 = nc - 1\n",
    "    \n",
    "    # Convert keratometry to radius using keratometric index\n",
    "    # This is where edema causes problems - k_index assumes normal cornea!\n",
    "    r = (k_index - 1) * 1000 / K_avg\n",
    "    \n",
    "    # Axial length correction for long eyes\n",
    "    if AL <= 24.2:\n",
    "        LCOR = AL\n",
    "    else:\n",
    "        LCOR = 3.446 + 1.716 * AL - 0.0237 * AL * AL\n",
    "    \n",
    "    # H2 calculation (corneal height) - Sheard's modification\n",
    "    H2 = -10.326 + 0.32630 * LCOR + 0.13533 * K_avg\n",
    "    \n",
    "    # ACD (Anterior Chamber Depth) estimation\n",
    "    # Edema can affect this too!\n",
    "    ACD_const = 0.62467 * A_constant - 68.747\n",
    "    offset = ACD_const - 3.336\n",
    "    ACD_est = H2 + offset\n",
    "    \n",
    "    # Retinal thickness correction\n",
    "    RETHICK = 0.65696 - 0.02029 * AL\n",
    "    LOPT = AL + RETHICK  # Optical axial length\n",
    "    \n",
    "    # SRK/T2 refraction calculation - the complex optics formula\n",
    "    numerator = (1000 * na * (na * r - ncm1 * LOPT) - \n",
    "                 IOL_power * (LOPT - ACD_est) * (na * r - ncm1 * ACD_est))\n",
    "    \n",
    "    denominator = (na * (V * (na * r - ncm1 * LOPT) + LOPT * r) - \n",
    "                   0.001 * IOL_power * (LOPT - ACD_est) * \n",
    "                   (V * (na * r - ncm1 * ACD_est) + ACD_est * r))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SRK/T2 FORMULA (Sheard et al. 2010)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"‚Ä¢ SKR/T2 assumes normal corneal properties\")\n",
    "print(\"‚Ä¢ In Fuchs' dystrophy, the cornea is NOT normal:\")\n",
    "print(\"  - Edema changes refractive index (nc)\")\n",
    "print(\"  - Swelling alters keratometric index (k_index)\")\n",
    "print(\"  - Anterior chamber depth is affected\")\n",
    "print(\"\\nOur strategy: Keep the formula structure, optimize the parameters!\")\n",
    "\n",
    "print(\"\\nüìê THE SRK/T2 FORMULA:\")\n",
    "print()\n",
    "print(\"         1000¬∑n‚Çê¬∑(n‚Çê¬∑r - nc‚Çã‚ÇÅ¬∑Lopt) - P¬∑(Lopt - ACDest)¬∑(n‚Çê¬∑r - nc‚Çã‚ÇÅ¬∑ACDest)\")\n",
    "print(\"REF = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"       n‚Çê¬∑(V¬∑(n‚Çê¬∑r - nc‚Çã‚ÇÅ¬∑Lopt) + Lopt¬∑r) - 0.001¬∑P¬∑(Lopt - ACDest)¬∑(V¬∑(n‚Çê¬∑r - nc‚Çã‚ÇÅ¬∑ACDest) + ACDest¬∑r)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db415cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE PERFORMANCE EVALUATION\n",
    "# =================================\n",
    "# PURPOSE: Calculate how well standard SRK/T2 performs on our Fuchs' patients\n",
    "# This establishes the baseline that we need to beat\n",
    "# Spoiler: It won't be great due to the edematous corneas!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE SRK/T2 PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìã WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Calculate average K from steep and flat readings\")\n",
    "print(\"2. Apply standard SRK/T2 to all 96 patients\")\n",
    "print(\"3. Compare predictions to actual outcomes\")\n",
    "print(\"4. Measure error to establish baseline performance\")\n",
    "\n",
    "# Calculate average K (needed for SRK/T2)\n",
    "df['K_avg'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "\n",
    "# Apply standard SRK/T2 formula to all patients\n",
    "df['SRKT2_Prediction'] = df.apply(\n",
    "    lambda row: calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant']\n",
    "        # Note: Using DEFAULT nc=1.333 and k_index=1.3375\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Calculate prediction errors\n",
    "df['Prediction_Error'] = df['PostOP Spherical Equivalent'] - df['SRKT2_Prediction']\n",
    "df['Absolute_Error'] = abs(df['Prediction_Error'])\n",
    "\n",
    "# Calculate key metrics\n",
    "mae = df['Absolute_Error'].mean()\n",
    "me = df['Prediction_Error'].mean()\n",
    "std = df['Prediction_Error'].std()\n",
    "median_ae = df['Absolute_Error'].median()\n",
    "\n",
    "print(\"\\nüìä BASELINE PERFORMANCE METRICS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean Absolute Error (MAE):     {mae:.4f} D\")\n",
    "print(f\"  Mean Error (ME):                {me:+.4f} D\")\n",
    "print(f\"  Standard Deviation (SD):        {std:.4f} D\")\n",
    "print(f\"  Median Absolute Error:          {median_ae:.4f} D\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "print(\"-\" * 50)\n",
    "if mae > 1.0:\n",
    "    print(f\"‚Ä¢ MAE of {mae:.2f} D is POOR (>1.0 D is clinically unacceptable)\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ MAE of {mae:.2f} D is moderate\")\n",
    "    \n",
    "if abs(me) > 0.25:\n",
    "    print(f\"‚Ä¢ Mean error of {me:+.2f} D shows systematic bias\")\n",
    "    if me < 0:\n",
    "        print(\"  ‚Üí Formula tends to predict too myopic (negative)\")\n",
    "    else:\n",
    "        print(\"  ‚Üí Formula tends to predict too hyperopic (positive)\")\n",
    "\n",
    "# Calculate clinical accuracy rates\n",
    "within_025 = (df['Absolute_Error'] <= 0.25).sum() / len(df) * 100\n",
    "within_050 = (df['Absolute_Error'] <= 0.50).sum() / len(df) * 100\n",
    "within_075 = (df['Absolute_Error'] <= 0.75).sum() / len(df) * 100\n",
    "within_100 = (df['Absolute_Error'] <= 1.00).sum() / len(df) * 100\n",
    "\n",
    "print(\"\\nüìà CLINICAL ACCURACY:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Within ¬±0.25 D:  {within_025:.1f}% of eyes\")\n",
    "print(f\"  Within ¬±0.50 D:  {within_050:.1f}% of eyes\")\n",
    "print(f\"  Within ¬±0.75 D:  {within_075:.1f}% of eyes\")\n",
    "print(f\"  Within ¬±1.00 D:  {within_100:.1f}% of eyes\")\n",
    "\n",
    "print(\"\\nüéØ CLINICAL TARGETS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Modern standard: >70% within ¬±0.50 D\")\n",
    "print(\"‚Ä¢ Acceptable: >90% within ¬±1.00 D\")\n",
    "print(f\"‚Ä¢ Our baseline: {within_050:.1f}% within ¬±0.50 D\")\n",
    "print(\"\\n‚ö†Ô∏è Standard SRK/T2 clearly struggles with Fuchs' dystrophy!\")\n",
    "print(\"This is why we need optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ridge_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION ANALYSIS - IDENTIFYING IMPORTANT FEATURES\n",
    "# ===========================================================\n",
    "# PURPOSE: Use machine learning to identify which features matter most\n",
    "# This will guide our optimization strategy\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RIDGE REGRESSION FEATURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîç WHY START WITH RIDGE?\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Ridge regression identifies important features\")\n",
    "print(\"‚Ä¢ Helps us understand what drives prediction errors\")\n",
    "print(\"‚Ä¢ Guides our formula optimization strategy\")\n",
    "print(\"‚Ä¢ If CCT features are important, our hypothesis is correct!\")\n",
    "\n",
    "# Create feature matrix with interactions\n",
    "print(\"\\nüìä CREATING FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "features = []\n",
    "feature_names = []\n",
    "\n",
    "# Basic features\n",
    "for col in ['Bio-AL', 'Bio-Ks', 'Bio-Kf', 'IOL Power', 'CCT']:\n",
    "    features.append(df[col].values)\n",
    "    feature_names.append(col)\n",
    "\n",
    "# Add K_avg\n",
    "features.append(df['K_avg'].values)\n",
    "feature_names.append('K_avg')\n",
    "\n",
    "# CCT-derived features\n",
    "df['CCT_squared'] = df['CCT'] ** 2\n",
    "df['CCT_deviation'] = df['CCT'] - 550\n",
    "df['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_squared'].values,\n",
    "    df['CCT_deviation'].values,\n",
    "    df['CCT_norm'].values\n",
    "])\n",
    "feature_names.extend(['CCT_squared', 'CCT_deviation', 'CCT_norm'])\n",
    "\n",
    "# Interaction terms\n",
    "df['CCT_x_AL'] = df['CCT'] * df['Bio-AL']\n",
    "df['CCT_x_K'] = df['CCT'] * df['K_avg']\n",
    "df['CCT_ratio_AL'] = df['CCT'] / df['Bio-AL']\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_x_AL'].values,\n",
    "    df['CCT_x_K'].values,\n",
    "    df['CCT_ratio_AL'].values\n",
    "])\n",
    "feature_names.extend(['CCT_x_AL', 'CCT_x_K', 'CCT_ratio_AL'])\n",
    "\n",
    "X = np.column_stack(features)\n",
    "y = df['PostOP Spherical Equivalent'].values\n",
    "\n",
    "print(f\"Created {len(feature_names)} features including CCT interactions\")\n",
    "\n",
    "# Standardize and train Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Ridge to get feature importance\n",
    "ridge_analysis = Ridge(alpha=1.0)\n",
    "ridge_analysis.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importance from coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': ridge_analysis.coef_,\n",
    "    'Abs_Coefficient': np.abs(ridge_analysis.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['Feature']:20} Coef={row['Coefficient']:+.4f}\")\n",
    "\n",
    "# Analyze CCT importance\n",
    "cct_features = feature_importance[feature_importance['Feature'].str.contains('CCT')]\n",
    "cct_importance = cct_features['Abs_Coefficient'].sum()\n",
    "total_importance = feature_importance['Abs_Coefficient'].sum()\n",
    "cct_percentage = (cct_importance / total_importance) * 100\n",
    "\n",
    "print(\"\\nüí° KEY FINDINGS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚Ä¢ CCT-related features account for {cct_percentage:.1f}% of total importance\")\n",
    "print(f\"‚Ä¢ Top feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "\n",
    "if 'CCT_ratio_AL' in feature_importance.head(3)['Feature'].values:\n",
    "    print(\"‚Ä¢ CCT/AL ratio is among top 3 features!\")\n",
    "    print(\"‚Ä¢ This validates that CCT relative to eye size matters\")\n",
    "\n",
    "if cct_percentage > 50:\n",
    "    print(\"\\n‚úÖ HYPOTHESIS CONFIRMED:\")\n",
    "    print(\"CCT features dominate prediction - our CCT-dependent approach is justified!\")\n",
    "\n",
    "print(\"\\nüéØ OPTIMIZATION STRATEGY BASED ON RIDGE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Make optical parameters CCT-dependent (nc, k_index)\")\n",
    "print(\"2. Consider CCT/AL ratio in corrections\")\n",
    "print(\"3. Account for CCT interactions with other measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rt23gheoiv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER OPTIMIZATION WITH K-FOLD CROSS-VALIDATION\n",
    "# =============================================\n",
    "# PURPOSE: Optimize SRK/T2 parameters with nested CV for robust validation\n",
    "# Outer: 75/25 train/test split | Inner: 5-fold CV on training set\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARAMETER OPTIMIZATION WITH K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ NESTED CROSS-VALIDATION STRATEGY:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Outer: 75% train (72 pts), 25% test (24 pts)\")\n",
    "print(\"‚Ä¢ Inner: 5-fold CV on training set\")\n",
    "print(\"‚Ä¢ Each fold: ~58 train, ~14 validate\")\n",
    "print(\"‚Ä¢ Final: Average params ‚Üí test on holdout\")\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "# OUTER SPLIT: Create train/test split\n",
    "X_train_param, X_test_param = train_test_split(df, test_size=0.25, random_state=42)\n",
    "X_train_param['K_avg'] = (X_train_param['Bio-Ks'] + X_train_param['Bio-Kf']) / 2\n",
    "X_test_param['K_avg'] = (X_test_param['Bio-Ks'] + X_test_param['Bio-Kf']) / 2\n",
    "\n",
    "print(f\"\\nüìä OUTER SPLIT:\")\n",
    "print(f\"  Training set: {len(X_train_param)} patients (for K-fold CV)\")\n",
    "print(f\"  Test set:     {len(X_test_param)} patients (final holdout)\")\n",
    "\n",
    "def calculate_mae_param(params, df_data):\n",
    "    \"\"\"Calculate MAE for parameter optimization\"\"\"\n",
    "    nc_base, nc_cct_coef, k_index_base, k_index_cct_coef, acd_offset_base, acd_offset_cct_coef = params\n",
    "    \n",
    "    predictions = []\n",
    "    for _, row in df_data.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        nc = nc_base + nc_cct_coef * cct_norm\n",
    "        k_index = k_index_base + k_index_cct_coef * cct_norm\n",
    "        acd_offset = acd_offset_base + acd_offset_cct_coef * cct_norm\n",
    "        \n",
    "        pred = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc,\n",
    "            k_index=k_index\n",
    "        )\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    mae = mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    return mae\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INNER K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# INNER CV: 5-fold on training set\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_params = []\n",
    "fold_maes = []\n",
    "\n",
    "bounds_param = [\n",
    "    (1.20, 1.50),    # nc_base\n",
    "    (-0.20, 0.20),   # nc_cct_coef  \n",
    "    (1.20, 1.60),    # k_index_base\n",
    "    (-0.30, 0.30),   # k_index_cct_coef\n",
    "    (-3.0, 3.0),     # acd_offset_base\n",
    "    (-3.0, 3.0),     # acd_offset_cct_coef\n",
    "]\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_param), 1):\n",
    "    print(f\"\\nüìÅ FOLD {fold_num}/5:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Split data for this fold\n",
    "    fold_train = X_train_param.iloc[train_idx]\n",
    "    fold_val = X_train_param.iloc[val_idx]\n",
    "    print(f\"  Train: {len(fold_train)} | Validate: {len(fold_val)}\")\n",
    "    \n",
    "    # Optimize on fold training data\n",
    "    result_fold = differential_evolution(\n",
    "        lambda p: calculate_mae_param(p, fold_train),\n",
    "        bounds_param,\n",
    "        maxiter=30,  # Reduced for speed in CV\n",
    "        seed=42 + fold_num,  # Different seed per fold\n",
    "        workers=1,\n",
    "        updating='deferred',\n",
    "        disp=False\n",
    "    )\n",
    "    \n",
    "    # Store parameters\n",
    "    fold_params.append(result_fold.x)\n",
    "    \n",
    "    # Validate on fold validation set\n",
    "    val_mae = calculate_mae_param(result_fold.x, fold_val)\n",
    "    fold_maes.append(val_mae)\n",
    "    print(f\"  Validation MAE: {val_mae:.4f} D\")\n",
    "    \n",
    "    # Show parameters for this fold\n",
    "    nc_b, nc_c, k_b, k_c, acd_b, acd_c = result_fold.x\n",
    "    print(f\"  Params: nc={nc_b:.3f}¬±{nc_c:.3f}*CCT, k={k_b:.3f}¬±{k_c:.3f}*CCT\")\n",
    "\n",
    "# Calculate average parameters and performance\n",
    "avg_params = np.mean(fold_params, axis=0)\n",
    "std_params = np.std(fold_params, axis=0)\n",
    "avg_mae = np.mean(fold_maes)\n",
    "std_mae = np.std(fold_maes)\n",
    "\n",
    "nc_base_opt, nc_cct_coef_opt, k_index_base_opt, k_index_cct_coef_opt, acd_offset_base_opt, acd_offset_cct_coef_opt = avg_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä CROSS-VALIDATION PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Average CV MAE: {avg_mae:.4f} ¬± {std_mae:.4f} D\")\n",
    "print(f\"  Best fold MAE:  {min(fold_maes):.4f} D\")\n",
    "print(f\"  Worst fold MAE: {max(fold_maes):.4f} D\")\n",
    "\n",
    "print(\"\\n‚úÖ AVERAGED PARAMETERS (from 5 folds):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  nc_base:           {nc_base_opt:.4f} ¬± {std_params[0]:.4f}\")\n",
    "print(f\"  nc_cct_coef:       {nc_cct_coef_opt:+.4f} ¬± {std_params[1]:.4f}\")\n",
    "print(f\"  k_index_base:      {k_index_base_opt:.4f} ¬± {std_params[2]:.4f}\")\n",
    "print(f\"  k_index_cct_coef:  {k_index_cct_coef_opt:+.4f} ¬± {std_params[3]:.4f}\")\n",
    "print(f\"  acd_offset_base:   {acd_offset_base_opt:+.4f} ¬± {std_params[4]:.4f}\")\n",
    "print(f\"  acd_offset_cct_coef: {acd_offset_cct_coef_opt:+.4f} ¬± {std_params[5]:.4f}\")\n",
    "\n",
    "# FINAL RETRAINING: Optionally retrain on full training set\n",
    "print(\"\\nüîß FINAL RETRAINING on full training set...\")\n",
    "result_final = differential_evolution(\n",
    "    lambda p: calculate_mae_param(p, X_train_param),\n",
    "    bounds_param,\n",
    "    maxiter=50,\n",
    "    seed=42,\n",
    "    workers=1,\n",
    "    updating='deferred',\n",
    "    disp=False\n",
    ")\n",
    "nc_base_opt, nc_cct_coef_opt, k_index_base_opt, k_index_cct_coef_opt, acd_offset_base_opt, acd_offset_cct_coef_opt = result_final.x\n",
    "\n",
    "# NOW TEST ON FINAL HOLDOUT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST ON HOLDOUT SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate baseline for test set\n",
    "X_test_param['SRKT2_Baseline'] = X_test_param.apply(\n",
    "    lambda row: calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Apply optimized parameters to test set\n",
    "predictions_param_test = []\n",
    "for _, row in X_test_param.iterrows():\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    nc = nc_base_opt + nc_cct_coef_opt * cct_norm\n",
    "    k_index = k_index_base_opt + k_index_cct_coef_opt * cct_norm\n",
    "    acd_offset = acd_offset_base_opt + acd_offset_cct_coef_opt * cct_norm\n",
    "    \n",
    "    pred = calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant'] + acd_offset,\n",
    "        nc=nc,\n",
    "        k_index=k_index\n",
    "    )\n",
    "    predictions_param_test.append(pred)\n",
    "\n",
    "mae_test_baseline = np.abs(X_test_param['SRKT2_Baseline'] - X_test_param['PostOP Spherical Equivalent']).mean()\n",
    "mae_test_optimized = mean_absolute_error(X_test_param['PostOP Spherical Equivalent'], predictions_param_test)\n",
    "improvement_test = (mae_test_baseline - mae_test_optimized) / mae_test_baseline * 100\n",
    "\n",
    "print(f\"\\nüìä FINAL TEST PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {mae_test_baseline:.4f} D\")\n",
    "print(f\"  Optimized MAE:     {mae_test_optimized:.4f} D\")\n",
    "print(f\"  REAL Improvement:  {improvement_test:.1f}%\")\n",
    "\n",
    "# Store for later comparison\n",
    "mae_param_test = mae_test_optimized\n",
    "\n",
    "print(\"\\nüí° K-FOLD INSIGHTS:\")\n",
    "print(\"-\" * 50)\n",
    "if std_mae < 0.1:\n",
    "    print(f\"‚úÖ Low CV std ({std_mae:.4f}) ‚Üí stable optimization\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è High CV std ({std_mae:.4f}) ‚Üí parameters vary across folds\")\n",
    "\n",
    "if abs(avg_mae - mae_test_optimized) < 0.2:\n",
    "    print(f\"‚úÖ CV estimate ({avg_mae:.4f}) close to test ({mae_test_optimized:.4f})\")\n",
    "    print(\"   Good generalization!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Gap between CV ({avg_mae:.4f}) and test ({mae_test_optimized:.4f})\")\n",
    "    print(\"   Possible overfitting or distribution shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829090ggs0r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPLICATIVE CORRECTION WITH K-FOLD CROSS-VALIDATION\n",
    "# ====================================\n",
    "# PURPOSE: Multiplicative correction with nested CV for robust validation\n",
    "# K-fold helps find stable correction factors across data subsets\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTIPLICATIVE CORRECTION WITH K-FOLD CV\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ NESTED CV STRATEGY:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Outer: 75/25 train/test split\")\n",
    "print(\"‚Ä¢ Inner: 5-fold CV on training\")\n",
    "print(\"‚Ä¢ Find stable multiplicative factors\")\n",
    "print(\"‚Ä¢ Test final model on holdout\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "# OUTER SPLIT\n",
    "X_train_mult, X_test_mult = train_test_split(df, test_size=0.25, random_state=42)\n",
    "X_train_mult['K_avg'] = (X_train_mult['Bio-Ks'] + X_train_mult['Bio-Kf']) / 2\n",
    "X_test_mult['K_avg'] = (X_test_mult['Bio-Ks'] + X_test_mult['Bio-Kf']) / 2\n",
    "\n",
    "print(f\"\\nüìä OUTER SPLIT:\")\n",
    "print(f\"  Training: {len(X_train_mult)} patients (for K-fold)\")\n",
    "print(f\"  Test:     {len(X_test_mult)} patients (holdout)\")\n",
    "\n",
    "# Calculate baseline SRK/T2 for all data\n",
    "for dataset in [X_train_mult, X_test_mult]:\n",
    "    dataset['SRKT2_Prediction'] = dataset.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "def multiplicative_objective(params, df_data):\n",
    "    \"\"\"Objective function for multiplicative correction\"\"\"\n",
    "    m0, m1, m2 = params\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for _, row in df_data.iterrows():\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        correction_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "        corrected_pred = base_pred * correction_factor\n",
    "        \n",
    "        predictions.append(corrected_pred)\n",
    "        actuals.append(row['PostOP Spherical Equivalent'])\n",
    "    \n",
    "    return mean_absolute_error(actuals, predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INNER K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# INNER CV: 5-fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_params = []\n",
    "fold_maes = []\n",
    "\n",
    "x0_mult = [0, 0, 0]\n",
    "bounds_mult = [(-0.5, 0.5), (-0.5, 0.5), (-0.5, 0.5)]\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_mult), 1):\n",
    "    print(f\"\\nüìÅ FOLD {fold_num}/5:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Split for this fold\n",
    "    fold_train = X_train_mult.iloc[train_idx]\n",
    "    fold_val = X_train_mult.iloc[val_idx]\n",
    "    print(f\"  Train: {len(fold_train)} | Validate: {len(fold_val)}\")\n",
    "    \n",
    "    # Optimize on fold training\n",
    "    result_fold = minimize(\n",
    "        lambda p: multiplicative_objective(p, fold_train),\n",
    "        x0_mult,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds_mult\n",
    "    )\n",
    "    \n",
    "    # Store parameters\n",
    "    fold_params.append(result_fold.x)\n",
    "    \n",
    "    # Validate on fold validation\n",
    "    val_mae = multiplicative_objective(result_fold.x, fold_val)\n",
    "    fold_maes.append(val_mae)\n",
    "    \n",
    "    m0_f, m1_f, m2_f = result_fold.x\n",
    "    print(f\"  Validation MAE: {val_mae:.4f} D\")\n",
    "    print(f\"  Params: m‚ÇÄ={m0_f:.4f}, m‚ÇÅ={m1_f:.4f}, m‚ÇÇ={m2_f:.4f}\")\n",
    "\n",
    "# Average across folds\n",
    "avg_params = np.mean(fold_params, axis=0)\n",
    "std_params = np.std(fold_params, axis=0)\n",
    "avg_mae = np.mean(fold_maes)\n",
    "std_mae = np.std(fold_maes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä CROSS-VALIDATION PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Average CV MAE: {avg_mae:.4f} ¬± {std_mae:.4f} D\")\n",
    "print(f\"  Best fold:      {min(fold_maes):.4f} D\")\n",
    "print(f\"  Worst fold:     {max(fold_maes):.4f} D\")\n",
    "print(f\"  Stability:      CV = {avg_mae/std_mae:.1f} (higher=better)\")\n",
    "\n",
    "print(\"\\n‚úÖ AVERAGED PARAMETERS (from 5 folds):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  m‚ÇÄ (constant):     {avg_params[0]:+.4f} ¬± {std_params[0]:.4f}\")\n",
    "print(f\"  m‚ÇÅ (CCT coef):     {avg_params[1]:+.4f} ¬± {std_params[1]:.4f}\")\n",
    "print(f\"  m‚ÇÇ (ratio coef):   {avg_params[2]:+.4f} ¬± {std_params[2]:.4f}\")\n",
    "\n",
    "# FINAL RETRAINING on full training set\n",
    "print(\"\\nüîß FINAL OPTIMIZATION on full training set...\")\n",
    "result_mult = minimize(\n",
    "    lambda p: multiplicative_objective(p, X_train_mult),\n",
    "    x0_mult,\n",
    "    method='L-BFGS-B',\n",
    "    bounds=bounds_mult\n",
    ")\n",
    "m0_opt, m1_opt, m2_opt = result_mult.x\n",
    "\n",
    "print(f\"Final params: m‚ÇÄ={m0_opt:.4f}, m‚ÇÅ={m1_opt:.4f}, m‚ÇÇ={m2_opt:.4f}\")\n",
    "\n",
    "# FINAL TEST ON HOLDOUT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST ON HOLDOUT SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply to test set\n",
    "predictions_mult_test = []\n",
    "for _, row in X_test_mult.iterrows():\n",
    "    base_pred = row['SRKT2_Prediction']\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "    \n",
    "    correction_factor = 1 + m0_opt + m1_opt * cct_norm + m2_opt * cct_ratio\n",
    "    corrected_pred = base_pred * correction_factor\n",
    "    predictions_mult_test.append(corrected_pred)\n",
    "\n",
    "mae_test_baseline_mult = np.abs(X_test_mult['SRKT2_Prediction'] - X_test_mult['PostOP Spherical Equivalent']).mean()\n",
    "mae_test_mult = mean_absolute_error(X_test_mult['PostOP Spherical Equivalent'], predictions_mult_test)\n",
    "improvement_test_mult = (mae_test_baseline_mult - mae_test_mult) / mae_test_baseline_mult * 100\n",
    "\n",
    "print(f\"\\nüìä FINAL TEST PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {mae_test_baseline_mult:.4f} D\")\n",
    "print(f\"  Multiplicative MAE: {mae_test_mult:.4f} D\")\n",
    "print(f\"  REAL Improvement:  {improvement_test_mult:.1f}%\")\n",
    "\n",
    "# Store for comparison\n",
    "mae_mult_test = mae_test_mult\n",
    "\n",
    "print(\"\\nüìê FINAL CORRECTION FORMULA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Corrected_REF = Standard_SRK/T2 √ó Correction_Factor\")\n",
    "print(f\"Correction_Factor = 1 {m0_opt:+.4f} {m1_opt:+.4f}√óCCT_norm {m2_opt:+.4f}√ó(CCT/AL)\")\n",
    "\n",
    "print(\"\\nüí° K-FOLD INSIGHTS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check parameter stability\n",
    "param_cv = np.mean([std_params[i]/abs(avg_params[i]) for i in range(3) if avg_params[i] != 0])\n",
    "if param_cv < 0.2:\n",
    "    print(f\"‚úÖ Parameters stable across folds (CV={param_cv:.2f})\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Parameters vary across folds (CV={param_cv:.2f})\")\n",
    "\n",
    "# Check generalization\n",
    "if abs(avg_mae - mae_test_mult) < 0.15:\n",
    "    print(f\"‚úÖ Good generalization: CV={avg_mae:.3f}, Test={mae_test_mult:.3f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Generalization gap: CV={avg_mae:.3f}, Test={mae_test_mult:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Parameter consistency check:\")\n",
    "for i, param_name in enumerate(['m‚ÇÄ', 'm‚ÇÅ', 'm‚ÇÇ']):\n",
    "    fold_values = [p[i] for p in fold_params]\n",
    "    print(f\"  {param_name}: min={min(fold_values):.4f}, max={max(fold_values):.4f}, range={max(fold_values)-min(fold_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9g3yzsp3n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIVE CORRECTION WITH PROPER VALIDATION\n",
    "# ================================================\n",
    "# PURPOSE: Create an additive correction term with train/test split\n",
    "# Based on Ridge-identified features, validated properly\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADDITIVE CORRECTION FROM RIDGE INSIGHTS - PROPER VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ STRATEGY WITH TRAIN/TEST SPLIT:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Train on 75% of data\")\n",
    "print(\"‚Ä¢ Test on held-out 25%\")\n",
    "print(\"‚Ä¢ Formula: Corrected = SRK/T2 + Correction_Term\")\n",
    "print(\"‚Ä¢ Uses Ridge-identified important features\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train/test split\n",
    "X_train_add, X_test_add = train_test_split(df, test_size=0.25, random_state=42)\n",
    "X_train_add['K_avg'] = (X_train_add['Bio-Ks'] + X_train_add['Bio-Kf']) / 2\n",
    "X_test_add['K_avg'] = (X_test_add['Bio-Ks'] + X_test_add['Bio-Kf']) / 2\n",
    "\n",
    "print(f\"\\nüìä DATA SPLIT:\")\n",
    "print(f\"  Training set: {len(X_train_add)} patients\")\n",
    "print(f\"  Test set:     {len(X_test_add)} patients (never seen)\")\n",
    "\n",
    "# Calculate baseline SRK/T2 for both sets\n",
    "for dataset in [X_train_add, X_test_add]:\n",
    "    dataset['SRKT2_Prediction'] = dataset.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "def additive_objective(params, df_data):\n",
    "    \"\"\"Objective for additive correction using Ridge-identified features\"\"\"\n",
    "    a0, a1, a2, a3 = params\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for _, row in df_data.iterrows():\n",
    "        # Standard SRK/T2 prediction\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        \n",
    "        # Ridge-identified features\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        k_avg = row['K_avg']\n",
    "        \n",
    "        # Additive correction based on Ridge insights\n",
    "        correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * k_avg\n",
    "        corrected_pred = base_pred + correction\n",
    "        \n",
    "        predictions.append(corrected_pred)\n",
    "        actuals.append(row['PostOP Spherical Equivalent'])\n",
    "    \n",
    "    return mean_absolute_error(actuals, predictions)\n",
    "\n",
    "print(\"\\nüîß OPTIMIZING ON TRAINING SET...\")\n",
    "\n",
    "# Initial guess and bounds\n",
    "x0_add = [0, 0, 0, 0]\n",
    "bounds_add = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1)]\n",
    "\n",
    "# Optimize on TRAINING SET ONLY\n",
    "result_add = minimize(\n",
    "    lambda p: additive_objective(p, X_train_add),  # TRAIN ONLY\n",
    "    x0_add,\n",
    "    method='L-BFGS-B',\n",
    "    bounds=bounds_add\n",
    ")\n",
    "\n",
    "a0_opt, a1_opt, a2_opt, a3_opt = result_add.x\n",
    "\n",
    "print(\"\\n‚úÖ OPTIMIZED ADDITIVE PARAMETERS (from training):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  a‚ÇÄ (constant):     {a0_opt:+.4f}\")\n",
    "print(f\"  a‚ÇÅ (CCT_norm):     {a1_opt:+.4f}\")\n",
    "print(f\"  a‚ÇÇ (CCT_ratio):    {a2_opt:+.4f}\")\n",
    "print(f\"  a‚ÇÉ (K_avg):        {a3_opt:+.4f}\")\n",
    "\n",
    "# Evaluate on TRAINING SET\n",
    "predictions_add_train = []\n",
    "for _, row in X_train_add.iterrows():\n",
    "    base_pred = row['SRKT2_Prediction']\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "    k_avg = row['K_avg']\n",
    "    \n",
    "    correction = a0_opt + a1_opt * cct_norm + a2_opt * cct_ratio + a3_opt * k_avg\n",
    "    corrected_pred = base_pred + correction\n",
    "    predictions_add_train.append(corrected_pred)\n",
    "\n",
    "mae_train_baseline_add = np.abs(X_train_add['SRKT2_Prediction'] - X_train_add['PostOP Spherical Equivalent']).mean()\n",
    "mae_train_add = mean_absolute_error(X_train_add['PostOP Spherical Equivalent'], predictions_add_train)\n",
    "\n",
    "print(f\"\\nüìà TRAINING SET PERFORMANCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {mae_train_baseline_add:.4f} D\")\n",
    "print(f\"  Additive MAE:      {mae_train_add:.4f} D\")\n",
    "print(f\"  Improvement:       {(mae_train_baseline_add - mae_train_add) / mae_train_baseline_add * 100:.1f}%\")\n",
    "\n",
    "# NOW TEST ON UNSEEN TEST SET\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING ON HOLDOUT SET (HONEST PERFORMANCE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply to TEST SET\n",
    "predictions_add_test = []\n",
    "for _, row in X_test_add.iterrows():\n",
    "    base_pred = row['SRKT2_Prediction']\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "    k_avg = row['K_avg']\n",
    "    \n",
    "    correction = a0_opt + a1_opt * cct_norm + a2_opt * cct_ratio + a3_opt * k_avg\n",
    "    corrected_pred = base_pred + correction\n",
    "    predictions_add_test.append(corrected_pred)\n",
    "\n",
    "mae_test_baseline_add = np.abs(X_test_add['SRKT2_Prediction'] - X_test_add['PostOP Spherical Equivalent']).mean()\n",
    "mae_test_add = mean_absolute_error(X_test_add['PostOP Spherical Equivalent'], predictions_add_test)\n",
    "improvement_test_add = (mae_test_baseline_add - mae_test_add) / mae_test_baseline_add * 100\n",
    "\n",
    "print(f\"\\nüìä TEST SET PERFORMANCE (REAL):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {mae_test_baseline_add:.4f} D\")\n",
    "print(f\"  Additive MAE:      {mae_test_add:.4f} D\")\n",
    "print(f\"  REAL Improvement:  {improvement_test_add:.1f}%\")\n",
    "\n",
    "# Store for later comparison\n",
    "mae_add_test = mae_test_add\n",
    "\n",
    "print(\"\\nüìê CORRECTION FORMULA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Corrected_REF = Standard_SRK/T2 + Correction_Term\")\n",
    "print(\"\")\n",
    "print(f\"Correction_Term = {a0_opt:+.4f} {a1_opt:+.4f}√óCCT_norm {a2_opt:+.4f}√ó(CCT/AL) {a3_opt:+.4f}√óK_avg\")\n",
    "print(\"\")\n",
    "print(\"Where: CCT_norm = (CCT - 600) / 100\")\n",
    "\n",
    "print(\"\\nüí° RIDGE VALIDATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ This formula uses features identified by Ridge as important\")\n",
    "print(\"‚Ä¢ CCT_norm and CCT_ratio were top Ridge features\")\n",
    "if improvement_test_add > 0:\n",
    "    print(f\"‚Ä¢ Achieving {improvement_test_add:.1f}% improvement confirms Ridge insights work!\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Limited improvement suggests these features may not generalize well\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "print(\"-\" * 50)\n",
    "if improvement_test_add < (mae_train_baseline_add - mae_train_add) / mae_train_baseline_add * 100 - 5:\n",
    "    print(\"‚ö†Ô∏è Performance drop from train to test suggests overfitting\")\n",
    "else:\n",
    "    print(\"‚úÖ Consistent performance - robust additive correction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u4unlmjdt3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED APPROACH WITH K-FOLD CROSS-VALIDATION\n",
    "# ========================================================\n",
    "# PURPOSE: Combine all three methods with nested K-fold CV\n",
    "# Most complex but potentially most accurate approach\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED FORMULA WITH K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ NESTED CV FOR COMBINED APPROACH:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Outer: 75/25 train/test split\")\n",
    "print(\"‚Ä¢ Inner: 5-fold CV for each method\")\n",
    "print(\"‚Ä¢ Combine all optimized corrections\")\n",
    "print(\"‚Ä¢ Final test on 24-patient holdout\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "import numpy as np\n",
    "\n",
    "# OUTER SPLIT - consistent across all methods\n",
    "X_train_comb, X_test_comb = train_test_split(df, test_size=0.25, random_state=42)\n",
    "X_train_comb['K_avg'] = (X_train_comb['Bio-Ks'] + X_train_comb['Bio-Kf']) / 2\n",
    "X_test_comb['K_avg'] = (X_test_comb['Bio-Ks'] + X_test_comb['Bio-Kf']) / 2\n",
    "\n",
    "print(f\"\\nüìä DATA SPLIT:\")\n",
    "print(f\"  Training: {len(X_train_comb)} patients (for K-fold)\")\n",
    "print(f\"  Test:     {len(X_test_comb)} patients (holdout)\")\n",
    "\n",
    "# Calculate baseline for all\n",
    "for dataset in [X_train_comb, X_test_comb]:\n",
    "    dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD CV FOR EACH METHOD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Setup K-fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store fold results for each method\n",
    "param_fold_results = []\n",
    "mult_fold_results = []\n",
    "add_fold_results = []\n",
    "combined_fold_maes = []\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_comb), 1):\n",
    "    print(f\"\\nüìÅ FOLD {fold_num}/5:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fold_train = X_train_comb.iloc[train_idx]\n",
    "    fold_val = X_train_comb.iloc[val_idx]\n",
    "    print(f\"  Train: {len(fold_train)} | Validate: {len(fold_val)}\")\n",
    "    \n",
    "    # 1. PARAMETER METHOD\n",
    "    def param_obj(params, df_data):\n",
    "        nc_base, nc_cct, k_base, k_cct, acd_base, acd_cct = params\n",
    "        predictions = []\n",
    "        for _, row in df_data.iterrows():\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            nc = nc_base + nc_cct * cct_norm\n",
    "            k_index = k_base + k_cct * cct_norm\n",
    "            acd_offset = acd_base + acd_cct * cct_norm\n",
    "            pred = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + acd_offset,\n",
    "                nc=nc, k_index=k_index\n",
    "            )\n",
    "            predictions.append(pred)\n",
    "        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    \n",
    "    bounds_p = [(1.20, 1.50), (-0.20, 0.20), (1.20, 1.60), (-0.30, 0.30), (-3.0, 3.0), (-3.0, 3.0)]\n",
    "    result_p = differential_evolution(lambda p: param_obj(p, fold_train), bounds_p, \n",
    "                                     maxiter=20, seed=42+fold_num, disp=False)\n",
    "    param_fold_results.append(result_p.x)\n",
    "    \n",
    "    # 2. MULTIPLICATIVE METHOD\n",
    "    def mult_obj(params, df_data):\n",
    "        m0, m1, m2 = params\n",
    "        predictions = []\n",
    "        for _, row in df_data.iterrows():\n",
    "            base_pred = row['SRKT2_Baseline']\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            correction = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            predictions.append(base_pred * correction)\n",
    "        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    \n",
    "    result_m = minimize(lambda p: mult_obj(p, fold_train), [0,0,0], \n",
    "                       method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "    mult_fold_results.append(result_m.x)\n",
    "    \n",
    "    # 3. ADDITIVE METHOD\n",
    "    def add_obj(params, df_data):\n",
    "        a0, a1, a2, a3 = params\n",
    "        predictions = []\n",
    "        for _, row in df_data.iterrows():\n",
    "            base_pred = row['SRKT2_Baseline']\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "            predictions.append(base_pred + correction)\n",
    "        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    \n",
    "    result_a = minimize(lambda p: add_obj(p, fold_train), [0,0,0,0],\n",
    "                       method='L-BFGS-B', bounds=[(-2,2),(-2,2),(-2,2),(-0.1,0.1)])\n",
    "    add_fold_results.append(result_a.x)\n",
    "    \n",
    "    # VALIDATE COMBINED on fold validation set\n",
    "    nc_b, nc_c, k_b, k_c, acd_b, acd_c = result_p.x\n",
    "    m0, m1, m2 = result_m.x\n",
    "    a0, a1, a2, a3 = result_a.x\n",
    "    \n",
    "    combined_preds = []\n",
    "    for _, row in fold_val.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        # Modified SRK/T2\n",
    "        nc = nc_b + nc_c * cct_norm\n",
    "        k_index = k_b + k_c * cct_norm\n",
    "        acd_offset = acd_b + acd_c * cct_norm\n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Apply multiplicative\n",
    "        mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "        after_mult = modified * mult_factor\n",
    "        \n",
    "        # Apply additive\n",
    "        add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "        final = after_mult + add_correction\n",
    "        \n",
    "        combined_preds.append(final)\n",
    "    \n",
    "    fold_mae = mean_absolute_error(fold_val['PostOP Spherical Equivalent'], combined_preds)\n",
    "    combined_fold_maes.append(fold_mae)\n",
    "    print(f\"  Combined Validation MAE: {fold_mae:.4f} D\")\n",
    "\n",
    "# Average parameters across folds\n",
    "avg_param = np.mean(param_fold_results, axis=0)\n",
    "avg_mult = np.mean(mult_fold_results, axis=0)\n",
    "avg_add = np.mean(add_fold_results, axis=0)\n",
    "avg_combined_mae = np.mean(combined_fold_maes)\n",
    "std_combined_mae = np.std(combined_fold_maes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä COMBINED METHOD CV PERFORMANCE:\")\n",
    "print(f\"  Average MAE: {avg_combined_mae:.4f} ¬± {std_combined_mae:.4f} D\")\n",
    "print(f\"  Best fold:   {min(combined_fold_maes):.4f} D\")\n",
    "print(f\"  Worst fold:  {max(combined_fold_maes):.4f} D\")\n",
    "\n",
    "# FINAL RETRAINING on full training set\n",
    "print(\"\\nüîß FINAL OPTIMIZATION on full training set...\")\n",
    "\n",
    "result_p_final = differential_evolution(lambda p: param_obj(p, X_train_comb), bounds_p, \n",
    "                                       maxiter=50, seed=42, disp=False)\n",
    "nc_base_c, nc_cct_c, k_base_c, k_cct_c, acd_base_c, acd_cct_c = result_p_final.x\n",
    "\n",
    "result_m_final = minimize(lambda p: mult_obj(p, X_train_comb), [0,0,0], \n",
    "                         method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "m0_c, m1_c, m2_c = result_m_final.x\n",
    "\n",
    "result_a_final = minimize(lambda p: add_obj(p, X_train_comb), [0,0,0,0],\n",
    "                         method='L-BFGS-B', bounds=[(-2,2),(-2,2),(-2,2),(-0.1,0.1)])\n",
    "a0_c, a1_c, a2_c, a3_c = result_a_final.x\n",
    "\n",
    "print(\"‚úÖ Final parameters optimized\")\n",
    "\n",
    "# FINAL TEST ON HOLDOUT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST ON HOLDOUT SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test individual methods and combined\n",
    "predictions_combined_test = []\n",
    "predictions_mult_only = []\n",
    "\n",
    "for _, row in X_test_comb.iterrows():\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "    k_avg = row['K_avg']\n",
    "    \n",
    "    # Modified SRK/T2\n",
    "    nc = nc_base_c + nc_cct_c * cct_norm\n",
    "    k_index = k_base_c + k_cct_c * cct_norm\n",
    "    acd_offset = acd_base_c + acd_cct_c * cct_norm\n",
    "    \n",
    "    modified_srkt2 = calculate_SRKT2(\n",
    "        AL=row['Bio-AL'], K_avg=k_avg,\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant'] + acd_offset,\n",
    "        nc=nc, k_index=k_index\n",
    "    )\n",
    "    \n",
    "    # Multiplicative only (for comparison)\n",
    "    mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio\n",
    "    mult_only = row['SRKT2_Baseline'] * mult_factor\n",
    "    predictions_mult_only.append(mult_only)\n",
    "    \n",
    "    # Combined: all three\n",
    "    after_mult = modified_srkt2 * mult_factor\n",
    "    add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg\n",
    "    final_combined = after_mult + add_correction\n",
    "    predictions_combined_test.append(final_combined)\n",
    "\n",
    "mae_baseline_test = np.abs(X_test_comb['SRKT2_Baseline'] - X_test_comb['PostOP Spherical Equivalent']).mean()\n",
    "mae_mult_test_comb = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], predictions_mult_only)\n",
    "mae_combined_test = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], predictions_combined_test)\n",
    "\n",
    "print(f\"\\nüìä FINAL TEST RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Baseline:              {mae_baseline_test:.4f} D\")\n",
    "print(f\"  Multiplicative only:   {mae_mult_test_comb:.4f} D ({(mae_baseline_test-mae_mult_test_comb)/mae_baseline_test*100:.1f}%)\")\n",
    "print(f\"  COMBINED (all 3):      {mae_combined_test:.4f} D ({(mae_baseline_test-mae_combined_test)/mae_baseline_test*100:.1f}%)\")\n",
    "\n",
    "if mae_combined_test < mae_mult_test_comb:\n",
    "    improvement = mae_mult_test_comb - mae_combined_test\n",
    "    print(f\"\\n‚úÖ COMBINED APPROACH WINS!\")\n",
    "    print(f\"   Beats multiplicative by {improvement:.4f} D\")\n",
    "else:\n",
    "    print(f\"\\nüìä Multiplicative alone is still best\")\n",
    "\n",
    "# Clinical accuracy\n",
    "errors_combined = np.abs(np.array(predictions_combined_test) - X_test_comb['PostOP Spherical Equivalent'])\n",
    "within_050 = (errors_combined <= 0.50).sum() / len(X_test_comb) * 100\n",
    "within_100 = (errors_combined <= 1.00).sum() / len(X_test_comb) * 100\n",
    "\n",
    "print(f\"\\nüìà CLINICAL ACCURACY (Combined):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Within ¬±0.50 D:  {within_050:.1f}%\")\n",
    "print(f\"  Within ¬±1.00 D:  {within_100:.1f}%\")\n",
    "\n",
    "print(\"\\nüí° K-FOLD INSIGHTS:\")\n",
    "print(\"-\" * 70)\n",
    "if std_combined_mae < 0.15:\n",
    "    print(f\"‚úÖ Stable across folds (std={std_combined_mae:.4f})\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Variable across folds (std={std_combined_mae:.4f})\")\n",
    "\n",
    "if abs(avg_combined_mae - mae_combined_test) < 0.2:\n",
    "    print(f\"‚úÖ Good generalization: CV={avg_combined_mae:.3f} vs Test={mae_combined_test:.3f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Generalization gap: CV={avg_combined_mae:.3f} vs Test={mae_combined_test:.3f}\")\n",
    "\n",
    "print(\"\\nüìê FINAL COMBINED FORMULA:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Modified SRK/T2:\")\n",
    "print(f\"   nc = {nc_base_c:.4f} + {nc_cct_c:.4f} √ó CCT_norm\")\n",
    "print(f\"   k_index = {k_base_c:.4f} + {k_cct_c:.4f} √ó CCT_norm\")\n",
    "print(\"2. Multiply by:\")\n",
    "print(f\"   Factor = 1 + {m0_c:.4f} + {m1_c:.4f} √ó CCT_norm + {m2_c:.4f} √ó CCT_ratio\")\n",
    "print(\"3. Add:\")\n",
    "print(f\"   Term = {a0_c:.4f} + {a1_c:.4f} √ó CCT_norm + {a2_c:.4f} √ó CCT_ratio + {a3_c:.4f} √ó K_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cyzg6jqeoho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED APPROACH WITHOUT ADDITIVE - FIXED VERSION\n",
    "# ========================================================\n",
    "# PURPOSE: Combine Parameter Optimization + Multiplicative Correction ONLY\n",
    "# Using JOINT optimization in K-fold CV (not separate!)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED FORMULA: PARAMETER + MULTIPLICATIVE (FIXED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ FIXED COMBINED APPROACH:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Joint optimization of parameters + multiplicative\")\n",
    "print(\"‚Ä¢ NO separate optimization (that was the bug!)\")\n",
    "print(\"‚Ä¢ NO additive correction\")\n",
    "print(\"‚Ä¢ Proper K-fold validation\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import differential_evolution\n",
    "import numpy as np\n",
    "\n",
    "# OUTER SPLIT\n",
    "X_train_fixed, X_test_fixed = train_test_split(df, test_size=0.25, random_state=42)\n",
    "X_train_fixed['K_avg'] = (X_train_fixed['Bio-Ks'] + X_train_fixed['Bio-Kf']) / 2\n",
    "X_test_fixed['K_avg'] = (X_test_fixed['Bio-Ks'] + X_test_fixed['Bio-Kf']) / 2\n",
    "\n",
    "print(f\"\\nüìä DATA SPLIT:\")\n",
    "print(f\"  Training: {len(X_train_fixed)} patients\")\n",
    "print(f\"  Test:     {len(X_test_fixed)} patients (holdout)\")\n",
    "\n",
    "# Define joint objective function\n",
    "def joint_objective_fixed(all_params, df_data):\n",
    "    \"\"\"Joint optimization of param + mult\"\"\"\n",
    "    # Split parameters\n",
    "    nc_base, nc_cct, k_base, k_cct, acd_base, acd_cct = all_params[:6]\n",
    "    m0, m1, m2 = all_params[6:]\n",
    "    \n",
    "    predictions = []\n",
    "    for _, row in df_data.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        # Step 1: Modified SRK/T2\n",
    "        nc = nc_base + nc_cct * cct_norm\n",
    "        k_index = k_base + k_cct * cct_norm\n",
    "        acd_offset = acd_base + acd_cct * cct_norm\n",
    "        \n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], \n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, \n",
    "            k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Step 2: Apply multiplicative\n",
    "        mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "        final = modified * mult_factor\n",
    "        \n",
    "        predictions.append(final)\n",
    "    \n",
    "    return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "\n",
    "# Joint bounds\n",
    "bounds_joint = [\n",
    "    (1.20, 1.50), (-0.20, 0.20),   # nc_base, nc_cct\n",
    "    (1.20, 1.60), (-0.30, 0.30),   # k_index_base, k_index_cct\n",
    "    (-3.0, 3.0), (-3.0, 3.0),       # acd_offset_base, acd_offset_cct\n",
    "    (-0.5, 0.5), (-0.5, 0.5), (-0.5, 0.5)  # m0, m1, m2\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD CV WITH JOINT OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_params_fixed = []\n",
    "fold_maes_fixed = []\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_fixed), 1):\n",
    "    print(f\"\\nüìÅ FOLD {fold_num}/5:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fold_train = X_train_fixed.iloc[train_idx]\n",
    "    fold_val = X_train_fixed.iloc[val_idx]\n",
    "    print(f\"  Train: {len(fold_train)} | Validate: {len(fold_val)}\")\n",
    "    \n",
    "    # JOINT OPTIMIZATION (the fix!)\n",
    "    result_fold = differential_evolution(\n",
    "        lambda p: joint_objective_fixed(p, fold_train),\n",
    "        bounds_joint,\n",
    "        maxiter=50,  # Reasonable iterations for CV\n",
    "        seed=42 + fold_num,\n",
    "        disp=False,\n",
    "        workers=1\n",
    "    )\n",
    "    \n",
    "    fold_params_fixed.append(result_fold.x)\n",
    "    \n",
    "    # Validate on fold validation set\n",
    "    val_mae = joint_objective_fixed(result_fold.x, fold_val)\n",
    "    fold_maes_fixed.append(val_mae)\n",
    "    \n",
    "    print(f\"  Validation MAE: {val_mae:.4f} D\")\n",
    "    \n",
    "    # Show optimized parameters\n",
    "    nc_b, nc_c, k_b, k_c, acd_b, acd_c, m0, m1, m2 = result_fold.x\n",
    "    print(f\"  Param: nc={nc_b:.3f}¬±{nc_c:.3f}, k={k_b:.3f}¬±{k_c:.3f}\")\n",
    "    print(f\"  Mult:  1 + {m0:.3f} + {m1:.3f}√óCCT + {m2:.3f}√óratio\")\n",
    "\n",
    "# Average results\n",
    "avg_params_fixed = np.mean(fold_params_fixed, axis=0)\n",
    "std_params_fixed = np.std(fold_params_fixed, axis=0)\n",
    "avg_mae_fixed = np.mean(fold_maes_fixed)\n",
    "std_mae_fixed = np.std(fold_maes_fixed)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä CV PERFORMANCE:\")\n",
    "print(f\"  Average MAE: {avg_mae_fixed:.4f} ¬± {std_mae_fixed:.4f} D\")\n",
    "print(f\"  Best fold:   {min(fold_maes_fixed):.4f} D\")\n",
    "print(f\"  Worst fold:  {max(fold_maes_fixed):.4f} D\")\n",
    "\n",
    "print(\"\\n‚úÖ AVERAGED PARAMETERS (from 5 folds):\")\n",
    "for i, name in enumerate(['nc_base', 'nc_cct', 'k_base', 'k_cct', 'acd_base', 'acd_cct', 'm0', 'm1', 'm2']):\n",
    "    print(f\"  {name:12} = {avg_params_fixed[i]:+.4f} ¬± {std_params_fixed[i]:.4f}\")\n",
    "\n",
    "# FINAL RETRAINING on full training set\n",
    "print(\"\\nüîß FINAL OPTIMIZATION on full training set...\")\n",
    "\n",
    "result_final = differential_evolution(\n",
    "    lambda p: joint_objective_fixed(p, X_train_fixed),\n",
    "    bounds_joint,\n",
    "    maxiter=100,\n",
    "    seed=42,\n",
    "    disp=False,\n",
    "    workers=1\n",
    ")\n",
    "\n",
    "nc_base_f, nc_cct_f, k_base_f, k_cct_f, acd_base_f, acd_cct_f, m0_f, m1_f, m2_f = result_final.x\n",
    "print(\"‚úÖ Final optimization completed\")\n",
    "\n",
    "# Calculate baseline for test set\n",
    "X_test_fixed['SRKT2_Baseline'] = X_test_fixed.apply(\n",
    "    lambda row: calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# FINAL TEST ON HOLDOUT\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST ON HOLDOUT SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "predictions_fixed = []\n",
    "for _, row in X_test_fixed.iterrows():\n",
    "    cct_norm = (row['CCT'] - 600) / 100\n",
    "    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "    \n",
    "    # Modified SRK/T2\n",
    "    nc = nc_base_f + nc_cct_f * cct_norm\n",
    "    k_index = k_base_f + k_cct_f * cct_norm\n",
    "    acd_offset = acd_base_f + acd_cct_f * cct_norm\n",
    "    \n",
    "    modified = calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant'] + acd_offset,\n",
    "        nc=nc,\n",
    "        k_index=k_index\n",
    "    )\n",
    "    \n",
    "    # Multiplicative correction\n",
    "    mult_factor = 1 + m0_f + m1_f * cct_norm + m2_f * cct_ratio\n",
    "    final = modified * mult_factor\n",
    "    predictions_fixed.append(final)\n",
    "\n",
    "mae_baseline_fixed = np.abs(X_test_fixed['SRKT2_Baseline'] - X_test_fixed['PostOP Spherical Equivalent']).mean()\n",
    "mae_fixed_test = mean_absolute_error(X_test_fixed['PostOP Spherical Equivalent'], predictions_fixed)\n",
    "improvement_fixed = (mae_baseline_fixed - mae_fixed_test) / mae_baseline_fixed * 100\n",
    "\n",
    "print(f\"\\nüìä TEST RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Baseline SRK/T2:           {mae_baseline_fixed:.4f} D\")\n",
    "print(f\"  FIXED Combined (no add):   {mae_fixed_test:.4f} D\")\n",
    "print(f\"  Improvement:               {improvement_fixed:.1f}%\")\n",
    "\n",
    "# Clinical accuracy\n",
    "errors_fixed = np.abs(np.array(predictions_fixed) - X_test_fixed['PostOP Spherical Equivalent'])\n",
    "within_025 = (errors_fixed <= 0.25).sum() / len(X_test_fixed) * 100\n",
    "within_050 = (errors_fixed <= 0.50).sum() / len(X_test_fixed) * 100\n",
    "within_075 = (errors_fixed <= 0.75).sum() / len(X_test_fixed) * 100\n",
    "within_100 = (errors_fixed <= 1.00).sum() / len(X_test_fixed) * 100\n",
    "\n",
    "print(f\"\\nüìà CLINICAL ACCURACY:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Within ¬±0.25 D:  {within_025:.1f}%\")\n",
    "print(f\"  Within ¬±0.50 D:  {within_050:.1f}%\")\n",
    "print(f\"  Within ¬±0.75 D:  {within_075:.1f}%\")\n",
    "print(f\"  Within ¬±1.00 D:  {within_100:.1f}%\")\n",
    "\n",
    "# Compare with other methods if available\n",
    "print(\"\\nüí° COMPARISON WITH OTHER METHODS:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'mae_mult_test' in globals():\n",
    "    diff_mult = mae_mult_test - mae_fixed_test\n",
    "    if diff_mult > 0:\n",
    "        print(f\"‚úÖ Beats Multiplicative-only by {diff_mult:.4f} D\")\n",
    "    else:\n",
    "        print(f\"üìä Multiplicative-only still better by {-diff_mult:.4f} D\")\n",
    "\n",
    "if 'mae_combined_test' in globals():\n",
    "    diff_comb = mae_combined_test - mae_fixed_test\n",
    "    if diff_comb > 0:\n",
    "        print(f\"‚úÖ Beats Full Combined (with add) by {diff_comb:.4f} D\")\n",
    "    else:\n",
    "        print(f\"üìä Full Combined still better by {-diff_comb:.4f} D\")\n",
    "\n",
    "print(\"\\nüìê FINAL FORMULA (PARAM + MULT):\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Modified SRK/T2:\")\n",
    "print(f\"   nc = {nc_base_f:.4f} {nc_cct_f:+.4f} √ó CCT_norm\")\n",
    "print(f\"   k_index = {k_base_f:.4f} {k_cct_f:+.4f} √ó CCT_norm\")\n",
    "print(f\"   ACD_offset = {acd_base_f:.4f} {acd_cct_f:+.4f} √ó CCT_norm\")\n",
    "print(\"\\n2. Multiplicative correction:\")\n",
    "print(f\"   Factor = 1 {m0_f:+.4f} {m1_f:+.4f} √ó CCT_norm {m2_f:+.4f} √ó CCT_ratio\")\n",
    "print(\"\\nWhere: CCT_norm = (CCT - 600) / 100, CCT_ratio = CCT / AL\")\n",
    "\n",
    "# Store for comparison\n",
    "mae_fixed_combined = mae_fixed_test\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è KEY FIX:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ Previous version optimized param and mult SEPARATELY\")\n",
    "print(\"‚Ä¢ This caused incompatibility when combined\")\n",
    "print(\"‚Ä¢ Now using JOINT optimization throughout\")\n",
    "print(\"‚Ä¢ Results should be much better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llbtf8a9trq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL RESULTS SUMMARY WITH K-FOLD VALIDATION\n",
    "# ============================================\n",
    "# PURPOSE: Summarize and compare all K-fold validated methods\n",
    "# Uses results from properly validated cells with train/test split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY - K-FOLD VALIDATED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT NOTES:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ All methods use SAME train/test split (random_state=42)\")\n",
    "print(\"‚Ä¢ Training: 72 patients with 5-fold CV\")\n",
    "print(\"‚Ä¢ Test: 24 patients (never seen during optimization)\")\n",
    "print(\"‚Ä¢ These are HONEST performance estimates!\")\n",
    "\n",
    "# Check if K-fold cells have been run\n",
    "required_vars = ['mae_param_test', 'mae_mult_test', 'mae_add_test']\n",
    "missing_vars = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some K-fold cells haven't been run yet!\")\n",
    "    print(f\"   Missing: {missing_vars}\")\n",
    "    print(\"   Please run all optimization cells first.\")\n",
    "else:\n",
    "    # Collect test results from K-fold validated cells\n",
    "    \n",
    "    # We need to recalculate baseline on the test set\n",
    "    # Using the same split as in K-fold cells\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_final, X_test_final = train_test_split(df, test_size=0.25, random_state=42)\n",
    "    X_test_final['K_avg'] = (X_test_final['Bio-Ks'] + X_test_final['Bio-Kf']) / 2\n",
    "    X_test_final['SRKT2_Baseline'] = X_test_final.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    mae_baseline_test = np.abs(X_test_final['SRKT2_Baseline'] - X_test_final['PostOP Spherical Equivalent']).mean()\n",
    "    \n",
    "    # Collect results from K-fold validated methods\n",
    "    results_summary = {\n",
    "        'Baseline SRK/T2': mae_baseline_test,\n",
    "        'Parameter Optimization': mae_param_test if 'mae_param_test' in globals() else None,\n",
    "        'Multiplicative Correction': mae_mult_test if 'mae_mult_test' in globals() else None,\n",
    "        'Additive Correction': mae_add_test if 'mae_add_test' in globals() else None,\n",
    "        'Combined (if available)': mae_combined_test if 'mae_combined_test' in globals() else None,\n",
    "        'Advanced Features (if available)': mae_test_adv if 'mae_test_adv' in globals() else None\n",
    "    }\n",
    "    \n",
    "    # Remove None values\n",
    "    results_summary = {k: v for k, v in results_summary.items() if v is not None}\n",
    "    \n",
    "    print(\"\\nüìä TEST SET PERFORMANCE (24 holdout patients):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for method, mae in results_summary.items():\n",
    "        if method == 'Baseline SRK/T2':\n",
    "            print(f\"  {method:30} MAE: {mae:.4f} D\")\n",
    "        else:\n",
    "            improvement = (mae_baseline_test - mae) / mae_baseline_test * 100\n",
    "            print(f\"  {method:30} MAE: {mae:.4f} D ({improvement:+.1f}%)\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method = min(results_summary.items(), key=lambda x: x[1])\n",
    "    best_name, best_mae = best_method\n",
    "    \n",
    "    if best_name != 'Baseline SRK/T2':\n",
    "        best_improvement = (mae_baseline_test - best_mae) / mae_baseline_test * 100\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST METHOD: {best_name}\")\n",
    "        print(f\"   MAE: {best_mae:.4f} D\")\n",
    "        print(f\"   Improvement: {best_improvement:.1f}%\")\n",
    "    \n",
    "    # Clinical accuracy for best method (need to recalculate)\n",
    "    print(\"\\nüìà CLINICAL ACCURACY ANALYSIS:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Calculate baseline clinical accuracy\n",
    "    baseline_errors = np.abs(X_test_final['SRKT2_Baseline'] - X_test_final['PostOP Spherical Equivalent'])\n",
    "    baseline_025 = (baseline_errors <= 0.25).sum() / len(X_test_final) * 100\n",
    "    baseline_050 = (baseline_errors <= 0.50).sum() / len(X_test_final) * 100\n",
    "    baseline_075 = (baseline_errors <= 0.75).sum() / len(X_test_final) * 100\n",
    "    baseline_100 = (baseline_errors <= 1.00).sum() / len(X_test_final) * 100\n",
    "    \n",
    "    print(\"Baseline SRK/T2:\")\n",
    "    print(f\"  Within ¬±0.25 D: {baseline_025:.1f}%\")\n",
    "    print(f\"  Within ¬±0.50 D: {baseline_050:.1f}%\")\n",
    "    print(f\"  Within ¬±0.75 D: {baseline_075:.1f}%\")\n",
    "    print(f\"  Within ¬±1.00 D: {baseline_100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nüí° KEY INSIGHTS FROM K-FOLD VALIDATION:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"1. These results use proper train/test separation\")\n",
    "    print(\"2. K-fold CV was used to find stable parameters\")\n",
    "    print(\"3. Test performance is on 24 completely unseen patients\")\n",
    "    print(\"4. Lower than original results but MORE HONEST\")\n",
    "    \n",
    "    # Compare methods if multiple available\n",
    "    if len(results_summary) > 2:  # More than just baseline + 1 method\n",
    "        print(\"\\nüìä METHOD COMPARISON:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by MAE\n",
    "        sorted_results = sorted(results_summary.items(), key=lambda x: x[1])\n",
    "        \n",
    "        for i, (method, mae) in enumerate(sorted_results, 1):\n",
    "            if method != 'Baseline SRK/T2':\n",
    "                improvement = (mae_baseline_test - mae) / mae_baseline_test * 100\n",
    "                print(f\"  {i}. {method:28} {improvement:+5.1f}% improvement\")\n",
    "    \n",
    "    print(\"\\nüìã VALIDATION APPROACH:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"‚Ä¢ Outer: 75/25 train/test split\")\n",
    "    print(\"‚Ä¢ Inner: 5-fold CV on training set\")\n",
    "    print(\"‚Ä¢ Final: Test once on holdout\")\n",
    "    print(\"‚Ä¢ Seed: 42 for reproducibility\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è IMPORTANT FOR PUBLICATION:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"‚Ä¢ Report these K-fold validated results\")\n",
    "    print(\"‚Ä¢ Original results without proper validation were overfitted\")\n",
    "    print(f\"‚Ä¢ Best honest improvement: {best_improvement:.1f}% (vs inflated ~30+%)\")\n",
    "    print(\"‚Ä¢ Still clinically meaningful improvement!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF K-FOLD VALIDATED RESULTS\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
