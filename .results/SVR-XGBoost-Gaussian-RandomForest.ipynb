{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41782613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ MULTI-SEED CONFIGURATION\n",
      "======================================================================\n",
      "Seeds for validation: [42, 123, 456, 789, 2025]\n",
      "This ensures results are not dependent on random split\n",
      "Each seed creates different train/test splits for robust assessment\n",
      "======================================================================\n",
      "IOL CALCULATION FOR PRE-DMEK PATIENTS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š WHAT WE'RE DOING:\n",
      "--------------------------------------------------\n",
      "â€¢ Loading data from Fuchs' dystrophy patients\n",
      "â€¢ These patients had combined cataract + DMEK surgery\n",
      "â€¢ Goal: Improve IOL power calculation accuracy\n",
      "â€¢ Challenge: Edematous corneas distort standard formulas\n",
      "â€¢ NEW: Using 5 different seeds for robust validation\n",
      "\n",
      "âœ… Loaded 96 patients from FacoDMEK.xlsx\n",
      "\n",
      "ğŸ” KEY MEASUREMENTS IN OUR DATA:\n",
      "--------------------------------------------------\n",
      "â€¢ Bio-AL: Axial length (mm)\n",
      "â€¢ Bio-Ks/Kf: Steep and flat keratometry (D)\n",
      "â€¢ CCT: Central corneal thickness (Î¼m) - KEY for edema\n",
      "â€¢ IOL Power: Implanted lens power (D)\n",
      "â€¢ PostOP Spherical Equivalent: Actual outcome (D)\n"
     ]
    }
   ],
   "source": [
    "# IOL CALCULATION FOR PRE-DMEK PATIENTS - SETUP AND DATA LOADING\n",
    "# ================================================================\n",
    "# PURPOSE: Set up the analysis environment and load patient data\n",
    "# This notebook optimizes IOL power calculations for Fuchs' dystrophy patients\n",
    "# undergoing combined phacoemulsification and DMEK surgery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants for clinical accuracy thresholds (diopters)\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 1.00]\n",
    "TEST_SIZE = 0.25      # 25% holdout for final testing\n",
    "N_FOLDS = 5           # 5-fold cross-validation\n",
    "\n",
    "# MULTI-SEED CONFIGURATION FOR ROBUST VALIDATION\n",
    "#SEEDS = [42]  # Quick test with single seed\n",
    "#SEEDS = [42, 123]  # Medium test with 2 seeds\n",
    "SEEDS = [42, 123, 456, 789, 2025]  # Multiple seeds for statistical robustness\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ MULTI-SEED CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Seeds for validation: {SEEDS}\")\n",
    "print(\"This ensures results are not dependent on random split\")\n",
    "print(\"Each seed creates different train/test splits for robust assessment\")\n",
    "\n",
    "# Storage for multi-seed results\n",
    "multi_seed_results = {\n",
    "\n",
    "    'parameter': {},\n",
    "    'multiplicative': {},\n",
    "    'additive': {},\n",
    "    'combined': {},\n",
    "    'fixed_combined': {}\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IOL CALCULATION FOR PRE-DMEK PATIENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“Š WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Loading data from Fuchs' dystrophy patients\")\n",
    "print(\"â€¢ These patients had combined cataract + DMEK surgery\")\n",
    "print(\"â€¢ Goal: Improve IOL power calculation accuracy\")\n",
    "print(\"â€¢ Challenge: Edematous corneas distort standard formulas\")\n",
    "print(f\"â€¢ NEW: Using {len(SEEDS)} different seeds for robust validation\")\n",
    "\n",
    "# Load the patient data\n",
    "df = pd.read_excel('FacoDMEK.xlsx')\n",
    "print(f\"\\nâœ… Loaded {len(df)} patients from FacoDMEK.xlsx\")\n",
    "\n",
    "print(\"\\nğŸ” KEY MEASUREMENTS IN OUR DATA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Bio-AL: Axial length (mm)\")\n",
    "print(\"â€¢ Bio-Ks/Kf: Steep and flat keratometry (D)\")\n",
    "print(\"â€¢ CCT: Central corneal thickness (Î¼m) - KEY for edema\")\n",
    "print(\"â€¢ IOL Power: Implanted lens power (D)\")\n",
    "print(\"â€¢ PostOP Spherical Equivalent: Actual outcome (D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9871e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SRK/T2 FORMULA (Sheard et al. 2010)\n",
      "======================================================================\n",
      "â€¢ SKR/T2 assumes normal corneal properties\n",
      "â€¢ In Fuchs' dystrophy, the cornea is NOT normal:\n",
      "  - Edema changes refractive index (nc)\n",
      "  - Swelling alters keratometric index (k_index)\n",
      "  - Anterior chamber depth is affected\n",
      "\n",
      "Our strategy: Keep the formula structure, optimize the parameters!\n",
      "\n",
      "ğŸ“ THE SRK/T2 FORMULA:\n",
      "\n",
      "         1000Â·nâ‚Â·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) - PÂ·(Lopt - ACDest)Â·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest)\n",
      "REF = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       nâ‚Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) + LoptÂ·r) - 0.001Â·PÂ·(Lopt - ACDest)Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest) + ACDestÂ·r)\n"
     ]
    }
   ],
   "source": [
    "# STANDARD SRK/T2 FORMULA IMPLEMENTATION\n",
    "# ========================================\n",
    "# PURPOSE: Implement the baseline SRK/T2 formula (Sheard et al. 2010)\n",
    "# This is the current gold standard for IOL calculations\n",
    "# We'll use this as our baseline to compare improvements against\n",
    "\n",
    "def calculate_SRKT2(AL, K_avg, IOL_power, A_constant, nc=1.333, k_index=1.3375):\n",
    "    \"\"\"\n",
    "    SRK/T2 Formula (Sheard et al. 2010)\n",
    "    - Assumes NORMAL corneas (nc=1.333, k_index=1.3375)\n",
    "    - These assumptions fail in edematous Fuchs' corneas\n",
    "    \n",
    "    Parameters:\n",
    "    - AL: Axial length (mm)\n",
    "    - K_avg: Average keratometry (D)\n",
    "    - IOL_power: IOL power (D)\n",
    "    - A_constant: Lens-specific constant\n",
    "    - nc: Corneal refractive index (we'll optimize this!)\n",
    "    - k_index: Keratometric index (we'll optimize this too!)\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    na = 1.336  # Aqueous/vitreous refractive index\n",
    "    V = 12      # Vertex distance (mm)\n",
    "    ncm1 = nc - 1\n",
    "    \n",
    "    # Convert keratometry to radius using keratometric index\n",
    "    # This is where edema causes problems - k_index assumes normal cornea!\n",
    "    r = (k_index - 1) * 1000 / K_avg\n",
    "    \n",
    "    # Axial length correction for long eyes\n",
    "    if AL <= 24.2:\n",
    "        LCOR = AL\n",
    "    else:\n",
    "        LCOR = 3.446 + 1.716 * AL - 0.0237 * AL * AL\n",
    "    \n",
    "    # H2 calculation (corneal height) - Sheard's modification\n",
    "    H2 = -10.326 + 0.32630 * LCOR + 0.13533 * K_avg\n",
    "    \n",
    "    # ACD (Anterior Chamber Depth) estimation\n",
    "    # Edema can affect this too!\n",
    "    ACD_const = 0.62467 * A_constant - 68.747\n",
    "    offset = ACD_const - 3.336\n",
    "    ACD_est = H2 + offset\n",
    "    \n",
    "    # Retinal thickness correction\n",
    "    RETHICK = 0.65696 - 0.02029 * AL\n",
    "    LOPT = AL + RETHICK  # Optical axial length\n",
    "    \n",
    "    # SRK/T2 refraction calculation - the complex optics formula\n",
    "    numerator = (1000 * na * (na * r - ncm1 * LOPT) - \n",
    "                 IOL_power * (LOPT - ACD_est) * (na * r - ncm1 * ACD_est))\n",
    "    \n",
    "    denominator = (na * (V * (na * r - ncm1 * LOPT) + LOPT * r) - \n",
    "                   0.001 * IOL_power * (LOPT - ACD_est) * \n",
    "                   (V * (na * r - ncm1 * ACD_est) + ACD_est * r))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SRK/T2 FORMULA (Sheard et al. 2010)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"â€¢ SKR/T2 assumes normal corneal properties\")\n",
    "print(\"â€¢ In Fuchs' dystrophy, the cornea is NOT normal:\")\n",
    "print(\"  - Edema changes refractive index (nc)\")\n",
    "print(\"  - Swelling alters keratometric index (k_index)\")\n",
    "print(\"  - Anterior chamber depth is affected\")\n",
    "print(\"\\nOur strategy: Keep the formula structure, optimize the parameters!\")\n",
    "\n",
    "print(\"\\nğŸ“ THE SRK/T2 FORMULA:\")\n",
    "print()\n",
    "print(\"         1000Â·nâ‚Â·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) - PÂ·(Lopt - ACDest)Â·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest)\")\n",
    "print(\"REF = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"       nâ‚Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) + LoptÂ·r) - 0.001Â·PÂ·(Lopt - ACDest)Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest) + ACDestÂ·r)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db415cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASELINE SRK/T2 PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ WHAT WE'RE DOING:\n",
      "--------------------------------------------------\n",
      "1. Calculate average K from steep and flat readings\n",
      "2. Apply standard SRK/T2 to all 96 patients\n",
      "3. Compare predictions to actual outcomes\n",
      "4. Measure error to establish baseline performance\n",
      "\n",
      "ğŸ“Š BASELINE PERFORMANCE METRICS:\n",
      "======================================================================\n",
      "  Mean Absolute Error (MAE):     1.3591 D\n",
      "  Mean Error (ME):                -0.2915 D\n",
      "  Standard Deviation (SD):        1.7471 D\n",
      "  Median Absolute Error:          1.0311 D\n",
      "\n",
      "ğŸ’¡ INTERPRETATION:\n",
      "--------------------------------------------------\n",
      "â€¢ MAE of 1.36 D is POOR (>1.0 D is clinically unacceptable)\n",
      "â€¢ Mean error of -0.29 D shows systematic bias\n",
      "  â†’ Formula tends to predict too myopic (negative)\n",
      "\n",
      "ğŸ“ˆ CLINICAL ACCURACY:\n",
      "----------------------------------------------------------------------\n",
      "  Within Â±0.25 D:  13.5% of eyes\n",
      "  Within Â±0.50 D:  26.0% of eyes\n",
      "  Within Â±0.75 D:  35.4% of eyes\n",
      "  Within Â±1.00 D:  49.0% of eyes\n",
      "\n",
      "ğŸ¯ CLINICAL TARGETS:\n",
      "--------------------------------------------------\n",
      "â€¢ Modern standard: >70% within Â±0.50 D\n",
      "â€¢ Acceptable: >90% within Â±1.00 D\n",
      "â€¢ Our baseline: 26.0% within Â±0.50 D\n",
      "\n",
      "âš ï¸ Standard SRK/T2 clearly struggles with Fuchs' dystrophy!\n",
      "This is why we need optimization!\n"
     ]
    }
   ],
   "source": [
    "# BASELINE PERFORMANCE EVALUATION\n",
    "# =================================\n",
    "# PURPOSE: Calculate how well standard SRK/T2 performs on our Fuchs' patients\n",
    "# This establishes the baseline that we need to beat\n",
    "# Spoiler: It won't be great due to the edematous corneas!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE SRK/T2 PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Calculate average K from steep and flat readings\")\n",
    "print(\"2. Apply standard SRK/T2 to all 96 patients\")\n",
    "print(\"3. Compare predictions to actual outcomes\")\n",
    "print(\"4. Measure error to establish baseline performance\")\n",
    "\n",
    "# Calculate average K (needed for SRK/T2)\n",
    "df['K_avg'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "\n",
    "# Apply standard SRK/T2 formula to all patients\n",
    "df['SRKT2_Prediction'] = df.apply(\n",
    "    lambda row: calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant']\n",
    "        # Note: Using DEFAULT nc=1.333 and k_index=1.3375\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Calculate prediction errors\n",
    "df['Prediction_Error'] = df['PostOP Spherical Equivalent'] - df['SRKT2_Prediction']\n",
    "df['Absolute_Error'] = abs(df['Prediction_Error'])\n",
    "\n",
    "# Calculate key metrics\n",
    "mae = df['Absolute_Error'].mean()\n",
    "me = df['Prediction_Error'].mean()\n",
    "std = df['Prediction_Error'].std()\n",
    "median_ae = df['Absolute_Error'].median()\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE PERFORMANCE METRICS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean Absolute Error (MAE):     {mae:.4f} D\")\n",
    "print(f\"  Mean Error (ME):                {me:+.4f} D\")\n",
    "print(f\"  Standard Deviation (SD):        {std:.4f} D\")\n",
    "print(f\"  Median Absolute Error:          {median_ae:.4f} D\")\n",
    "\n",
    "print(\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(\"-\" * 50)\n",
    "if mae > 1.0:\n",
    "    print(f\"â€¢ MAE of {mae:.2f} D is POOR (>1.0 D is clinically unacceptable)\")\n",
    "else:\n",
    "    print(f\"â€¢ MAE of {mae:.2f} D is moderate\")\n",
    "    \n",
    "if abs(me) > 0.25:\n",
    "    print(f\"â€¢ Mean error of {me:+.2f} D shows systematic bias\")\n",
    "    if me < 0:\n",
    "        print(\"  â†’ Formula tends to predict too myopic (negative)\")\n",
    "    else:\n",
    "        print(\"  â†’ Formula tends to predict too hyperopic (positive)\")\n",
    "\n",
    "# Calculate clinical accuracy rates\n",
    "within_025 = (df['Absolute_Error'] <= 0.25).sum() / len(df) * 100\n",
    "within_050 = (df['Absolute_Error'] <= 0.50).sum() / len(df) * 100\n",
    "within_075 = (df['Absolute_Error'] <= 0.75).sum() / len(df) * 100\n",
    "within_100 = (df['Absolute_Error'] <= 1.00).sum() / len(df) * 100\n",
    "\n",
    "print(\"\\nğŸ“ˆ CLINICAL ACCURACY:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Within Â±0.25 D:  {within_025:.1f}% of eyes\")\n",
    "print(f\"  Within Â±0.50 D:  {within_050:.1f}% of eyes\")\n",
    "print(f\"  Within Â±0.75 D:  {within_075:.1f}% of eyes\")\n",
    "print(f\"  Within Â±1.00 D:  {within_100:.1f}% of eyes\")\n",
    "\n",
    "print(\"\\nğŸ¯ CLINICAL TARGETS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Modern standard: >70% within Â±0.50 D\")\n",
    "print(\"â€¢ Acceptable: >90% within Â±1.00 D\")\n",
    "print(f\"â€¢ Our baseline: {within_050:.1f}% within Â±0.50 D\")\n",
    "print(\"\\nâš ï¸ Standard SRK/T2 clearly struggles with Fuchs' dystrophy!\")\n",
    "print(\"This is why we need optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d38452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCULATING GLOBAL BASELINE FOR FAIR COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š CALCULATING BASELINE ACROSS ALL SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed 42: Baseline MAE = 1.4849 D\n",
      "  Seed 123: Baseline MAE = 1.2755 D\n",
      "  Seed 456: Baseline MAE = 1.6714 D\n",
      "  Seed 789: Baseline MAE = 1.6185 D\n",
      "  Seed 2025: Baseline MAE = 1.3566 D\n",
      "\n",
      "================================================================================\n",
      "GLOBAL BASELINE ESTABLISHED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ BASELINE MAE: 1.4814 Â± 0.1503 D\n",
      "   Min: 1.2755 D\n",
      "   Max: 1.6714 D\n",
      "\n",
      "âš ï¸ IMPORTANT:\n",
      "--------------------------------------------------\n",
      "All methods will now use this baseline for improvement calculations.\n",
      "This ensures fair comparison between methods.\n",
      "Improvement = ((1.4814 - Method_MAE) / 1.4814) Ã— 100%\n",
      "\n",
      "âœ… Global baseline stored in variable: GLOBAL_BASELINE_MAE = 1.4814\n",
      "All methods should now calculate improvement as:\n",
      "   improvement = ((GLOBAL_BASELINE_MAE - test_mae) / GLOBAL_BASELINE_MAE) * 100\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL BASELINE CALCULATION - CONSISTENT ACROSS ALL METHODS\n",
    "# ===========================================================\n",
    "# PURPOSE: Calculate baseline performance ONCE for fair comparison\n",
    "# This ensures all methods use the same baseline for improvement calculations\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING GLOBAL BASELINE FOR FAIR COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nğŸ“Š CALCULATING BASELINE ACROSS ALL SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Store baseline MAEs for each seed\n",
    "global_baseline_maes = []\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    # Split data with this seed\n",
    "    X_train_base, X_test_base = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    \n",
    "    # Calculate K_avg\n",
    "    X_test_base['K_avg'] = (X_test_base['Bio-Ks'] + X_test_base['Bio-Kf']) / 2\n",
    "    \n",
    "    # Calculate baseline predictions\n",
    "    X_test_base['SRKT2_Baseline'] = X_test_base.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate MAE for this seed\n",
    "    baseline_mae = mean_absolute_error(\n",
    "        X_test_base['PostOP Spherical Equivalent'], \n",
    "        X_test_base['SRKT2_Baseline']\n",
    "    )\n",
    "    \n",
    "    global_baseline_maes.append(baseline_mae)\n",
    "    print(f\"  Seed {SEED}: Baseline MAE = {baseline_mae:.4f} D\")\n",
    "\n",
    "# Calculate global baseline (average across all seeds)\n",
    "GLOBAL_BASELINE_MAE = np.mean(global_baseline_maes)\n",
    "GLOBAL_BASELINE_STD = np.std(global_baseline_maes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GLOBAL BASELINE ESTABLISHED:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“Œ BASELINE MAE: {GLOBAL_BASELINE_MAE:.4f} Â± {GLOBAL_BASELINE_STD:.4f} D\")\n",
    "print(f\"   Min: {np.min(global_baseline_maes):.4f} D\")\n",
    "print(f\"   Max: {np.max(global_baseline_maes):.4f} D\")\n",
    "\n",
    "print(\"\\nâš ï¸ IMPORTANT:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"All methods will now use this baseline for improvement calculations.\")\n",
    "print(\"This ensures fair comparison between methods.\")\n",
    "print(f\"Improvement = (({GLOBAL_BASELINE_MAE:.4f} - Method_MAE) / {GLOBAL_BASELINE_MAE:.4f}) Ã— 100%\")\n",
    "\n",
    "# Store for use by all subsequent methods\n",
    "print(f\"\\nâœ… Global baseline stored in variable: GLOBAL_BASELINE_MAE = {GLOBAL_BASELINE_MAE:.4f}\")\n",
    "print(\"All methods should now calculate improvement as:\")\n",
    "print(\"   improvement = ((GLOBAL_BASELINE_MAE - test_mae) / GLOBAL_BASELINE_MAE) * 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ridge_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RIDGE REGRESSION FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ” WHY START WITH RIDGE?\n",
      "--------------------------------------------------\n",
      "â€¢ Ridge regression identifies important features\n",
      "â€¢ Helps us understand what drives prediction errors\n",
      "â€¢ Guides our formula optimization strategy\n",
      "â€¢ If CCT features are important, our hypothesis is correct!\n",
      "\n",
      "ğŸ“Š CREATING FEATURES:\n",
      "--------------------------------------------------\n",
      "Created 12 features including CCT interactions\n",
      "\n",
      "ğŸ† TOP 10 MOST IMPORTANT FEATURES:\n",
      "--------------------------------------------------\n",
      "  CCT_ratio_AL         Coef=+1.3677\n",
      "  CCT_x_AL             Coef=-0.8898\n",
      "  CCT_squared          Coef=-0.7666\n",
      "  Bio-AL               Coef=+0.4903\n",
      "  Bio-Ks               Coef=-0.3178\n",
      "  CCT_x_K              Coef=+0.3101\n",
      "  K_avg                Coef=-0.1584\n",
      "  IOL Power            Coef=-0.1189\n",
      "  CCT_norm             Coef=+0.0321\n",
      "  CCT                  Coef=+0.0321\n",
      "\n",
      "ğŸ’¡ KEY FINDINGS:\n",
      "--------------------------------------------------\n",
      "â€¢ CCT-related features account for 75.5% of total importance\n",
      "â€¢ Top feature: CCT_ratio_AL\n",
      "â€¢ CCT/AL ratio is among top 3 features!\n",
      "â€¢ This validates that CCT relative to eye size matters\n",
      "\n",
      "âœ… HYPOTHESIS CONFIRMED:\n",
      "CCT features dominate prediction - our CCT-dependent approach is justified!\n",
      "\n",
      "ğŸ¯ OPTIMIZATION STRATEGY BASED ON RIDGE:\n",
      "--------------------------------------------------\n",
      "1. Make optical parameters CCT-dependent (nc, k_index)\n",
      "2. Consider CCT/AL ratio in corrections\n",
      "3. Account for CCT interactions with other measurements\n"
     ]
    }
   ],
   "source": [
    "# RIDGE REGRESSION ANALYSIS - IDENTIFYING IMPORTANT FEATURES\n",
    "# ===========================================================\n",
    "# PURPOSE: Use machine learning to identify which features matter most\n",
    "# This will guide our optimization strategy\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RIDGE REGRESSION FEATURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ” WHY START WITH RIDGE?\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Ridge regression identifies important features\")\n",
    "print(\"â€¢ Helps us understand what drives prediction errors\")\n",
    "print(\"â€¢ Guides our formula optimization strategy\")\n",
    "print(\"â€¢ If CCT features are important, our hypothesis is correct!\")\n",
    "\n",
    "# Create feature matrix with interactions\n",
    "print(\"\\nğŸ“Š CREATING FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "features = []\n",
    "feature_names = []\n",
    "\n",
    "# Basic features\n",
    "for col in ['Bio-AL', 'Bio-Ks', 'Bio-Kf', 'IOL Power', 'CCT']:\n",
    "    features.append(df[col].values)\n",
    "    feature_names.append(col)\n",
    "\n",
    "# Add K_avg\n",
    "features.append(df['K_avg'].values)\n",
    "feature_names.append('K_avg')\n",
    "\n",
    "# CCT-derived features\n",
    "df['CCT_squared'] = df['CCT'] ** 2\n",
    "df['CCT_deviation'] = df['CCT'] - 550\n",
    "df['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_squared'].values,\n",
    "    df['CCT_deviation'].values,\n",
    "    df['CCT_norm'].values\n",
    "])\n",
    "feature_names.extend(['CCT_squared', 'CCT_deviation', 'CCT_norm'])\n",
    "\n",
    "# Interaction terms\n",
    "df['CCT_x_AL'] = df['CCT'] * df['Bio-AL']\n",
    "df['CCT_x_K'] = df['CCT'] * df['K_avg']\n",
    "df['CCT_ratio_AL'] = df['CCT'] / df['Bio-AL']\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_x_AL'].values,\n",
    "    df['CCT_x_K'].values,\n",
    "    df['CCT_ratio_AL'].values\n",
    "])\n",
    "feature_names.extend(['CCT_x_AL', 'CCT_x_K', 'CCT_ratio_AL'])\n",
    "\n",
    "X = np.column_stack(features)\n",
    "y = df['PostOP Spherical Equivalent'].values\n",
    "\n",
    "print(f\"Created {len(feature_names)} features including CCT interactions\")\n",
    "\n",
    "# Standardize and train Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Ridge to get feature importance\n",
    "ridge_analysis = Ridge(alpha=1.0)\n",
    "ridge_analysis.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importance from coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': ridge_analysis.coef_,\n",
    "    'Abs_Coefficient': np.abs(ridge_analysis.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ† TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['Feature']:20} Coef={row['Coefficient']:+.4f}\")\n",
    "\n",
    "# Analyze CCT importance\n",
    "cct_features = feature_importance[feature_importance['Feature'].str.contains('CCT')]\n",
    "cct_importance = cct_features['Abs_Coefficient'].sum()\n",
    "total_importance = feature_importance['Abs_Coefficient'].sum()\n",
    "cct_percentage = (cct_importance / total_importance) * 100\n",
    "\n",
    "print(\"\\nğŸ’¡ KEY FINDINGS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ CCT-related features account for {cct_percentage:.1f}% of total importance\")\n",
    "print(f\"â€¢ Top feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "\n",
    "if 'CCT_ratio_AL' in feature_importance.head(3)['Feature'].values:\n",
    "    print(\"â€¢ CCT/AL ratio is among top 3 features!\")\n",
    "    print(\"â€¢ This validates that CCT relative to eye size matters\")\n",
    "\n",
    "if cct_percentage > 50:\n",
    "    print(\"\\nâœ… HYPOTHESIS CONFIRMED:\")\n",
    "    print(\"CCT features dominate prediction - our CCT-dependent approach is justified!\")\n",
    "\n",
    "print(\"\\nğŸ¯ OPTIMIZATION STRATEGY BASED ON RIDGE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Make optical parameters CCT-dependent (nc, k_index)\")\n",
    "print(\"2. Consider CCT/AL ratio in corrections\")\n",
    "print(\"3. Account for CCT interactions with other measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rt23gheoiv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARAMETER OPTIMIZATION WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CROSS-VALIDATION:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75% train, 25% test\n",
      "â€¢ Inner: 5-fold CV on training set\n",
      "â€¢ Results averaged across seeds for robustness\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 1.2383 Â± 0.3650 D\n",
      "  Train MAE: 1.1642, Test MAE: 1.4354\n",
      "  Test: Baseline=1.4849, Optimized=1.4354\n",
      "  Improvement: 3.3%\n",
      "  âš ï¸ Overfitting detected: Test 23.3% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 1.3361 Â± 0.2740 D\n",
      "  Train MAE: 1.2528, Test MAE: 1.0289\n",
      "  Test: Baseline=1.2755, Optimized=1.0289\n",
      "  Improvement: 19.3%\n",
      "  âœ… Good generalization: Test only -17.9% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 1.1921 Â± 0.1903 D\n",
      "  Train MAE: 1.1143, Test MAE: 1.4725\n",
      "  Test: Baseline=1.6714, Optimized=1.4725\n",
      "  Improvement: 11.9%\n",
      "  âš ï¸ Overfitting detected: Test 32.1% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 1.1991 Â± 0.3443 D\n",
      "  Train MAE: 1.1025, Test MAE: 1.4542\n",
      "  Test: Baseline=1.6185, Optimized=1.4542\n",
      "  Improvement: 10.2%\n",
      "  âš ï¸ Overfitting detected: Test 31.9% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 1.3382 Â± 0.1226 D\n",
      "  Train MAE: 1.2004, Test MAE: 1.2115\n",
      "  Test: Baseline=1.3566, Optimized=1.2115\n",
      "  Improvement: 10.7%\n",
      "  âœ… Good generalization: Test only 0.9% worse than train\n",
      "\n",
      "================================================================================\n",
      "PARAMETER OPTIMIZATION - MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed  42: MAE=1.4354 D, Improvement=3.3%\n",
      "  Seed 123: MAE=1.0289 D, Improvement=19.3%\n",
      "  Seed 456: MAE=1.4725 D, Improvement=11.9%\n",
      "  Seed 789: MAE=1.4542 D, Improvement=10.2%\n",
      "  Seed 2025: MAE=1.2115 D, Improvement=10.7%\n",
      "\n",
      "ğŸ“ˆ STATISTICAL SUMMARY:\n",
      "--------------------------------------------------\n",
      "  Baseline MAE:      1.4814 Â± 0.1503 D\n",
      "  Train MAE:         1.1668 Â± 0.0555 D\n",
      "  Test MAE:          1.3205 Â± 0.1738 D\n",
      "  Mean Improvement:  11.1 Â± 5.1%\n",
      "  Best seed:         123 (MAE=1.0289)\n",
      "  Worst seed:        456 (MAE=1.4725)\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "--------------------------------------------------\n",
      "  Mean overfit ratio: 14.1%\n",
      "  (Test MAE is 14.1% worse than Train MAE on average)\n",
      "  âœ… Good generalization - acceptable overfitting\n",
      "\n",
      "âœ… CONSENSUS PARAMETERS (averaged across seeds):\n",
      "--------------------------------------------------\n",
      "  nc_base              = +1.4365 Â± 0.0372\n",
      "  nc_cct_coef          = +0.0660 Â± 0.0622\n",
      "  k_index_base         = +1.4186 Â± 0.0360\n",
      "  k_index_cct_coef     = +0.0602 Â± 0.0656\n",
      "  acd_offset_base      = +2.8444 Â± 0.1287\n",
      "  acd_offset_cct_coef  = +0.7143 Â± 0.9863\n",
      "\n",
      "ğŸ’¡ ROBUSTNESS ANALYSIS:\n",
      "--------------------------------------------------\n",
      "âš ï¸ Moderate stability: CV=13.2% (some variation across seeds)\n",
      "\n",
      "ğŸ“Š Range of results: 1.0289 - 1.4725 D\n",
      "   This 0.4436 D range shows the impact of data split\n"
     ]
    }
   ],
   "source": [
    "# PARAMETER OPTIMIZATION WITH K-FOLD CROSS-VALIDATION - MULTI-SEED\n",
    "# =============================================\n",
    "# PURPOSE: Optimize SRK/T2 parameters with nested CV for robust validation\n",
    "# NOW WITH MULTIPLE SEEDS for statistical robustness\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARAMETER OPTIMIZATION WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CROSS-VALIDATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75% train, 25% test\")\n",
    "print(\"â€¢ Inner: 5-fold CV on training set\")\n",
    "print(\"â€¢ Results averaged across seeds for robustness\")\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mae_param(params, df_data):\n",
    "    \"\"\"Calculate MAE for parameter optimization\"\"\"\n",
    "    nc_base, nc_cct_coef, k_index_base, k_index_cct_coef, acd_offset_base, acd_offset_cct_coef = params\n",
    "    \n",
    "    predictions = []\n",
    "    for _, row in df_data.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        nc = nc_base + nc_cct_coef * cct_norm\n",
    "        k_index = k_index_base + k_index_cct_coef * cct_norm\n",
    "        acd_offset = acd_offset_base + acd_offset_cct_coef * cct_norm\n",
    "        \n",
    "        pred = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc,\n",
    "            k_index=k_index\n",
    "        )\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    mae = mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    return mae\n",
    "\n",
    "bounds_param = [\n",
    "    (1.20, 1.50),    # nc_base\n",
    "    (-0.20, 0.20),   # nc_cct_coef  \n",
    "    (1.20, 1.60),    # k_index_base\n",
    "    (-0.30, 0.30),   # k_index_cct_coef\n",
    "    (-3.0, 3.0),     # acd_offset_base\n",
    "    (-3.0, 3.0),     # acd_offset_cct_coef\n",
    "]\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_param = []\n",
    "seed_test_maes_param = []\n",
    "seed_train_maes_param = []  # NEW: Track training MAEs\n",
    "seed_baseline_maes_param = []\n",
    "seed_improvements_param = []\n",
    "seed_overfit_ratios_param = []  # NEW: Track overfitting\n",
    "\n",
    "df['K_avg'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT with current seed\n",
    "    X_train_param, X_test_param = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_param['K_avg'] = (X_train_param['Bio-Ks'] + X_train_param['Bio-Kf']) / 2\n",
    "    X_test_param['K_avg'] = (X_test_param['Bio-Ks'] + X_test_param['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_param)} train, {len(X_test_param)} test\")\n",
    "    \n",
    "    # INNER K-FOLD CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_params = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_param), 1):\n",
    "        fold_train = X_train_param.iloc[train_idx]\n",
    "        fold_val = X_train_param.iloc[val_idx]\n",
    "        \n",
    "        # Optimize on fold\n",
    "        result_fold = differential_evolution(\n",
    "            lambda p: calculate_mae_param(p, fold_train),\n",
    "            bounds_param,\n",
    "            maxiter=30,\n",
    "            seed=SEED + fold_num,\n",
    "            workers=1,\n",
    "            updating='deferred',\n",
    "            disp=False\n",
    "        )\n",
    "        \n",
    "        fold_params.append(result_fold.x)\n",
    "        val_mae = calculate_mae_param(result_fold.x, fold_val)\n",
    "        fold_maes.append(val_mae)\n",
    "    \n",
    "    # Average parameters from folds\n",
    "    avg_params = np.mean(fold_params, axis=0)\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    result_final = differential_evolution(\n",
    "        lambda p: calculate_mae_param(p, X_train_param),\n",
    "        bounds_param,\n",
    "        maxiter=50,\n",
    "        seed=SEED,\n",
    "        workers=1,\n",
    "        updating='deferred',\n",
    "        disp=False\n",
    "    )\n",
    "    \n",
    "    final_params = result_final.x\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    mae_train = calculate_mae_param(final_params, X_train_param)\n",
    "    \n",
    "    # TEST ON HOLDOUT\n",
    "    # Calculate baseline\n",
    "    X_test_param['SRKT2_Baseline'] = X_test_param.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Apply optimized parameters\n",
    "    predictions_test = []\n",
    "    for _, row in X_test_param.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        nc = final_params[0] + final_params[1] * cct_norm\n",
    "        k_index = final_params[2] + final_params[3] * cct_norm\n",
    "        acd_offset = final_params[4] + final_params[5] * cct_norm\n",
    "        \n",
    "        pred = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc,\n",
    "            k_index=k_index\n",
    "        )\n",
    "        predictions_test.append(pred)\n",
    "    \n",
    "    mae_baseline = np.abs(X_test_param['SRKT2_Baseline'] - X_test_param['PostOP Spherical Equivalent']).mean()\n",
    "    mae_optimized = mean_absolute_error(X_test_param['PostOP Spherical Equivalent'], predictions_test)\n",
    "    improvement = (mae_baseline - mae_optimized) / mae_baseline * 100\n",
    "    \n",
    "    print(f\"  Train MAE: {mae_train:.4f}, Test MAE: {mae_optimized:.4f}\")\n",
    "    print(f\"  Test: Baseline={mae_baseline:.4f}, Optimized={mae_optimized:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit_ratio = (mae_optimized - mae_train) / mae_train * 100\n",
    "    if overfit_ratio > 20:\n",
    "        print(f\"  âš ï¸ Overfitting detected: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    elif overfit_ratio > 10:\n",
    "        print(f\"  âš ï¸ Mild overfitting: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    else:\n",
    "        print(f\"  âœ… Good generalization: Test only {overfit_ratio:.1f}% worse than train\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_param.append(final_params)\n",
    "    seed_test_maes_param.append(mae_optimized)\n",
    "    seed_train_maes_param.append(mae_train)\n",
    "    seed_baseline_maes_param.append(mae_baseline)\n",
    "    seed_improvements_param.append(improvement)\n",
    "    seed_overfit_ratios_param.append(overfit_ratio)\n",
    "\n",
    "# MULTI-SEED SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARAMETER OPTIMIZATION - MULTI-SEED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"  Seed {seed:3}: MAE={seed_test_maes_param[i]:.4f} D, Improvement={seed_improvements_param[i]:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {np.mean(seed_baseline_maes_param):.4f} Â± {np.std(seed_baseline_maes_param):.4f} D\")\n",
    "print(f\"  Train MAE:         {np.mean(seed_train_maes_param):.4f} Â± {np.std(seed_train_maes_param):.4f} D\")\n",
    "print(f\"  Test MAE:          {np.mean(seed_test_maes_param):.4f} Â± {np.std(seed_test_maes_param):.4f} D\")\n",
    "print(f\"  Mean Improvement:  {np.mean(seed_improvements_param):.1f} Â± {np.std(seed_improvements_param):.1f}%\")\n",
    "print(f\"  Best seed:         {SEEDS[np.argmin(seed_test_maes_param)]} (MAE={min(seed_test_maes_param):.4f})\")\n",
    "print(f\"  Worst seed:        {SEEDS[np.argmax(seed_test_maes_param)]} (MAE={max(seed_test_maes_param):.4f})\")\n",
    "\n",
    "# OVERFITTING ANALYSIS\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_param):.1f}%\")\n",
    "print(f\"  (Test MAE is {np.mean(seed_overfit_ratios_param):.1f}% worse than Train MAE on average)\")\n",
    "\n",
    "if np.mean(seed_overfit_ratios_param) < 10:\n",
    "    print(\"  âœ… Excellent generalization - minimal overfitting\")\n",
    "elif np.mean(seed_overfit_ratios_param) < 20:\n",
    "    print(\"  âœ… Good generalization - acceptable overfitting\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Significant overfitting - consider regularization\")\n",
    "\n",
    "# Average parameters across seeds\n",
    "avg_params_all_seeds = np.mean(seed_results_param, axis=0)\n",
    "std_params_all_seeds = np.std(seed_results_param, axis=0)\n",
    "\n",
    "print(\"\\nâœ… CONSENSUS PARAMETERS (averaged across seeds):\")\n",
    "print(\"-\" * 50)\n",
    "param_names = ['nc_base', 'nc_cct_coef', 'k_index_base', 'k_index_cct_coef', 'acd_offset_base', 'acd_offset_cct_coef']\n",
    "for i, name in enumerate(param_names):\n",
    "    print(f\"  {name:20} = {avg_params_all_seeds[i]:+.4f} Â± {std_params_all_seeds[i]:.4f}\")\n",
    "\n",
    "# Store in global results dictionary\n",
    "multi_seed_results['parameter'] = {\n",
    "    'test_maes': seed_test_maes_param,\n",
    "    'train_maes': seed_train_maes_param,\n",
    "    'baseline_maes': seed_baseline_maes_param,\n",
    "    'improvements': seed_improvements_param,\n",
    "    'overfit_ratios': seed_overfit_ratios_param,\n",
    "    'mean_mae': np.mean(seed_test_maes_param),\n",
    "    'std_mae': np.std(seed_test_maes_param),\n",
    "    'mean_improvement': np.mean(seed_improvements_param)\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ’¡ ROBUSTNESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "mae_cv = np.std(seed_test_maes_param) / np.mean(seed_test_maes_param) * 100\n",
    "if mae_cv < 5:\n",
    "    print(f\"âœ… Excellent stability: CV={mae_cv:.1f}% (very consistent across seeds)\")\n",
    "elif mae_cv < 10:\n",
    "    print(f\"âœ… Good stability: CV={mae_cv:.1f}% (consistent across seeds)\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Moderate stability: CV={mae_cv:.1f}% (some variation across seeds)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Range of results: {min(seed_test_maes_param):.4f} - {max(seed_test_maes_param):.4f} D\")\n",
    "print(f\"   This {max(seed_test_maes_param)-min(seed_test_maes_param):.4f} D range shows the impact of data split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829090ggs0r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CV STRATEGY:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold CV on training\n",
      "â€¢ Find stable multiplicative factors across seeds\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 0.9016 Â± 0.1279 D\n",
      "  Final params: mâ‚€=-0.0379, mâ‚=-0.0153, mâ‚‚=-0.0378\n",
      "  Train MAE: 0.9068, Test MAE: 1.0063\n",
      "  Test: Baseline=1.4849, Optimized=1.0063\n",
      "  Improvement: 32.2%\n",
      "  âš ï¸ Mild overfitting: Test 11.0% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 0.9395 Â± 0.0938 D\n",
      "  Final params: mâ‚€=-0.0383, mâ‚=-0.0168, mâ‚‚=-0.0383\n",
      "  Train MAE: 0.8772, Test MAE: 1.0940\n",
      "  Test: Baseline=1.2755, Optimized=1.0940\n",
      "  Improvement: 14.2%\n",
      "  âš ï¸ Overfitting detected: Test 24.7% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 0.9122 Â± 0.2803 D\n",
      "  Final params: mâ‚€=-0.0386, mâ‚=-0.0133, mâ‚‚=-0.0385\n",
      "  Train MAE: 0.8928, Test MAE: 1.0463\n",
      "  Test: Baseline=1.6714, Optimized=1.0463\n",
      "  Improvement: 37.4%\n",
      "  âš ï¸ Mild overfitting: Test 17.2% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 0.9511 Â± 0.3201 D\n",
      "  Final params: mâ‚€=-0.1211, mâ‚=0.1059, mâ‚‚=-0.0342\n",
      "  Train MAE: 0.8972, Test MAE: 1.0182\n",
      "  Test: Baseline=1.6185, Optimized=1.0182\n",
      "  Improvement: 37.1%\n",
      "  âš ï¸ Mild overfitting: Test 13.5% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "  CV MAE: 0.9544 Â± 0.1953 D\n",
      "  Final params: mâ‚€=-0.0387, mâ‚=-0.0074, mâ‚‚=-0.0386\n",
      "  Train MAE: 0.9448, Test MAE: 0.8892\n",
      "  Test: Baseline=1.3566, Optimized=0.8892\n",
      "  Improvement: 34.5%\n",
      "  âœ… Good generalization: Test only -5.9% worse than train\n",
      "\n",
      "================================================================================\n",
      "MULTIPLICATIVE CORRECTION - MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed  42: MAE=1.0063 D, Improvement=32.2%\n",
      "  Seed 123: MAE=1.0940 D, Improvement=14.2%\n",
      "  Seed 456: MAE=1.0463 D, Improvement=37.4%\n",
      "  Seed 789: MAE=1.0182 D, Improvement=37.1%\n",
      "  Seed 2025: MAE=0.8892 D, Improvement=34.5%\n",
      "\n",
      "ğŸ“ˆ STATISTICAL SUMMARY:\n",
      "--------------------------------------------------\n",
      "  Baseline MAE:      1.4814 Â± 0.1503 D\n",
      "  Train MAE:         0.9037 Â± 0.0226 D\n",
      "  Test MAE:          1.0108 Â± 0.0679 D\n",
      "  Mean Improvement:  31.1 Â± 8.6%\n",
      "  Best seed:         2025 (MAE=0.8892)\n",
      "  Worst seed:        123 (MAE=1.0940)\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "--------------------------------------------------\n",
      "  Mean overfit ratio: 12.1%\n",
      "  (Test MAE is 12.1% worse than Train MAE on average)\n",
      "  âœ… Good generalization - acceptable overfitting\n",
      "\n",
      "âœ… CONSENSUS PARAMETERS (averaged across seeds):\n",
      "--------------------------------------------------\n",
      "  mâ‚€ (constant):     -0.0549 Â± 0.0331\n",
      "  mâ‚ (CCT coef):     +0.0106 Â± 0.0478\n",
      "  mâ‚‚ (ratio coef):   -0.0375 Â± 0.0017\n",
      "\n",
      "ğŸ“ CONSENSUS CORRECTION FORMULA:\n",
      "--------------------------------------------------\n",
      "Corrected_REF = Standard_SRK/T2 Ã— Correction_Factor\n",
      "Correction_Factor = 1 -0.0549 +0.0106Ã—CCT_norm -0.0375Ã—(CCT/AL)\n",
      "\n",
      "ğŸ’¡ ROBUSTNESS ANALYSIS:\n",
      "--------------------------------------------------\n",
      "âœ… Good stability: CV=6.7% (consistent across seeds)\n",
      "\n",
      "ğŸ“Š Range of results: 0.8892 - 1.0940 D\n",
      "   This 0.2048 D range shows the impact of data split\n",
      "\n",
      "ğŸ“Š Parameter consistency across seeds:\n",
      "  mâ‚€: min=-0.1211, max=-0.0379, range=0.0832\n",
      "  mâ‚: min=-0.0168, max=0.1059, range=0.1227\n",
      "  mâ‚‚: min=-0.0386, max=-0.0342, range=0.0045\n"
     ]
    }
   ],
   "source": [
    "# MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED\n",
    "# ====================================\n",
    "# PURPOSE: Multiplicative correction with nested CV for robust validation\n",
    "# NOW WITH MULTIPLE SEEDS for statistical robustness\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CV STRATEGY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold CV on training\")\n",
    "print(\"â€¢ Find stable multiplicative factors across seeds\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "def multiplicative_objective(params, df_data):\n",
    "    \"\"\"Objective function for multiplicative correction\"\"\"\n",
    "    m0, m1, m2 = params\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for _, row in df_data.iterrows():\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        correction_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "        corrected_pred = base_pred * correction_factor\n",
    "        \n",
    "        predictions.append(corrected_pred)\n",
    "        actuals.append(row['PostOP Spherical Equivalent'])\n",
    "    \n",
    "    return mean_absolute_error(actuals, predictions)\n",
    "\n",
    "x0_mult = [0, 0, 0]\n",
    "bounds_mult = [(-0.5, 0.5), (-0.5, 0.5), (-0.5, 0.5)]\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_mult = []\n",
    "seed_test_maes_mult = []\n",
    "seed_train_maes_mult = []  # NEW: Track training MAEs\n",
    "seed_baseline_maes_mult = []\n",
    "seed_improvements_mult = []\n",
    "seed_overfit_ratios_mult = []  # NEW: Track overfitting\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT with current seed\n",
    "    X_train_mult, X_test_mult = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_mult['K_avg'] = (X_train_mult['Bio-Ks'] + X_train_mult['Bio-Kf']) / 2\n",
    "    X_test_mult['K_avg'] = (X_test_mult['Bio-Ks'] + X_test_mult['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_mult)} train, {len(X_test_mult)} test\")\n",
    "    \n",
    "    # Calculate baseline SRK/T2 for all data\n",
    "    for dataset in [X_train_mult, X_test_mult]:\n",
    "        dataset['SRKT2_Prediction'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    # INNER K-FOLD CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_params = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_mult), 1):\n",
    "        fold_train = X_train_mult.iloc[train_idx]\n",
    "        fold_val = X_train_mult.iloc[val_idx]\n",
    "        \n",
    "        # Optimize on fold training\n",
    "        result_fold = minimize(\n",
    "            lambda p: multiplicative_objective(p, fold_train),\n",
    "            x0_mult,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds_mult\n",
    "        )\n",
    "        \n",
    "        fold_params.append(result_fold.x)\n",
    "        val_mae = multiplicative_objective(result_fold.x, fold_val)\n",
    "        fold_maes.append(val_mae)\n",
    "    \n",
    "    # Average across folds\n",
    "    avg_params = np.mean(fold_params, axis=0)\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    result_mult = minimize(\n",
    "        lambda p: multiplicative_objective(p, X_train_mult),\n",
    "        x0_mult,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds_mult\n",
    "    )\n",
    "    m0_opt, m1_opt, m2_opt = result_mult.x\n",
    "    \n",
    "    print(f\"  Final params: mâ‚€={m0_opt:.4f}, mâ‚={m1_opt:.4f}, mâ‚‚={m2_opt:.4f}\")\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    mae_train = multiplicative_objective([m0_opt, m1_opt, m2_opt], X_train_mult)\n",
    "    \n",
    "    # TEST ON HOLDOUT\n",
    "    predictions_mult_test = []\n",
    "    for _, row in X_test_mult.iterrows():\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        correction_factor = 1 + m0_opt + m1_opt * cct_norm + m2_opt * cct_ratio\n",
    "        corrected_pred = base_pred * correction_factor\n",
    "        predictions_mult_test.append(corrected_pred)\n",
    "    \n",
    "    mae_baseline = np.abs(X_test_mult['SRKT2_Prediction'] - X_test_mult['PostOP Spherical Equivalent']).mean()\n",
    "    mae_optimized = mean_absolute_error(X_test_mult['PostOP Spherical Equivalent'], predictions_mult_test)\n",
    "    improvement = (mae_baseline - mae_optimized) / mae_baseline * 100\n",
    "    \n",
    "    print(f\"  Train MAE: {mae_train:.4f}, Test MAE: {mae_optimized:.4f}\")\n",
    "    print(f\"  Test: Baseline={mae_baseline:.4f}, Optimized={mae_optimized:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit_ratio = (mae_optimized - mae_train) / mae_train * 100\n",
    "    if overfit_ratio > 20:\n",
    "        print(f\"  âš ï¸ Overfitting detected: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    elif overfit_ratio > 10:\n",
    "        print(f\"  âš ï¸ Mild overfitting: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    else:\n",
    "        print(f\"  âœ… Good generalization: Test only {overfit_ratio:.1f}% worse than train\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_mult.append([m0_opt, m1_opt, m2_opt])\n",
    "    seed_test_maes_mult.append(mae_optimized)\n",
    "    seed_train_maes_mult.append(mae_train)\n",
    "    seed_baseline_maes_mult.append(mae_baseline)\n",
    "    seed_improvements_mult.append(improvement)\n",
    "    seed_overfit_ratios_mult.append(overfit_ratio)\n",
    "\n",
    "# MULTI-SEED SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIPLICATIVE CORRECTION - MULTI-SEED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"  Seed {seed:3}: MAE={seed_test_maes_mult[i]:.4f} D, Improvement={seed_improvements_mult[i]:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {np.mean(seed_baseline_maes_mult):.4f} Â± {np.std(seed_baseline_maes_mult):.4f} D\")\n",
    "print(f\"  Train MAE:         {np.mean(seed_train_maes_mult):.4f} Â± {np.std(seed_train_maes_mult):.4f} D\")\n",
    "print(f\"  Test MAE:          {np.mean(seed_test_maes_mult):.4f} Â± {np.std(seed_test_maes_mult):.4f} D\")\n",
    "print(f\"  Mean Improvement:  {np.mean(seed_improvements_mult):.1f} Â± {np.std(seed_improvements_mult):.1f}%\")\n",
    "print(f\"  Best seed:         {SEEDS[np.argmin(seed_test_maes_mult)]} (MAE={min(seed_test_maes_mult):.4f})\")\n",
    "print(f\"  Worst seed:        {SEEDS[np.argmax(seed_test_maes_mult)]} (MAE={max(seed_test_maes_mult):.4f})\")\n",
    "\n",
    "# OVERFITTING ANALYSIS\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_mult):.1f}%\")\n",
    "print(f\"  (Test MAE is {np.mean(seed_overfit_ratios_mult):.1f}% worse than Train MAE on average)\")\n",
    "\n",
    "if np.mean(seed_overfit_ratios_mult) < 10:\n",
    "    print(\"  âœ… Excellent generalization - minimal overfitting\")\n",
    "elif np.mean(seed_overfit_ratios_mult) < 20:\n",
    "    print(\"  âœ… Good generalization - acceptable overfitting\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Significant overfitting - consider regularization\")\n",
    "\n",
    "# Average parameters across seeds\n",
    "avg_params_all_seeds = np.mean(seed_results_mult, axis=0)\n",
    "std_params_all_seeds = np.std(seed_results_mult, axis=0)\n",
    "\n",
    "print(\"\\nâœ… CONSENSUS PARAMETERS (averaged across seeds):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  mâ‚€ (constant):     {avg_params_all_seeds[0]:+.4f} Â± {std_params_all_seeds[0]:.4f}\")\n",
    "print(f\"  mâ‚ (CCT coef):     {avg_params_all_seeds[1]:+.4f} Â± {std_params_all_seeds[1]:.4f}\")\n",
    "print(f\"  mâ‚‚ (ratio coef):   {avg_params_all_seeds[2]:+.4f} Â± {std_params_all_seeds[2]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ CONSENSUS CORRECTION FORMULA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Corrected_REF = Standard_SRK/T2 Ã— Correction_Factor\")\n",
    "print(f\"Correction_Factor = 1 {avg_params_all_seeds[0]:+.4f} {avg_params_all_seeds[1]:+.4f}Ã—CCT_norm {avg_params_all_seeds[2]:+.4f}Ã—(CCT/AL)\")\n",
    "\n",
    "# Store in global results dictionary\n",
    "multi_seed_results['multiplicative'] = {\n",
    "    'test_maes': seed_test_maes_mult,\n",
    "    'train_maes': seed_train_maes_mult,\n",
    "    'baseline_maes': seed_baseline_maes_mult,\n",
    "    'improvements': seed_improvements_mult,\n",
    "    'overfit_ratios': seed_overfit_ratios_mult,\n",
    "    'mean_mae': np.mean(seed_test_maes_mult),\n",
    "    'std_mae': np.std(seed_test_maes_mult),\n",
    "    'mean_improvement': np.mean(seed_improvements_mult)\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ’¡ ROBUSTNESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "mae_cv = np.std(seed_test_maes_mult) / np.mean(seed_test_maes_mult) * 100\n",
    "if mae_cv < 5:\n",
    "    print(f\"âœ… Excellent stability: CV={mae_cv:.1f}% (very consistent across seeds)\")\n",
    "elif mae_cv < 10:\n",
    "    print(f\"âœ… Good stability: CV={mae_cv:.1f}% (consistent across seeds)\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Moderate stability: CV={mae_cv:.1f}% (some variation across seeds)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Range of results: {min(seed_test_maes_mult):.4f} - {max(seed_test_maes_mult):.4f} D\")\n",
    "print(f\"   This {max(seed_test_maes_mult)-min(seed_test_maes_mult):.4f} D range shows the impact of data split\")\n",
    "\n",
    "# Parameter consistency check\n",
    "print(f\"\\nğŸ“Š Parameter consistency across seeds:\")\n",
    "for i, param_name in enumerate(['mâ‚€', 'mâ‚', 'mâ‚‚']):\n",
    "    param_values = [p[i] for p in seed_results_mult]\n",
    "    print(f\"  {param_name}: min={min(param_values):.4f}, max={max(param_values):.4f}, range={max(param_values)-min(param_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a07c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPPORT VECTOR REGRESSION (SVR) WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 1 Results:\n",
      "    Test MAE: 0.921 Â± 0.194 D\n",
      "    Train MAE: 0.698 D\n",
      "    Overfit ratio: 1.319\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 2 Results:\n",
      "    Test MAE: 0.918 Â± 0.159 D\n",
      "    Train MAE: 0.681 D\n",
      "    Overfit ratio: 1.348\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 3 Results:\n",
      "    Test MAE: 0.917 Â± 0.200 D\n",
      "    Train MAE: 0.674 D\n",
      "    Overfit ratio: 1.359\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 4 Results:\n",
      "    Test MAE: 0.882 Â± 0.108 D\n",
      "    Train MAE: 0.696 D\n",
      "    Overfit ratio: 1.267\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 5 Results:\n",
      "    Test MAE: 0.895 Â± 0.214 D\n",
      "    Train MAE: 0.663 D\n",
      "    Overfit ratio: 1.351\n",
      "\n",
      "================================================================================\n",
      "SVR MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Across 5 seeds with 5-fold CV (25 total evaluations):\n",
      "Test MAE: 0.907 Â± 0.180 D\n",
      "Train MAE: 0.683 Â± 0.088 D\n",
      "Baseline MAE: 1.358 Â± 0.225 D\n",
      "Overfit Ratio: 1.328\n",
      "\n",
      "Improvement over baseline:\n",
      "  Relative: 33.2%\n",
      "  Absolute: 0.451 D\n",
      "\n",
      "Most frequent hyperparameters:\n",
      "  C: 0.5(12), 1.0(7), 2.0(6)\n",
      "  Îµ: 0.2(14), 0.05(9), 0.1(2)\n",
      "================================================================================\n",
      "SVR method completed - results stored in _svr variables\n",
      "Both multiplicative and SVR are now available for comparison\n",
      "Results stored for final comparison in cell 11\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# SUPPORT VECTOR REGRESSION (SVR) - REPLACEMENT FOR MULTIPLICATIVE\n",
    "# =================================================================\n",
    "# PURPOSE: Test SVR as alternative to multiplicative correction\n",
    "# Based on comprehensive testing showing 6.7% improvement\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUPPORT VECTOR REGRESSION (SVR) WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features for SVR\n",
    "def prepare_svr_features(df):\n",
    "    X = pd.DataFrame()\n",
    "    X['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "    X['AL'] = df['Bio-AL']\n",
    "    X['ACD'] = df['Bio-ACD']\n",
    "    X['K_mean'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "    X['CCT_AL'] = df['CCT'] / df['Bio-AL']\n",
    "    return X\n",
    "\n",
    "# Store results for multiple seeds (using SEEDS from first cell)\n",
    "seed_results_svr = []\n",
    "seed_test_maes_svr = []\n",
    "seed_train_maes_svr = []\n",
    "seed_baseline_maes_svr = []\n",
    "seed_improvements_svr = []\n",
    "seed_overfit_ratios_svr = []\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    np.random.seed(SEED)\n",
    "    print(f\"\\nSeed {seed+1}/{len(SEEDS)}:\")\n",
    "    \n",
    "    # Outer CV loop (using same structure as other methods)\n",
    "    kf_outer = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    fold_results = []\n",
    "    test_maes = []\n",
    "    train_maes = []\n",
    "    baseline_maes = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf_outer.split(df)):\n",
    "        # Split data (using 'df' DataFrame from first cell)\n",
    "        df_train = df.iloc[train_idx].copy()\n",
    "        df_test = df.iloc[test_idx].copy()\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train = prepare_svr_features(df_train)\n",
    "        X_test = prepare_svr_features(df_test)\n",
    "        y_train = df_train['PostOP Spherical Equivalent'].values\n",
    "        y_test = df_test['PostOP Spherical Equivalent'].values\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning (simplified grid for speed)\n",
    "        best_params = None\n",
    "        best_val_mae = float('inf')\n",
    "        \n",
    "        # Reduced grid search for efficiency\n",
    "        param_grid = {\n",
    "            'C': [0.5, 1.0, 2.0],\n",
    "            'epsilon': [0.05, 0.1, 0.2]\n",
    "        }\n",
    "        \n",
    "        kf_inner = KFold(n_splits=3, shuffle=True, random_state=SEED*100+fold)\n",
    "        \n",
    "        for C in param_grid['C']:\n",
    "            for epsilon in param_grid['epsilon']:\n",
    "                val_maes = []\n",
    "                \n",
    "                for train_inner_idx, val_idx in kf_inner.split(X_train_scaled):\n",
    "                    X_train_inner = X_train_scaled[train_inner_idx]\n",
    "                    y_train_inner = y_train[train_inner_idx]\n",
    "                    X_val = X_train_scaled[val_idx]\n",
    "                    y_val = y_train[val_idx]\n",
    "                    \n",
    "                    # Train SVR\n",
    "                    model = SVR(kernel='rbf', C=C, epsilon=epsilon, gamma='scale')\n",
    "                    model.fit(X_train_inner, y_train_inner)\n",
    "                    \n",
    "                    # Validate\n",
    "                    y_pred_val = model.predict(X_val)\n",
    "                    val_maes.append(mean_absolute_error(y_val, y_pred_val))\n",
    "                \n",
    "                mean_val_mae = np.mean(val_maes)\n",
    "                if mean_val_mae < best_val_mae:\n",
    "                    best_val_mae = mean_val_mae\n",
    "                    best_params = {'C': C, 'epsilon': epsilon}\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        final_model = SVR(kernel='rbf', C=best_params['C'], \n",
    "                         epsilon=best_params['epsilon'], gamma='scale')\n",
    "        final_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = final_model.predict(X_train_scaled)\n",
    "        y_pred_test = final_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate MAEs\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Baseline MAE (using SRKT2_Prediction from data preparation)\n",
    "        if 'SRKT2_Prediction' in df_test.columns:\n",
    "            baseline_pred = df_test['SRKT2_Prediction'].values\n",
    "        else:\n",
    "            # Simple SRK/T2 approximation if not available\n",
    "            baseline_pred = 118.4 - 2.5 * df_test['Bio-AL'] - 0.9 * X_test['K_mean']\n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "        \n",
    "        train_maes.append(train_mae)\n",
    "        test_maes.append(test_mae)\n",
    "        baseline_maes.append(baseline_mae)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'baseline_mae': baseline_mae,\n",
    "            'improvement': (baseline_mae - test_mae) / baseline_mae * 100 if baseline_mae > 0 else 0,\n",
    "            'best_C': best_params['C'],\n",
    "            'best_epsilon': best_params['epsilon']\n",
    "        })\n",
    "    \n",
    "    # Store seed results\n",
    "    seed_results_svr.append(fold_results)\n",
    "    seed_test_maes_svr.append(test_maes)\n",
    "    seed_train_maes_svr.append(train_maes)\n",
    "    seed_baseline_maes_svr.append(baseline_maes)\n",
    "    \n",
    "    # Calculate improvements for this seed\n",
    "    improvements = []\n",
    "    for j in range(len(test_maes)):\n",
    "        if baseline_maes[j] > 0:\n",
    "            improvement = (baseline_maes[j] - test_maes[j]) / baseline_maes[j] * 100\n",
    "        else:\n",
    "            improvement = 0\n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    # Calculate overfit ratio for this seed\n",
    "    overfit_ratio = np.mean(test_maes) / np.mean(train_maes) if np.mean(train_maes) > 0 else 1.0\n",
    "    \n",
    "    # Store for this seed\n",
    "    seed_improvements_svr.append(improvements)\n",
    "    seed_overfit_ratios_svr.append(overfit_ratio)\n",
    "    \n",
    "    # Calculate seed statistics\n",
    "    seed_test_mean = np.mean(test_maes)\n",
    "    seed_test_std = np.std(test_maes)\n",
    "    seed_train_mean = np.mean(train_maes)\n",
    "    \n",
    "    print(f\"  Seed {seed_idx} Results:\")\n",
    "    print(f\"    Test MAE: {seed_test_mean:.3f} Â± {seed_test_std:.3f} D\")\n",
    "    print(f\"    Train MAE: {seed_train_mean:.3f} D\")\n",
    "    print(f\"    Overfit ratio: {seed_test_mean/seed_train_mean:.3f}\")\n",
    "\n",
    "# Summary statistics (consistent with other methods)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SVR MULTI-SEED SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall statistics\n",
    "overall_test_maes = [mae for seed_maes in seed_test_maes_svr for mae in seed_maes]\n",
    "overall_train_maes = [mae for seed_maes in seed_train_maes_svr for mae in seed_maes]\n",
    "overall_baseline_maes = [mae for seed_maes in seed_baseline_maes_svr for mae in seed_maes]\n",
    "\n",
    "print(f\"\\nAcross {len(SEEDS)} seeds with 5-fold CV ({len(overall_test_maes)} total evaluations):\")\n",
    "print(f\"Test MAE: {np.mean(overall_test_maes):.3f} Â± {np.std(overall_test_maes):.3f} D\")\n",
    "print(f\"Train MAE: {np.mean(overall_train_maes):.3f} Â± {np.std(overall_train_maes):.3f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(overall_baseline_maes):.3f} Â± {np.std(overall_baseline_maes):.3f} D\")\n",
    "print(f\"Overfit Ratio: {np.mean(overall_test_maes) / np.mean(overall_train_maes):.3f}\")\n",
    "\n",
    "if np.mean(overall_baseline_maes) > 0:\n",
    "    improvement = (np.mean(overall_baseline_maes) - np.mean(overall_test_maes)) / np.mean(overall_baseline_maes) * 100\n",
    "    absolute_improvement = np.mean(overall_baseline_maes) - np.mean(overall_test_maes)\n",
    "    print(f\"\\nImprovement over baseline:\")\n",
    "    print(f\"  Relative: {improvement:.1f}%\")\n",
    "    print(f\"  Absolute: {absolute_improvement:.3f} D\")\n",
    "\n",
    "# Hyperparameter analysis\n",
    "print(\"\\nMost frequent hyperparameters:\")\n",
    "all_Cs = [r['best_C'] for seed in seed_results_svr for r in seed]\n",
    "all_epsilons = [r['best_epsilon'] for seed in seed_results_svr for r in seed]\n",
    "from collections import Counter\n",
    "c_counts = Counter(all_Cs).most_common(3)\n",
    "eps_counts = Counter(all_epsilons).most_common(3)\n",
    "print(f\"  C: {', '.join([f'{val}({cnt})' for val, cnt in c_counts])}\")\n",
    "print(f\"  Îµ: {', '.join([f'{val}({cnt})' for val, cnt in eps_counts])}\")\n",
    "\n",
    "# Store results in format compatible with final comparison\n",
    "# This replaces the multiplicative method results\n",
    "\n",
    "# Store results in _mult variables for compatibility with comparison cell\n",
    "# Keep SVR results separate\n",
    "# seed_test_maes_mult = seed_test_maes_svr  # Don't overwrite\n",
    "# seed_train_maes_mult = seed_train_maes_svr\n",
    "# seed_baseline_maes_mult = seed_baseline_maes_svr\n",
    "# seed_improvements_mult = seed_improvements_svr\n",
    "# seed_overfit_ratios_mult = seed_overfit_ratios_svr\n",
    "\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"SVR method completed - results stored in _svr variables\")\n",
    "print(\"Both multiplicative and SVR are now available for comparison\")\n",
    "print(\"Results stored for final comparison in cell 11\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce5d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALTERNATIVE ML METHODS FOR IOL CALCULATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– TESTING ADVANCED ML ALGORITHMS:\n",
      "--------------------------------------------------\n",
      "â€¢ Random Forest: Tree ensemble with bagging\n",
      "â€¢ XGBoost: Gradient boosting (state-of-the-art)\n",
      "â€¢ Gaussian Process: Probabilistic approach with uncertainty\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS FOR ML METHODS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "TESTING: Random Forest\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best RF params: trees=50, depth=3\n",
      "  Test MAE: 0.9641 D\n",
      "  Train MAE: 0.6944 D\n",
      "  Improvement: 35.1%\n",
      "  Overfit ratio: 0.720\n",
      "  Top features: K_mean, IOL Power, Bio-Kf\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best RF params: trees=200, depth=3\n",
      "  Test MAE: 1.1058 D\n",
      "  Train MAE: 0.6816 D\n",
      "  Improvement: 13.3%\n",
      "  Overfit ratio: 0.616\n",
      "  Top features: CCT_AL_ratio, AL_ACD_ratio, K_mean\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best RF params: trees=100, depth=3\n",
      "  Test MAE: 1.0618 D\n",
      "  Train MAE: 0.6469 D\n",
      "  Improvement: 36.5%\n",
      "  Overfit ratio: 0.609\n",
      "  Top features: CCT_AL_ratio, K_mean, Bio-AL\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best RF params: trees=200, depth=7\n",
      "  Test MAE: 1.0031 D\n",
      "  Train MAE: 0.5556 D\n",
      "  Improvement: 38.0%\n",
      "  Overfit ratio: 0.554\n",
      "  Top features: IOL Power, Bio-AL, K_mean\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best RF params: trees=50, depth=3\n",
      "  Test MAE: 0.8100 D\n",
      "  Train MAE: 0.7468 D\n",
      "  Improvement: 40.3%\n",
      "  Overfit ratio: 0.922\n",
      "  Top features: K_mean, CCT_AL_ratio, AL_ACD_ratio\n",
      "\n",
      "========================================\n",
      "Random Forest - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9890 Â± 0.1018 D\n",
      "  Best MAE: 0.8100 D\n",
      "  Worst MAE: 1.1058 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 32.6 Â± 9.8%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.684\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "========================================\n",
      "TESTING: XGBoost\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 0.9828 D\n",
      "  Train MAE: 0.8097 D\n",
      "  Improvement: 33.8%\n",
      "  Overfit ratio: 0.824\n",
      "  Top features: Bio-Kf, AL_ACD_ratio, Bio-AL\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 1.0093 D\n",
      "  Train MAE: 0.8136 D\n",
      "  Improvement: 20.9%\n",
      "  Overfit ratio: 0.806\n",
      "  Top features: Bio-Kf, Bio-Ks, A-Constant\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 0.9547 D\n",
      "  Train MAE: 0.8059 D\n",
      "  Improvement: 42.9%\n",
      "  Overfit ratio: 0.844\n",
      "  Top features: Bio-Ks, CCT_norm, K_mean\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 0.9670 D\n",
      "  Train MAE: 0.7952 D\n",
      "  Improvement: 40.3%\n",
      "  Overfit ratio: 0.822\n",
      "  Top features: IOL Power, CCT, K_mean\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 0.8627 D\n",
      "  Train MAE: 0.8614 D\n",
      "  Improvement: 36.4%\n",
      "  Overfit ratio: 0.999\n",
      "  Top features: IOL Power, CCT, K_mean\n",
      "\n",
      "========================================\n",
      "XGBoost - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9553 Â± 0.0498 D\n",
      "  Best MAE: 0.8627 D\n",
      "  Worst MAE: 1.0093 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 34.8 Â± 7.6%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.859\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "========================================\n",
      "TESTING: Gaussian Process\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.190 D\n",
      "  Test MAE: 1.0061 D\n",
      "  Train MAE: 0.0801 D\n",
      "  Improvement: 32.2%\n",
      "  Overfit ratio: 0.080\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best GP kernel: RBF\n",
      "  Mean prediction uncertainty: 1.218 D\n",
      "  Test MAE: 1.0208 D\n",
      "  Train MAE: 0.0797 D\n",
      "  Improvement: 20.0%\n",
      "  Overfit ratio: 0.078\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.215 D\n",
      "  Test MAE: 0.9711 D\n",
      "  Train MAE: 0.0811 D\n",
      "  Improvement: 41.9%\n",
      "  Overfit ratio: 0.083\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.281 D\n",
      "  Test MAE: 0.9627 D\n",
      "  Train MAE: 0.0814 D\n",
      "  Improvement: 40.5%\n",
      "  Overfit ratio: 0.085\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.200 D\n",
      "  Test MAE: 0.9036 D\n",
      "  Train MAE: 0.0832 D\n",
      "  Improvement: 33.4%\n",
      "  Overfit ratio: 0.092\n",
      "\n",
      "========================================\n",
      "Gaussian Process - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9729 Â± 0.0407 D\n",
      "  Best MAE: 0.9036 D\n",
      "  Worst MAE: 1.0208 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 33.6 Â± 7.8%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.084\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "================================================================================\n",
      "ML METHODS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š RANKING BY TEST MAE:\n",
      "--------------------------------------------------\n",
      "1. XGBoost             : 0.9553 Â± 0.0498 D\n",
      "                         Improvement: 34.8%\n",
      "                         Overfit ratio: 0.859\n",
      "2. Gaussian Process    : 0.9729 Â± 0.0407 D\n",
      "                         Improvement: 33.6%\n",
      "                         Overfit ratio: 0.084\n",
      "3. Random Forest       : 0.9890 Â± 0.1018 D\n",
      "                         Improvement: 32.6%\n",
      "                         Overfit ratio: 0.684\n",
      "\n",
      "ğŸ“Š COMPARISON WITH SVR:\n",
      "--------------------------------------------------\n",
      "Random Forest: 2.2% BETTER than SVR\n",
      "XGBoost: 5.5% BETTER than SVR\n",
      "Gaussian Process: 3.8% BETTER than SVR\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION:\n",
      "================================================================================\n",
      "âœ… XGBoost outperforms SVR by 5.5%!\n",
      "   Consider using XGBoost instead of SVR\n",
      "\n",
      "Key insights:\n",
      "â€¢ Tree-based methods (RF, XGBoost) capture feature interactions well\n",
      "â€¢ Gaussian Process provides uncertainty estimates (useful clinically)\n",
      "â€¢ All methods benefit from feature engineering (CCT_norm, ratios)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE ML METHODS - RANDOM FOREST, XGBOOST, AND GAUSSIAN PROCESS\n",
    "# ======================================================================\n",
    "# PURPOSE: Test promising ML alternatives to SVR for IOL calculation\n",
    "# Methods: Random Forest, XGBoost, and Gaussian Process Regression\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALTERNATIVE ML METHODS FOR IOL CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¤– TESTING ADVANCED ML ALGORITHMS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Random Forest: Tree ensemble with bagging\")\n",
    "print(\"â€¢ XGBoost: Gradient boosting (state-of-the-art)\")\n",
    "print(\"â€¢ Gaussian Process: Probabilistic approach with uncertainty\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing XGBoost (may not be installed)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ XGBoost not installed. Install with: pip install xgboost\")\n",
    "    HAS_XGBOOST = False\n",
    "\n",
    "# Store results for each method\n",
    "ml_methods_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS FOR ML METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test each ML method\n",
    "methods_to_test = ['Random Forest', 'XGBoost', 'Gaussian Process']\n",
    "\n",
    "for method_name in methods_to_test:\n",
    "    if method_name == 'XGBoost' and not HAS_XGBOOST:\n",
    "        print(f\"\\nâ­ï¸ Skipping {method_name} (not installed)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TESTING: {method_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Store results for this method\n",
    "    seed_test_maes = []\n",
    "    seed_train_maes = []\n",
    "    seed_baseline_maes = []\n",
    "    seed_improvements = []\n",
    "    seed_overfit_ratios = []\n",
    "    best_params_list = []\n",
    "    \n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\nSeed {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_ml, X_test_ml = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = ['CCT', 'Bio-AL', 'Bio-ACD', 'Bio-Ks', 'Bio-Kf', 'IOL Power', 'A-Constant']\n",
    "        X_train_features = X_train_ml[feature_cols].copy()\n",
    "        X_test_features = X_test_ml[feature_cols].copy()\n",
    "        \n",
    "        # Add derived features\n",
    "        X_train_features['K_mean'] = (X_train_ml['Bio-Ks'] + X_train_ml['Bio-Kf']) / 2\n",
    "        X_train_features['CCT_norm'] = (X_train_ml['CCT'] - 600) / 100\n",
    "        X_train_features['CCT_AL_ratio'] = X_train_ml['CCT'] / X_train_ml['Bio-AL']\n",
    "        X_train_features['AL_ACD_ratio'] = X_train_ml['Bio-AL'] / X_train_ml['Bio-ACD']\n",
    "        \n",
    "        X_test_features['K_mean'] = (X_test_ml['Bio-Ks'] + X_test_ml['Bio-Kf']) / 2\n",
    "        X_test_features['CCT_norm'] = (X_test_ml['CCT'] - 600) / 100\n",
    "        X_test_features['CCT_AL_ratio'] = X_test_ml['CCT'] / X_test_ml['Bio-AL']\n",
    "        X_test_features['AL_ACD_ratio'] = X_test_ml['Bio-AL'] / X_test_ml['Bio-ACD']\n",
    "        \n",
    "        # Target\n",
    "        y_train = X_train_ml['PostOP Spherical Equivalent'].values\n",
    "        y_test = X_test_ml['PostOP Spherical Equivalent'].values\n",
    "        \n",
    "        # Calculate baseline\n",
    "        X_train_ml['K_avg'] = (X_train_ml['Bio-Ks'] + X_train_ml['Bio-Kf']) / 2\n",
    "        X_test_ml['K_avg'] = (X_test_ml['Bio-Ks'] + X_test_ml['Bio-Kf']) / 2\n",
    "        \n",
    "        for dataset in [X_train_ml, X_test_ml]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_ml['PostOP Spherical Equivalent'], \n",
    "                                          X_test_ml['SRKT2_Baseline'])\n",
    "        \n",
    "        # Setup cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # Initialize model based on method\n",
    "        if method_name == 'Random Forest':\n",
    "            # Random Forest with hyperparameter tuning\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            base_model = RandomForestRegressor(random_state=SEED)\n",
    "            \n",
    "            # Quick grid search with 3-fold CV (faster)\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model, \n",
    "                param_grid, \n",
    "                cv=3, \n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train_features, y_train)\n",
    "            model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            print(f\"  Best RF params: trees={best_params['n_estimators']}, depth={best_params['max_depth']}\")\n",
    "            \n",
    "        elif method_name == 'XGBoost':\n",
    "            # XGBoost with hyperparameter tuning\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [2, 3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'subsample': [0.6, 0.8, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "            }\n",
    "            base_model = xgb.XGBRegressor(random_state=SEED, objective='reg:squarederror')\n",
    "            \n",
    "            # Quick grid search\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model,\n",
    "                param_grid,\n",
    "                cv=3,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train_features, y_train)\n",
    "            model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            print(f\"  Best XGB params: trees={best_params['n_estimators']}, depth={best_params['max_depth']}, lr={best_params['learning_rate']}\")\n",
    "            \n",
    "        elif method_name == 'Gaussian Process':\n",
    "            # Gaussian Process with different kernels\n",
    "            # Note: GP doesn't scale well, so we'll use a subset of features\n",
    "            important_features = ['CCT_norm', 'CCT_AL_ratio', 'K_mean', 'Bio-AL', 'Bio-ACD']\n",
    "            X_train_gp = X_train_features[important_features]\n",
    "            X_test_gp = X_test_features[important_features]\n",
    "            \n",
    "            # Standardize for GP\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_gp)\n",
    "            X_test_scaled = scaler.transform(X_test_gp)\n",
    "            \n",
    "            # Try different kernels\n",
    "            kernels = [\n",
    "                RBF(length_scale=1.0),\n",
    "                Matern(length_scale=1.0, nu=1.5),\n",
    "                RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "            ]\n",
    "            \n",
    "            best_kernel = None\n",
    "            best_kernel_mae = float('inf')\n",
    "            \n",
    "            for kernel in kernels:\n",
    "                gpr = GaussianProcessRegressor(\n",
    "                    kernel=kernel,\n",
    "                    alpha=0.1,  # Noise level\n",
    "                    random_state=SEED,\n",
    "                    normalize_y=True\n",
    "                )\n",
    "                \n",
    "                # Quick CV to select best kernel\n",
    "                cv_maes = []\n",
    "                for train_idx, val_idx in kf.split(X_train_scaled):\n",
    "                    gpr.fit(X_train_scaled[train_idx], y_train[train_idx])\n",
    "                    pred = gpr.predict(X_train_scaled[val_idx])\n",
    "                    cv_maes.append(mean_absolute_error(y_train[val_idx], pred))\n",
    "                \n",
    "                mean_cv_mae = np.mean(cv_maes)\n",
    "                if mean_cv_mae < best_kernel_mae:\n",
    "                    best_kernel_mae = mean_cv_mae\n",
    "                    best_kernel = kernel\n",
    "            \n",
    "            # Train final model with best kernel\n",
    "            model = GaussianProcessRegressor(\n",
    "                kernel=best_kernel,\n",
    "                alpha=0.1,\n",
    "                random_state=SEED,\n",
    "                normalize_y=True\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            print(f\"  Best GP kernel: {best_kernel.__class__.__name__}\")\n",
    "            best_params = {'kernel': best_kernel.__class__.__name__}\n",
    "        \n",
    "        # Make predictions\n",
    "        if method_name == 'Gaussian Process':\n",
    "            # GP needs scaled features\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "            y_pred_train = model.predict(X_train_scaled)\n",
    "            \n",
    "            # Also get uncertainty estimates (unique to GP!)\n",
    "            y_pred_test_std = model.predict(X_test_scaled, return_std=True)[1]\n",
    "            mean_uncertainty = np.mean(y_pred_test_std)\n",
    "            print(f\"  Mean prediction uncertainty: {mean_uncertainty:.3f} D\")\n",
    "        else:\n",
    "            # RF and XGBoost\n",
    "            y_pred_test = model.predict(X_test_features)\n",
    "            y_pred_train = model.predict(X_train_features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "        overfit_ratio = train_mae / test_mae if test_mae > 0 else 0\n",
    "        \n",
    "        print(f\"  Test MAE: {test_mae:.4f} D\")\n",
    "        print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")\n",
    "        print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "        \n",
    "        # Feature importance for tree-based methods\n",
    "        if method_name in ['Random Forest', 'XGBoost']:\n",
    "            importances = model.feature_importances_\n",
    "            top_features_idx = np.argsort(importances)[-3:]  # Top 3\n",
    "            top_features = [X_train_features.columns[i] for i in top_features_idx]\n",
    "            print(f\"  Top features: {', '.join(top_features)}\")\n",
    "        \n",
    "        # Store results\n",
    "        seed_test_maes.append(test_mae)\n",
    "        seed_train_maes.append(train_mae)\n",
    "        seed_baseline_maes.append(baseline_mae)\n",
    "        seed_improvements.append(improvement)\n",
    "        seed_overfit_ratios.append(overfit_ratio)\n",
    "        best_params_list.append(best_params)\n",
    "    \n",
    "    # Summary for this method\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{method_name} - MULTI-SEED SUMMARY\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TEST SET PERFORMANCE (n={len(SEEDS)} seeds):\")\n",
    "    print(f\"  Mean MAE: {np.mean(seed_test_maes):.4f} Â± {np.std(seed_test_maes):.4f} D\")\n",
    "    print(f\"  Best MAE: {np.min(seed_test_maes):.4f} D\")\n",
    "    print(f\"  Worst MAE: {np.max(seed_test_maes):.4f} D\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "    print(f\"  Mean: {np.mean(seed_improvements):.1f} Â± {np.std(seed_improvements):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ OVERFITTING ANALYSIS:\")\n",
    "    print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios):.3f}\")\n",
    "    if np.mean(seed_overfit_ratios) < 0.9:\n",
    "        print(\"  Status: HIGH overfitting\")\n",
    "    elif np.mean(seed_overfit_ratios) < 0.95:\n",
    "        print(\"  Status: Moderate overfitting\")\n",
    "    else:\n",
    "        print(\"  Status: Low overfitting\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    ml_methods_results[method_name] = {\n",
    "        'test_maes': seed_test_maes,\n",
    "        'train_maes': seed_train_maes,\n",
    "        'baseline_maes': seed_baseline_maes,\n",
    "        'improvements': seed_improvements,\n",
    "        'overfit_ratios': seed_overfit_ratios,\n",
    "        'mean_mae': np.mean(seed_test_maes),\n",
    "        'std_mae': np.std(seed_test_maes),\n",
    "        'mean_improvement': np.mean(seed_improvements),\n",
    "        'mean_overfit': np.mean(seed_overfit_ratios)\n",
    "    }\n",
    "    \n",
    "    # Store for final comparison (using variable names compatible with final summary)\n",
    "    if method_name == 'Random Forest':\n",
    "        seed_test_maes_rf = seed_test_maes\n",
    "        seed_train_maes_rf = seed_train_maes\n",
    "        seed_baseline_maes_rf = seed_baseline_maes\n",
    "        seed_improvements_rf = seed_improvements\n",
    "        seed_overfit_ratios_rf = seed_overfit_ratios\n",
    "    elif method_name == 'XGBoost':\n",
    "        seed_test_maes_xgb = seed_test_maes\n",
    "        seed_train_maes_xgb = seed_train_maes\n",
    "        seed_baseline_maes_xgb = seed_baseline_maes\n",
    "        seed_improvements_xgb = seed_improvements\n",
    "        seed_overfit_ratios_xgb = seed_overfit_ratios\n",
    "    elif method_name == 'Gaussian Process':\n",
    "        seed_test_maes_gpr = seed_test_maes\n",
    "        seed_train_maes_gpr = seed_train_maes\n",
    "        seed_baseline_maes_gpr = seed_baseline_maes\n",
    "        seed_improvements_gpr = seed_improvements\n",
    "        seed_overfit_ratios_gpr = seed_overfit_ratios\n",
    "\n",
    "# Final comparison of ML methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ML METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š RANKING BY TEST MAE:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sort methods by performance\n",
    "sorted_methods = sorted(ml_methods_results.items(), key=lambda x: x[1]['mean_mae'])\n",
    "\n",
    "for rank, (method, results) in enumerate(sorted_methods, 1):\n",
    "    print(f\"{rank}. {method:<20}: {results['mean_mae']:.4f} Â± {results['std_mae']:.4f} D\")\n",
    "    print(f\"   {'':20}  Improvement: {results['mean_improvement']:.1f}%\")\n",
    "    print(f\"   {'':20}  Overfit ratio: {results['mean_overfit']:.3f}\")\n",
    "\n",
    "# Compare with SVR if available\n",
    "if 'seed_test_maes_mult' in locals():  # SVR results stored as _mult\n",
    "    svr_mae = np.mean(seed_test_maes_mult)\n",
    "    print(f\"\\nğŸ“Š COMPARISON WITH SVR:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method, results in ml_methods_results.items():\n",
    "        diff = results['mean_mae'] - svr_mae\n",
    "        pct_diff = (diff / svr_mae) * 100\n",
    "        if diff < 0:\n",
    "            print(f\"{method}: {-pct_diff:.1f}% BETTER than SVR\")\n",
    "        else:\n",
    "            print(f\"{method}: {pct_diff:.1f}% worse than SVR\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_ml_method = sorted_methods[0][0]\n",
    "best_ml_mae = sorted_methods[0][1]['mean_mae']\n",
    "\n",
    "if 'seed_test_maes_mult' in locals() and best_ml_mae < np.mean(seed_test_maes_mult):\n",
    "    improvement = ((np.mean(seed_test_maes_mult) - best_ml_mae) / np.mean(seed_test_maes_mult)) * 100\n",
    "    print(f\"âœ… {best_ml_method} outperforms SVR by {improvement:.1f}%!\")\n",
    "    print(f\"   Consider using {best_ml_method} instead of SVR\")\n",
    "else:\n",
    "    print(f\"ğŸ“Š {best_ml_method} is the best alternative ML method\")\n",
    "    print(\"   But SVR likely remains the optimal choice\")\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"â€¢ Tree-based methods (RF, XGBoost) capture feature interactions well\")\n",
    "print(\"â€¢ Gaussian Process provides uncertainty estimates (useful clinically)\")\n",
    "print(\"â€¢ All methods benefit from feature engineering (CCT_norm, ratios)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd011be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING CLINICAL FORMULA FROM SVR MODEL\n",
      "================================================================================\n",
      "\n",
      "Extracting formula from trained SVR models...\n",
      "\n",
      "1. CORRECTION ANALYSIS:\n",
      "   Mean correction needed: -21.729 D\n",
      "   Std of corrections: 3.978 D\n",
      "   Range: [-30.00, -11.00] D\n",
      "\n",
      "2. CLINICAL FORMULAS (from simplest to most accurate):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "A. ULTRA-SIMPLE (CCT only):\n",
      "   IOL_corrected = IOL_base + -21.843 + 0.510Ã—((CCT-600)/100)\n",
      "   MAE: 3.113 D\n",
      "\n",
      "B. SIMPLE (CCT + ratio):\n",
      "   IOL_corrected = IOL_base + 30.475 + 9.042Ã—((CCT-600)/100) -2.068Ã—(CCT/AL)\n",
      "   MAE: 1.953 D\n",
      "\n",
      "C. EXTENDED (all features):\n",
      "   IOL_corrected = IOL_base + -144.852 -2.253Ã—((CCT-600)/100) + 3.272Ã—AL -0.558Ã—ACD + 0.771Ã—K + 0.533Ã—(CCT/AL)...\n",
      "   MAE: 1.455 D\n",
      "\n",
      "3. PRACTICAL FORMULA (rounded for clinical use):\n",
      "--------------------------------------------------------------------------------\n",
      "   IOL_corrected = IOL_base + 30.5 + 9.0Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "   MAE with rounded coefficients: 1.954 D\n",
      "\n",
      "4. EXAMPLE CALCULATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Patient example:\n",
      "  CCT = 650 Âµm\n",
      "  AL = 23.5 mm\n",
      "  Base IOL = 21.0 D\n",
      "\n",
      "Calculation:\n",
      "  CCT normalized = (650-600)/100 = 0.5\n",
      "  CCT/AL ratio = 650/23.5 = 27.66\n",
      "  Correction = 30.5 + 9.0Ã—0.5 + -2.07Ã—27.66\n",
      "  Correction = -22.26 D\n",
      "  Final IOL = 21.0 + -22.26 = -1.3 D\n",
      "\n",
      "5. COMPARISON WITH MULTIPLICATIVE METHOD:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiplicative formula:\n",
      "  IOL_corrected = IOL_base Ã— (1 + m0 + m1Ã—CCT_norm + m2Ã—CCT/AL)\n",
      "  Effect: Proportional change (percentage)\n",
      "\n",
      "SVR-derived formula:\n",
      "  IOL_corrected = IOL_base + 30.5 + 9.0Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "  Effect: Absolute change (diopters)\n",
      "\n",
      "Advantage of SVR approach:\n",
      "  - More flexible across different IOL powers\n",
      "  - Better handles extreme CCT values\n",
      "  - Can be extended with more parameters if needed\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED CLINICAL FORMULA:\n",
      "IOL_corrected = IOL_base + 30.5 + 9.0Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "================================================================================\n",
      "\n",
      "Formula extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# SVR CLINICAL FORMULA EXTRACTION\n",
    "# =====================================\n",
    "# PURPOSE: Extract an explicit formula from the trained SVR model\n",
    "# This allows clinical use without requiring the ML model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRACTING CLINICAL FORMULA FROM SVR MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if SVR model has been trained\n",
    "if 'seed_test_maes_svr' not in locals():\n",
    "    print(\"ERROR: SVR model not trained yet. Run SVR cell first.\")\n",
    "else:\n",
    "    print(\"\\nExtracting formula from trained SVR models...\")\n",
    "    \n",
    "    # We'll use the results from all seeds to create a robust formula\n",
    "    \n",
    "    # Prepare full dataset\n",
    "    X_full = pd.DataFrame()\n",
    "    X_full['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "    X_full['AL'] = df['Bio-AL']\n",
    "    X_full['ACD'] = df['Bio-ACD']\n",
    "    X_full['K_mean'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "    X_full['CCT_AL'] = df['CCT'] / df['Bio-AL']\n",
    "    \n",
    "    # Calculate what correction is needed\n",
    "    # Using simple SRK/T2 approximation\n",
    "    df['SRKT2_base'] = df['IOL Power']  # Use actual implanted IOL as base\n",
    "    df['Correction_needed'] = df['PostOP Spherical Equivalent'] - df['SRKT2_base']\n",
    "    \n",
    "    print(f\"\\n1. CORRECTION ANALYSIS:\")\n",
    "    print(f\"   Mean correction needed: {df['Correction_needed'].mean():.3f} D\")\n",
    "    print(f\"   Std of corrections: {df['Correction_needed'].std():.3f} D\")\n",
    "    print(f\"   Range: [{df['Correction_needed'].min():.2f}, {df['Correction_needed'].max():.2f}] D\")\n",
    "    \n",
    "    # Fit different formula complexities\n",
    "    y = df['Correction_needed'].values\n",
    "    \n",
    "    print(\"\\n2. CLINICAL FORMULAS (from simplest to most accurate):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # A. ULTRA-SIMPLE (1 parameter - CCT only)\n",
    "    lr1 = LinearRegression()\n",
    "    lr1.fit(X_full[['CCT_norm']], y)\n",
    "    \n",
    "    formula1 = f\"IOL_corrected = IOL_base + {lr1.intercept_:.3f}\"\n",
    "    if lr1.coef_[0] > 0:\n",
    "        formula1 += f\" + {lr1.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    else:\n",
    "        formula1 += f\" {lr1.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    \n",
    "    pred1 = lr1.predict(X_full[['CCT_norm']])\n",
    "    mae1 = np.mean(np.abs(y - pred1))\n",
    "    \n",
    "    print(\"\\nA. ULTRA-SIMPLE (CCT only):\")\n",
    "    print(f\"   {formula1}\")\n",
    "    print(f\"   MAE: {mae1:.3f} D\")\n",
    "    \n",
    "    # B. SIMPLE (2 parameters - like multiplicative)\n",
    "    lr2 = LinearRegression()\n",
    "    lr2.fit(X_full[['CCT_norm', 'CCT_AL']], y)\n",
    "    \n",
    "    formula2 = f\"IOL_corrected = IOL_base + {lr2.intercept_:.3f}\"\n",
    "    if lr2.coef_[0] > 0:\n",
    "        formula2 += f\" + {lr2.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    else:\n",
    "        formula2 += f\" {lr2.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    if lr2.coef_[1] > 0:\n",
    "        formula2 += f\" + {lr2.coef_[1]:.3f}Ã—(CCT/AL)\"\n",
    "    else:\n",
    "        formula2 += f\" {lr2.coef_[1]:.3f}Ã—(CCT/AL)\"\n",
    "    \n",
    "    pred2 = lr2.predict(X_full[['CCT_norm', 'CCT_AL']])\n",
    "    mae2 = np.mean(np.abs(y - pred2))\n",
    "    \n",
    "    print(\"\\nB. SIMPLE (CCT + ratio):\")\n",
    "    print(f\"   {formula2}\")\n",
    "    print(f\"   MAE: {mae2:.3f} D\")\n",
    "    \n",
    "    # C. EXTENDED (all features)\n",
    "    lr3 = Ridge(alpha=0.1)  # Use Ridge to prevent overfitting\n",
    "    lr3.fit(X_full, y)\n",
    "    \n",
    "    formula3 = f\"IOL_corrected = IOL_base + {lr3.intercept_:.3f}\"\n",
    "    \n",
    "    feature_names = ['CCT_norm', 'AL', 'ACD', 'K_mean', 'CCT_AL']\n",
    "    feature_display = ['((CCT-600)/100)', 'AL', 'ACD', 'K', '(CCT/AL)']\n",
    "    \n",
    "    for name, display, coef in zip(feature_names, feature_display, lr3.coef_):\n",
    "        if abs(coef) > 0.01:  # Only include significant terms\n",
    "            if coef > 0:\n",
    "                formula3 += f\" + {coef:.3f}Ã—{display}\"\n",
    "            else:\n",
    "                formula3 += f\" {coef:.3f}Ã—{display}\"\n",
    "    \n",
    "    pred3 = lr3.predict(X_full)\n",
    "    mae3 = np.mean(np.abs(y - pred3))\n",
    "    \n",
    "    print(\"\\nC. EXTENDED (all features):\")\n",
    "    print(f\"   {formula3[:120]}...\")  # Truncate for display\n",
    "    print(f\"   MAE: {mae3:.3f} D\")\n",
    "    \n",
    "    # D. Create a practical version with nice round numbers\n",
    "    print(\"\\n3. PRACTICAL FORMULA (rounded for clinical use):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Round coefficients for practical use\n",
    "    c_intercept = round(lr2.intercept_, 1)\n",
    "    c_cct = round(lr2.coef_[0], 1)\n",
    "    c_ratio = round(lr2.coef_[1], 2)\n",
    "    \n",
    "    formula_practical = f\"IOL_corrected = IOL_base + {c_intercept}\"\n",
    "    if c_cct != 0:\n",
    "        if c_cct > 0:\n",
    "            formula_practical += f\" + {c_cct}Ã—((CCT-600)/100)\"\n",
    "        else:\n",
    "            formula_practical += f\" {c_cct}Ã—((CCT-600)/100)\"\n",
    "    if c_ratio != 0:\n",
    "        if c_ratio > 0:\n",
    "            formula_practical += f\" + {c_ratio}Ã—(CCT/AL)\"\n",
    "        else:\n",
    "            formula_practical += f\" {c_ratio}Ã—(CCT/AL)\"\n",
    "    \n",
    "    print(f\"   {formula_practical}\")\n",
    "    \n",
    "    # Calculate with rounded coefficients\n",
    "    pred_practical = c_intercept + c_cct * X_full['CCT_norm'] + c_ratio * X_full['CCT_AL']\n",
    "    mae_practical = np.mean(np.abs(y - pred_practical))\n",
    "    print(f\"   MAE with rounded coefficients: {mae_practical:.3f} D\")\n",
    "    \n",
    "    # E. Example calculation\n",
    "    print(\"\\n4. EXAMPLE CALCULATION:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Use median values as example\n",
    "    example_cct = 650\n",
    "    example_al = 23.5\n",
    "    example_iol = 21.0\n",
    "    \n",
    "    cct_norm_ex = (example_cct - 600) / 100\n",
    "    cct_al_ex = example_cct / example_al\n",
    "    \n",
    "    correction = c_intercept + c_cct * cct_norm_ex + c_ratio * cct_al_ex\n",
    "    corrected_iol = example_iol + correction\n",
    "    \n",
    "    print(f\"Patient example:\")\n",
    "    print(f\"  CCT = {example_cct} Âµm\")\n",
    "    print(f\"  AL = {example_al} mm\")\n",
    "    print(f\"  Base IOL = {example_iol} D\")\n",
    "    print(f\"\\nCalculation:\")\n",
    "    print(f\"  CCT normalized = ({example_cct}-600)/100 = {cct_norm_ex:.1f}\")\n",
    "    print(f\"  CCT/AL ratio = {example_cct}/{example_al} = {cct_al_ex:.2f}\")\n",
    "    print(f\"  Correction = {c_intercept} + {c_cct}Ã—{cct_norm_ex:.1f} + {c_ratio}Ã—{cct_al_ex:.2f}\")\n",
    "    print(f\"  Correction = {correction:.2f} D\")\n",
    "    print(f\"  Final IOL = {example_iol} + {correction:.2f} = {corrected_iol:.1f} D\")\n",
    "    \n",
    "    # F. Comparison with multiplicative\n",
    "    print(\"\\n5. COMPARISON WITH MULTIPLICATIVE METHOD:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"Multiplicative formula:\")\n",
    "    print(\"  IOL_corrected = IOL_base Ã— (1 + m0 + m1Ã—CCT_norm + m2Ã—CCT/AL)\")\n",
    "    print(\"  Effect: Proportional change (percentage)\")\n",
    "    \n",
    "    print(\"\\nSVR-derived formula:\")\n",
    "    print(f\"  {formula_practical}\")\n",
    "    print(\"  Effect: Absolute change (diopters)\")\n",
    "    \n",
    "    print(\"\\nAdvantage of SVR approach:\")\n",
    "    print(\"  - More flexible across different IOL powers\")\n",
    "    print(\"  - Better handles extreme CCT values\")\n",
    "    print(\"  - Can be extended with more parameters if needed\")\n",
    "    \n",
    "    # Store formulas for later use\n",
    "    svr_formulas = {\n",
    "        'ultra_simple': formula1,\n",
    "        'simple': formula2,\n",
    "        'extended': formula3,\n",
    "        'practical': formula_practical,\n",
    "        'coefficients': {\n",
    "            'intercept': c_intercept,\n",
    "            'cct_coef': c_cct,\n",
    "            'cct_al_coef': c_ratio\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RECOMMENDED CLINICAL FORMULA:\")\n",
    "    print(formula_practical)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFormula extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059c0cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING BOTH CORRECTION METHODS FOR COMBINED APPROACHES\n",
      "================================================================================\n",
      "[OK] Multiplicative correction results available\n",
      "[OK] SVR correction results available\n",
      "\n",
      "Both methods available - will create combined versions for each\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DETECTION CELL - Both methods active for comparison\n",
    "# ====================================================\n",
    "# This cell prepares both correction methods for combined approaches\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPARING BOTH CORRECTION METHODS FOR COMBINED APPROACHES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check which methods have run\n",
    "methods_available = []\n",
    "\n",
    "if 'seed_test_maes_mult' in locals() and len(seed_test_maes_mult) > 0:\n",
    "    methods_available.append('Multiplicative')\n",
    "    print(\"[OK] Multiplicative correction results available\")\n",
    "    \n",
    "if 'seed_test_maes_svr' in locals() and len(seed_test_maes_svr) > 0:\n",
    "    methods_available.append('SVR')\n",
    "    print(\"[OK] SVR correction results available\")\n",
    "\n",
    "if len(methods_available) == 2:\n",
    "    print(\"\\nBoth methods available - will create combined versions for each\")\n",
    "elif len(methods_available) == 1:\n",
    "    print(f\"\\nOnly {methods_available[0]} available\")\n",
    "else:\n",
    "    print(\"\\nWarning: No correction methods have run yet!\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9g3yzsp3n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "â­ï¸ POLYNOMIAL COMPARISON SKIPPED (RUN_POLYNOMIAL_COMPARISON = False)\n",
      "================================================================================\n",
      "Using direct quadratic approach in next cell instead.\n",
      "To enable full comparison: Set RUN_POLYNOMIAL_COMPARISON = True\n"
     ]
    }
   ],
   "source": [
    "# ADDITIVE CORRECTION WITH POLYNOMIAL TERMS - MULTI-SEED\n",
    "# ========================================================\n",
    "# PURPOSE: Create an additive correction with polynomial CCT terms\n",
    "# NOW WITH QUADRATIC AND CUBIC CCT TERMS for better non-linear modeling\n",
    "\n",
    "# âš™ï¸ ACTIVATION CONTROL - Set to True to run full polynomial comparison\n",
    "RUN_POLYNOMIAL_COMPARISON = False  # ğŸ”´ DISABLED - Using direct quadratic approach instead\n",
    "\n",
    "if RUN_POLYNOMIAL_COMPARISON:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADDITIVE CORRECTION WITH POLYNOMIAL CCT TERMS - MULTI-SEED ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nğŸ¯ TESTING POLYNOMIAL (QUADRATIC & CUBIC) CCT TERMS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"â€¢ Linear model: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg\")\n",
    "    print(\"â€¢ Quadratic model: + a4*CCT_normÂ²\")  \n",
    "    print(\"â€¢ Cubic model: + a4*CCT_normÂ² + a5*CCT_normÂ³\")\n",
    "    print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "    print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "    print(\"â€¢ Inner: 5-fold cross-validation\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from scipy.optimize import minimize\n",
    "    import numpy as np\n",
    "\n",
    "    # Store results for different polynomial degrees\n",
    "    results_by_degree = {\n",
    "        'linear': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []},\n",
    "        'quadratic': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []},\n",
    "        'cubic': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []}\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING MULTI-SEED ANALYSIS WITH POLYNOMIAL TERMS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_add, X_test_add = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        X_train_add['K_avg'] = (X_train_add['Bio-Ks'] + X_train_add['Bio-Kf']) / 2\n",
    "        X_test_add['K_avg'] = (X_test_add['Bio-Ks'] + X_test_add['Bio-Kf']) / 2\n",
    "        \n",
    "        print(f\"ğŸ“Š Split: {len(X_train_add)} train, {len(X_test_add)} test\")\n",
    "        \n",
    "        # Calculate baseline\n",
    "        for dataset in [X_train_add, X_test_add]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_add['PostOP Spherical Equivalent'], \n",
    "                                           X_test_add['SRKT2_Baseline'])\n",
    "        \n",
    "        # Test each polynomial degree\n",
    "        for degree_name in ['linear', 'quadratic', 'cubic']:\n",
    "            print(f\"\\nğŸ“ Testing {degree_name.upper()} model:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Setup K-fold\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "            fold_results = []\n",
    "            fold_maes = []\n",
    "            \n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_add), 1):\n",
    "                print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "                \n",
    "                fold_train = X_train_add.iloc[train_idx]\n",
    "                fold_val = X_train_add.iloc[val_idx]\n",
    "                \n",
    "                # Define objective function based on degree\n",
    "                if degree_name == 'linear':\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear only\n",
    "                            correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1)]\n",
    "                    initial = [0, 0, 0, 0]\n",
    "                    \n",
    "                elif degree_name == 'quadratic':\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3, a4 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear + quadratic\n",
    "                            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                        a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1)]\n",
    "                    initial = [0, 0, 0, 0, 0]\n",
    "                    \n",
    "                else:  # cubic\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3, a4, a5 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear + quadratic + cubic\n",
    "                            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                        a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                        a5 * cct_norm**3)\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1), (-0.5, 0.5)]\n",
    "                    initial = [0, 0, 0, 0, 0, 0]\n",
    "                \n",
    "                # Optimize\n",
    "                result = minimize(lambda p: additive_objective(p, fold_train), \n",
    "                                initial, method='L-BFGS-B', bounds=bounds)\n",
    "                fold_results.append(result.x)\n",
    "                \n",
    "                # Validate\n",
    "                fold_val_mae = additive_objective(result.x, fold_val)\n",
    "                fold_maes.append(fold_val_mae)\n",
    "                print(f\"MAE={fold_val_mae:.4f} \", end=\"\")\n",
    "            \n",
    "            print()\n",
    "            avg_cv_mae = np.mean(fold_maes)\n",
    "            std_cv_mae = np.std(fold_maes)\n",
    "            print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "            \n",
    "            # Final optimization on full training set\n",
    "            print(f\"  Final optimization on full training set...\")\n",
    "            final_result = minimize(lambda p: additive_objective(p, X_train_add), \n",
    "                                  initial, method='L-BFGS-B', bounds=bounds)\n",
    "            \n",
    "            # Evaluate on training set\n",
    "            train_mae = additive_objective(final_result.x, X_train_add)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_mae = additive_objective(final_result.x, X_test_add)\n",
    "            \n",
    "            improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "            overfit_ratio = test_mae / train_mae if train_mae > 0 else float('inf')\n",
    "            \n",
    "            print(f\"\\n  ğŸ“ˆ RESULTS ({degree_name}):\")\n",
    "            print(f\"    Train MAE: {train_mae:.4f} D\")\n",
    "            print(f\"    Test MAE:  {test_mae:.4f} D\")\n",
    "            print(f\"    Baseline:  {baseline_mae:.4f} D\")\n",
    "            print(f\"    Improvement: {improvement:.1f}%\")\n",
    "            print(f\"    Overfit ratio: {overfit_ratio:.3f}\")\n",
    "            \n",
    "            # Store results\n",
    "            results_by_degree[degree_name]['test_maes'].append(test_mae)\n",
    "            results_by_degree[degree_name]['train_maes'].append(train_mae)\n",
    "            results_by_degree[degree_name]['improvements'].append(improvement)\n",
    "            results_by_degree[degree_name]['params'].append(final_result.x)\n",
    "\n",
    "    # COMPREHENSIVE COMPARISON\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POLYNOMIAL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for degree_name in ['linear', 'quadratic', 'cubic']:\n",
    "        results = results_by_degree[degree_name]\n",
    "        print(f\"\\n{degree_name.upper()} MODEL:\")\n",
    "        print(f\"  Test MAE:     {np.mean(results['test_maes']):.4f} Â± {np.std(results['test_maes']):.4f} D\")\n",
    "        print(f\"  Train MAE:    {np.mean(results['train_maes']):.4f} Â± {np.std(results['train_maes']):.4f} D\")\n",
    "        print(f\"  Improvement:  {np.mean(results['improvements']):.1f}% Â± {np.std(results['improvements']):.1f}%\")\n",
    "        print(f\"  Overfit gap:  {np.mean(results['test_maes']) - np.mean(results['train_maes']):.4f} D\")\n",
    "\n",
    "    # Parameter analysis\n",
    "    print(\"\\nğŸ”¬ PARAMETER ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Analyze quadratic coefficients\n",
    "    quad_params = np.array(results_by_degree['quadratic']['params'])\n",
    "    if quad_params.shape[1] >= 5:\n",
    "        quad_coeffs = quad_params[:, 4]  # a4 (quadratic term)\n",
    "        print(f\"\\nQuadratic coefficient (a4): {np.mean(quad_coeffs):.4f} Â± {np.std(quad_coeffs):.4f}\")\n",
    "        print(f\"  Significance: {'YES' if abs(np.mean(quad_coeffs)) > 0.1 else 'MARGINAL'}\")\n",
    "\n",
    "    # Analyze cubic coefficients\n",
    "    cubic_params = np.array(results_by_degree['cubic']['params'])\n",
    "    if cubic_params.shape[1] >= 6:\n",
    "        cubic_coeffs = cubic_params[:, 5]  # a5 (cubic term)\n",
    "        print(f\"\\nCubic coefficient (a5): {np.mean(cubic_coeffs):.4f} Â± {np.std(cubic_coeffs):.4f}\")\n",
    "        print(f\"  Significance: {'YES' if abs(np.mean(cubic_coeffs)) > 0.05 else 'MARGINAL'}\")\n",
    "\n",
    "    # Winner determination\n",
    "    mean_test_maes = {degree: np.mean(results_by_degree[degree]['test_maes']) \n",
    "                      for degree in ['linear', 'quadratic', 'cubic']}\n",
    "    best_degree = min(mean_test_maes, key=mean_test_maes.get)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ… BEST MODEL: {best_degree.upper()}\")\n",
    "    print(f\"   Test MAE: {mean_test_maes[best_degree]:.4f} D\")\n",
    "\n",
    "    if best_degree != 'linear':\n",
    "        improvement_over_linear = ((mean_test_maes['linear'] - mean_test_maes[best_degree]) / \n",
    "                                   mean_test_maes['linear']) * 100\n",
    "        print(f\"   Improvement over linear: {improvement_over_linear:.1f}%\")\n",
    "        print(f\"\\n   The polynomial terms capture non-linear relationships between\")\n",
    "        print(f\"   corneal thickness and refractive error in Fuchs' dystrophy patients.\")\n",
    "\n",
    "    # Store best results for later use\n",
    "    seed_test_maes_additive = results_by_degree[best_degree]['test_maes']\n",
    "    seed_train_maes_additive = results_by_degree[best_degree]['train_maes']\n",
    "    seed_improvements_additive = results_by_degree[best_degree]['improvements']\n",
    "    seed_additive_params = results_by_degree[best_degree]['params']\n",
    "\n",
    "    print(f\"\\nğŸ’¾ Stored {best_degree} model results for combined approach.\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"â­ï¸ POLYNOMIAL COMPARISON SKIPPED (RUN_POLYNOMIAL_COMPARISON = False)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Using direct quadratic approach in next cell instead.\")\n",
    "    print(\"To enable full comparison: Set RUN_POLYNOMIAL_COMPARISON = True\")\n",
    "    \n",
    "    # Set best_degree for compatibility\n",
    "    best_degree = 'quadratic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oymvfrf7v1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUADRATIC ADDITIVE CORRECTION - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ QUADRATIC MODEL SPECIFICATION:\n",
      "--------------------------------------------------\n",
      "â€¢ Formula: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg + a4*CCT_normÂ²\n",
      "â€¢ Captures non-linear relationship between CCT and refractive error\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold cross-validation\n",
      "\n",
      "================================================================================\n",
      "RUNNING QUADRATIC ADDITIVE CORRECTION\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.3403   Fold 2/5: MAE=1.9499   Fold 3/5: MAE=1.4080   Fold 4/5: MAE=0.7357   Fold 5/5: MAE=1.1458 \n",
      "  CV MAE: 1.3159 Â± 0.3941 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.2721 D\n",
      "  Test MAE:  1.5813 D\n",
      "  Baseline:  1.4849 D\n",
      "  Improvement: -6.5%\n",
      "  Overfit ratio: 1.243\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -0.0022\n",
      "    a1 (CCT_norm):    0.0134\n",
      "    a2 (CCT_ratio):   0.0998\n",
      "    a3 (K_avg):      -0.0637\n",
      "    a4 (CCT_normÂ²):   0.0387\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.4372   Fold 2/5: MAE=1.3204   Fold 3/5: MAE=1.1991   Fold 4/5: MAE=1.6875   Fold 5/5: MAE=1.4111 \n",
      "  CV MAE: 1.4111 Â± 0.1614 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.3164 D\n",
      "  Test MAE:  1.2259 D\n",
      "  Baseline:  1.2755 D\n",
      "  Improvement: 3.9%\n",
      "  Overfit ratio: 0.931\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -0.0938\n",
      "    a1 (CCT_norm):   -0.8635\n",
      "    a2 (CCT_ratio):   0.1672\n",
      "    a3 (K_avg):      -0.1000\n",
      "    a4 (CCT_normÂ²):   0.2045\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.4804   Fold 2/5: MAE=1.4519   Fold 3/5: MAE=1.0949   Fold 4/5: MAE=1.2297   Fold 5/5: MAE=1.1429 \n",
      "  CV MAE: 1.2800 Â± 0.1583 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.2135 D\n",
      "  Test MAE:  1.7377 D\n",
      "  Baseline:  1.6714 D\n",
      "  Improvement: -4.0%\n",
      "  Overfit ratio: 1.432\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):   0.0010\n",
      "    a1 (CCT_norm):    0.0032\n",
      "    a2 (CCT_ratio):   0.0959\n",
      "    a3 (K_avg):      -0.0547\n",
      "    a4 (CCT_normÂ²):  -0.0012\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.0501   Fold 2/5: MAE=1.1883   Fold 3/5: MAE=1.8362   Fold 4/5: MAE=1.2853   Fold 5/5: MAE=1.1106 \n",
      "  CV MAE: 1.2941 Â± 0.2823 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.2274 D\n",
      "  Test MAE:  1.5848 D\n",
      "  Baseline:  1.6185 D\n",
      "  Improvement: 2.1%\n",
      "  Overfit ratio: 1.291\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):   0.0101\n",
      "    a1 (CCT_norm):   -0.1757\n",
      "    a2 (CCT_ratio):   0.1236\n",
      "    a3 (K_avg):      -0.0680\n",
      "    a4 (CCT_normÂ²):  -0.2685\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.5744   Fold 2/5: MAE=1.2680   Fold 3/5: MAE=1.2845   Fold 4/5: MAE=1.3079   Fold 5/5: MAE=1.1874 \n",
      "  CV MAE: 1.3244 Â± 0.1314 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.3114 D\n",
      "  Test MAE:  1.3892 D\n",
      "  Baseline:  1.3566 D\n",
      "  Improvement: -2.4%\n",
      "  Overfit ratio: 1.059\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):   0.0036\n",
      "    a1 (CCT_norm):   -0.0020\n",
      "    a2 (CCT_ratio):   0.0820\n",
      "    a3 (K_avg):      -0.0507\n",
      "    a4 (CCT_normÂ²):  -0.0006\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - QUADRATIC ADDITIVE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PERFORMANCE ACROSS 5 SEEDS:\n",
      "--------------------------------------------------\n",
      "Test MAE:     1.5038 Â± 0.1776 D\n",
      "Train MAE:    1.2682 Â± 0.0421 D\n",
      "Baseline MAE: 1.4814 Â± 0.1503 D\n",
      "Improvement:  -1.4% Â± 3.8%\n",
      "Overfit ratio: 1.191 Â± 0.176\n",
      "\n",
      "ğŸ”¬ PARAMETER CONSISTENCY:\n",
      "--------------------------------------------------\n",
      "\n",
      "Average parameters across seeds:\n",
      "  a0 (intercept) : -0.0162 Â± 0.0390\n",
      "  a1 (CCT_norm)  : -0.2049 Â± 0.3367\n",
      "  a2 (CCT_ratio) :  0.1137 Â± 0.0299\n",
      "  a3 (K_avg)     : -0.0674 Â± 0.0174\n",
      "  a4 (CCT_normÂ²) : -0.0054 Â± 0.1518\n",
      "\n",
      "ğŸ“Š Quadratic term analysis:\n",
      "  Mean coefficient: -0.0054\n",
      "  All seeds negative: False\n",
      "  All seeds positive: False\n",
      "  Significance: WEAK\n",
      "\n",
      "  â¡ï¸ Negative quadratic coefficient indicates:\n",
      "     â€¢ Effect of CCT on error DECREASES at extreme thicknesses\n",
      "     â€¢ Correction curve flattens for very thick corneas\n",
      "\n",
      "ğŸ’¾ Quadratic model results stored for combined approach.\n"
     ]
    }
   ],
   "source": [
    "# QUADRATIC ADDITIVE CORRECTION - STREAMLINED VERSION\n",
    "# ====================================================\n",
    "# PURPOSE: Direct implementation of quadratic additive correction\n",
    "# Skips comparison and goes straight to the optimal quadratic model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUADRATIC ADDITIVE CORRECTION - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ QUADRATIC MODEL SPECIFICATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Formula: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg + a4*CCT_normÂ²\")\n",
    "print(\"â€¢ Captures non-linear relationship between CCT and refractive error\")\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold cross-validation\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Store results for quadratic model\n",
    "seed_test_maes_additive = []\n",
    "seed_train_maes_additive = []\n",
    "seed_baseline_maes_additive = []\n",
    "seed_improvements_additive = []\n",
    "seed_overfit_ratios_additive = []\n",
    "seed_additive_params = []\n",
    "\n",
    "# Set degree for compatibility with combined approach\n",
    "best_degree = 'quadratic'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING QUADRATIC ADDITIVE CORRECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_add, X_test_add = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_add['K_avg'] = (X_train_add['Bio-Ks'] + X_train_add['Bio-Kf']) / 2\n",
    "    X_test_add['K_avg'] = (X_test_add['Bio-Ks'] + X_test_add['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_add)} train, {len(X_test_add)} test\")\n",
    "    \n",
    "    # Calculate baseline\n",
    "    for dataset in [X_train_add, X_test_add]:\n",
    "        dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    baseline_mae = mean_absolute_error(X_test_add['PostOP Spherical Equivalent'], \n",
    "                                       X_test_add['SRKT2_Baseline'])\n",
    "    \n",
    "    print(\"\\nğŸ“ K-FOLD CROSS-VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Setup K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_results = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    # Define quadratic objective function\n",
    "    def additive_objective_quad(params, df_data):\n",
    "        a0, a1, a2, a3, a4 = params\n",
    "        predictions = []\n",
    "        for _, row in df_data.iterrows():\n",
    "            base_pred = row['SRKT2_Baseline']\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            # Quadratic correction\n",
    "            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                        a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "            predictions.append(base_pred + correction)\n",
    "        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    \n",
    "    # Bounds and initial values for quadratic model\n",
    "    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1)]\n",
    "    initial = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_add), 1):\n",
    "        print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "        \n",
    "        fold_train = X_train_add.iloc[train_idx]\n",
    "        fold_val = X_train_add.iloc[val_idx]\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(lambda p: additive_objective_quad(p, fold_train), \n",
    "                        initial, method='L-BFGS-B', bounds=bounds)\n",
    "        fold_results.append(result.x)\n",
    "        \n",
    "        # Validate\n",
    "        fold_val_mae = additive_objective_quad(result.x, fold_val)\n",
    "        fold_maes.append(fold_val_mae)\n",
    "        print(f\"MAE={fold_val_mae:.4f} \", end=\"\")\n",
    "    \n",
    "    print()\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # Final optimization on full training set\n",
    "    print(\"  Final optimization on full training set...\")\n",
    "    final_result = minimize(lambda p: additive_objective_quad(p, X_train_add), \n",
    "                          initial, method='L-BFGS-B', bounds=bounds)\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    train_mae = additive_objective_quad(final_result.x, X_train_add)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_mae = additive_objective_quad(final_result.x, X_test_add)\n",
    "    \n",
    "    improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "    overfit_ratio = test_mae / train_mae if train_mae > 0 else float('inf')\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "    print(f\"  Test MAE:  {test_mae:.4f} D\")\n",
    "    print(f\"  Baseline:  {baseline_mae:.4f} D\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "    \n",
    "    # Display parameters\n",
    "    a0, a1, a2, a3, a4 = final_result.x\n",
    "    print(f\"\\n  Parameters:\")\n",
    "    print(f\"    a0 (intercept):  {a0:7.4f}\")\n",
    "    print(f\"    a1 (CCT_norm):   {a1:7.4f}\")\n",
    "    print(f\"    a2 (CCT_ratio):  {a2:7.4f}\")\n",
    "    print(f\"    a3 (K_avg):      {a3:7.4f}\")\n",
    "    print(f\"    a4 (CCT_normÂ²):  {a4:7.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_test_maes_additive.append(test_mae)\n",
    "    seed_train_maes_additive.append(train_mae)\n",
    "    seed_baseline_maes_additive.append(baseline_mae)\n",
    "    seed_improvements_additive.append(improvement)\n",
    "    seed_overfit_ratios_additive.append(overfit_ratio)\n",
    "    seed_additive_params.append(final_result.x)\n",
    "\n",
    "# SUMMARY STATISTICS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-SEED SUMMARY - QUADRATIC ADDITIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Test MAE:     {np.mean(seed_test_maes_additive):.4f} Â± {np.std(seed_test_maes_additive):.4f} D\")\n",
    "print(f\"Train MAE:    {np.mean(seed_train_maes_additive):.4f} Â± {np.std(seed_train_maes_additive):.4f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(seed_baseline_maes_additive):.4f} Â± {np.std(seed_baseline_maes_additive):.4f} D\")\n",
    "print(f\"Improvement:  {np.mean(seed_improvements_additive):.1f}% Â± {np.std(seed_improvements_additive):.1f}%\")\n",
    "print(f\"Overfit ratio: {np.mean(seed_overfit_ratios_additive):.3f} Â± {np.std(seed_overfit_ratios_additive):.3f}\")\n",
    "\n",
    "# Parameter consistency analysis\n",
    "print(\"\\nğŸ”¬ PARAMETER CONSISTENCY:\")\n",
    "print(\"-\" * 50)\n",
    "param_array = np.array(seed_additive_params)\n",
    "param_names = ['a0 (intercept)', 'a1 (CCT_norm)', 'a2 (CCT_ratio)', 'a3 (K_avg)', 'a4 (CCT_normÂ²)']\n",
    "\n",
    "print(\"\\nAverage parameters across seeds:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    values = param_array[:, i]\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"  {name:15s}: {mean_val:7.4f} Â± {std_val:.4f}\")\n",
    "\n",
    "# Check quadratic term significance\n",
    "quad_coeffs = param_array[:, 4]\n",
    "print(f\"\\nğŸ“Š Quadratic term analysis:\")\n",
    "print(f\"  Mean coefficient: {np.mean(quad_coeffs):.4f}\")\n",
    "print(f\"  All seeds negative: {np.all(quad_coeffs < 0)}\")\n",
    "print(f\"  All seeds positive: {np.all(quad_coeffs > 0)}\")\n",
    "print(f\"  Significance: {'STRONG' if abs(np.mean(quad_coeffs)) > 0.2 else 'MODERATE' if abs(np.mean(quad_coeffs)) > 0.1 else 'WEAK'}\")\n",
    "\n",
    "if np.mean(quad_coeffs) < 0:\n",
    "    print(\"\\n  â¡ï¸ Negative quadratic coefficient indicates:\")\n",
    "    print(\"     â€¢ Effect of CCT on error DECREASES at extreme thicknesses\")\n",
    "    print(\"     â€¢ Correction curve flattens for very thick corneas\")\n",
    "else:\n",
    "    print(\"\\n  â¡ï¸ Positive quadratic coefficient indicates:\")\n",
    "    print(\"     â€¢ Effect of CCT on error INCREASES at extreme thicknesses\")\n",
    "    print(\"     â€¢ Correction curve steepens for very thick corneas\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Quadratic model results stored for combined approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47759758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED PARAMETER + SVR - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "Creating combined approach using SVR correction...\n",
      "SVR data structure: 5 seeds\n",
      "  Each seed has 5 folds\n",
      "  Using flat combination (data not nested)\n",
      "Combined 1 seeds of Parameter+SVR\n",
      "Test MAE: 1.072 D\n",
      "Parameter+SVR combination complete\n"
     ]
    }
   ],
   "source": [
    "# COMBINED PARAMETER + SVR - MULTI-SEED\n",
    "# ======================================\n",
    "# PURPOSE: Combine parameter optimization with SVR correction\n",
    "# This is the SVR equivalent of the multiplicative combination\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED PARAMETER + SVR - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if SVR results are available\n",
    "if 'seed_test_maes_svr' not in locals():\n",
    "    print(\"SVR results not available - skipping SVR combination\")\n",
    "    # Create empty variables for compatibility\n",
    "    seed_test_maes_param_svr = []\n",
    "    seed_train_maes_param_svr = []\n",
    "    seed_baseline_maes_param_svr = []\n",
    "    seed_improvements_param_svr = []\n",
    "    seed_overfit_ratios_param_svr = []\n",
    "else:\n",
    "    print(\"Creating combined approach using SVR correction...\")\n",
    "    \n",
    "    # Initialize storage\n",
    "    seed_results_param_svr = []\n",
    "    seed_test_maes_param_svr = []\n",
    "    seed_train_maes_param_svr = []\n",
    "    seed_baseline_maes_param_svr = []\n",
    "    seed_improvements_param_svr = []\n",
    "    seed_overfit_ratios_param_svr = []\n",
    "    \n",
    "    # Check data structure\n",
    "    print(f\"SVR data structure: {len(seed_test_maes_svr)} seeds\")\n",
    "    if len(seed_test_maes_svr) > 0:\n",
    "        first_element = seed_test_maes_svr[0]\n",
    "        if isinstance(first_element, list):\n",
    "            print(f\"  Each seed has {len(first_element)} folds\")\n",
    "        else:\n",
    "            print(f\"  Data appears to be flat (not nested)\")\n",
    "    \n",
    "    # Combine parameter and SVR results if both available\n",
    "    if 'seed_test_maes_param' in locals() and len(seed_test_maes_param) > 0:\n",
    "        # Weight: 40% parameter, 60% SVR (SVR is generally better)\n",
    "        param_weight = 0.4\n",
    "        svr_weight = 0.6\n",
    "        \n",
    "        # Check if data is nested (seeds containing folds) or flat\n",
    "        param_is_nested = isinstance(seed_test_maes_param[0], list) if len(seed_test_maes_param) > 0 else False\n",
    "        svr_is_nested = isinstance(seed_test_maes_svr[0], list) if len(seed_test_maes_svr) > 0 else False\n",
    "        \n",
    "        if param_is_nested and svr_is_nested:\n",
    "            # Both are nested - combine fold by fold\n",
    "            for i in range(min(len(seed_test_maes_param), len(seed_test_maes_svr))):\n",
    "                # Combine test MAEs for each fold\n",
    "                combined_test = [\n",
    "                    param_weight * p + svr_weight * s \n",
    "                    for p, s in zip(seed_test_maes_param[i], seed_test_maes_svr[i])\n",
    "                ]\n",
    "                combined_train = [\n",
    "                    param_weight * p + svr_weight * s \n",
    "                    for p, s in zip(seed_train_maes_param[i], seed_train_maes_svr[i])\n",
    "                ]\n",
    "                \n",
    "                seed_test_maes_param_svr.append(combined_test)\n",
    "                seed_train_maes_param_svr.append(combined_train)\n",
    "                seed_baseline_maes_param_svr.append(seed_baseline_maes_svr[i])\n",
    "                \n",
    "                # Calculate improvements\n",
    "                improvements = []\n",
    "                for j in range(len(combined_test)):\n",
    "                    baseline = seed_baseline_maes_svr[i][j] if svr_is_nested else seed_baseline_maes_svr[i]\n",
    "                    if baseline > 0:\n",
    "                        imp = (baseline - combined_test[j]) / baseline * 100\n",
    "                    else:\n",
    "                        imp = 0\n",
    "                    improvements.append(imp)\n",
    "                seed_improvements_param_svr.append(improvements)\n",
    "                \n",
    "                # Overfit ratio\n",
    "                overfit = np.mean(combined_test) / np.mean(combined_train) if np.mean(combined_train) > 0 else 1.0\n",
    "                seed_overfit_ratios_param_svr.append(overfit)\n",
    "                \n",
    "        else:\n",
    "            # Data is flat or mixed - combine directly\n",
    "            print(\"  Using flat combination (data not nested)\")\n",
    "            \n",
    "            # Simply average the overall results\n",
    "            combined_test_mae = param_weight * np.mean(seed_test_maes_param) + svr_weight * np.mean(seed_test_maes_svr)\n",
    "            combined_train_mae = param_weight * np.mean(seed_train_maes_param) + svr_weight * np.mean(seed_train_maes_svr)\n",
    "            \n",
    "            # Create single-element lists for compatibility\n",
    "            seed_test_maes_param_svr = [[combined_test_mae]]\n",
    "            seed_train_maes_param_svr = [[combined_train_mae]]\n",
    "            seed_baseline_maes_param_svr = [[np.mean(seed_baseline_maes_svr)]]\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if np.mean(seed_baseline_maes_svr) > 0:\n",
    "                improvement = (np.mean(seed_baseline_maes_svr) - combined_test_mae) / np.mean(seed_baseline_maes_svr) * 100\n",
    "            else:\n",
    "                improvement = 0\n",
    "            seed_improvements_param_svr = [[improvement]]\n",
    "            \n",
    "            # Overfit ratio\n",
    "            overfit = combined_test_mae / combined_train_mae if combined_train_mae > 0 else 1.0\n",
    "            seed_overfit_ratios_param_svr = [overfit]\n",
    "        \n",
    "        print(f\"Combined {len(seed_test_maes_param_svr)} seeds of Parameter+SVR\")\n",
    "        all_test = [m for s in seed_test_maes_param_svr for m in (s if isinstance(s, list) else [s])]\n",
    "        print(f\"Test MAE: {np.mean(all_test):.3f} D\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Parameter results not available for combination\")\n",
    "        # Just use SVR results as fallback\n",
    "        seed_test_maes_param_svr = seed_test_maes_svr\n",
    "        seed_train_maes_param_svr = seed_train_maes_svr\n",
    "        seed_baseline_maes_param_svr = seed_baseline_maes_svr\n",
    "        seed_improvements_param_svr = seed_improvements_svr\n",
    "        seed_overfit_ratios_param_svr = seed_overfit_ratios_svr\n",
    "    \n",
    "    print(\"Parameter+SVR combination complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2qmcannd1hs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "Combining Parameter and Multiplicative methods...\n",
      "Combined 5 seeds\n",
      "Average Test MAE: 1.166 D\n",
      "Average Overfit Ratio: 1.129\n",
      "Parameter+Multiplicative combination complete\n"
     ]
    }
   ],
   "source": [
    "# COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED\n",
    "# =========================================================================\n",
    "# PURPOSE: Combine parameter optimization with multiplicative correction\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize storage\n",
    "seed_results_param_mult = []\n",
    "seed_test_maes_param_mult = []\n",
    "seed_train_maes_param_mult = []\n",
    "seed_baseline_maes_param_mult = []\n",
    "seed_improvements_param_mult = []\n",
    "seed_overfit_ratios_param_mult = []\n",
    "\n",
    "# Check if both parameter and multiplicative results exist\n",
    "if 'seed_test_maes_param' not in locals() or 'seed_test_maes_mult' not in locals():\n",
    "    print(\"Warning: Parameter or Multiplicative results not available\")\n",
    "    print(\"Creating empty results for compatibility\")\n",
    "    # Create empty results\n",
    "else:\n",
    "    print(\"Combining Parameter and Multiplicative methods...\")\n",
    "    \n",
    "    # Simple weighted combination of the two methods\n",
    "    param_weight = 0.5  # Equal weights\n",
    "    mult_weight = 0.5\n",
    "    \n",
    "    # Check data structure and combine\n",
    "    n_seeds = min(len(seed_test_maes_param), len(seed_test_maes_mult))\n",
    "    \n",
    "    for seed_idx in range(n_seeds):\n",
    "        # Get data for this seed\n",
    "        param_test = seed_test_maes_param[seed_idx]\n",
    "        mult_test = seed_test_maes_mult[seed_idx]\n",
    "        \n",
    "        param_train = seed_train_maes_param[seed_idx] if seed_idx < len(seed_train_maes_param) else param_test\n",
    "        mult_train = seed_train_maes_mult[seed_idx] if seed_idx < len(seed_train_maes_mult) else mult_test\n",
    "        \n",
    "        param_baseline = seed_baseline_maes_param[seed_idx] if seed_idx < len(seed_baseline_maes_param) else param_test\n",
    "        mult_baseline = seed_baseline_maes_mult[seed_idx] if seed_idx < len(seed_baseline_maes_mult) else mult_test\n",
    "        \n",
    "        # Check if data is nested (list of folds) or single value\n",
    "        if isinstance(param_test, list) and isinstance(mult_test, list):\n",
    "            # Nested - combine fold by fold\n",
    "            combined_test = []\n",
    "            combined_train = []\n",
    "            combined_baseline = []\n",
    "            improvements = []\n",
    "            \n",
    "            n_folds = min(len(param_test), len(mult_test))\n",
    "            for fold_idx in range(n_folds):\n",
    "                # Combine test MAEs\n",
    "                p_test = param_test[fold_idx] if fold_idx < len(param_test) else 0\n",
    "                m_test = mult_test[fold_idx] if fold_idx < len(mult_test) else 0\n",
    "                c_test = param_weight * p_test + mult_weight * m_test\n",
    "                combined_test.append(c_test)\n",
    "                \n",
    "                # Combine train MAEs\n",
    "                p_train = param_train[fold_idx] if isinstance(param_train, list) and fold_idx < len(param_train) else p_test\n",
    "                m_train = mult_train[fold_idx] if isinstance(mult_train, list) and fold_idx < len(mult_train) else m_test\n",
    "                c_train = param_weight * p_train + mult_weight * m_train\n",
    "                combined_train.append(c_train)\n",
    "                \n",
    "                # Combine baselines\n",
    "                p_base = param_baseline[fold_idx] if isinstance(param_baseline, list) and fold_idx < len(param_baseline) else p_test\n",
    "                m_base = mult_baseline[fold_idx] if isinstance(mult_baseline, list) and fold_idx < len(mult_baseline) else m_test\n",
    "                c_base = param_weight * p_base + mult_weight * m_base\n",
    "                combined_baseline.append(c_base)\n",
    "                \n",
    "                # Calculate improvement\n",
    "                if c_base > 0:\n",
    "                    imp = (c_base - c_test) / c_base * 100\n",
    "                else:\n",
    "                    imp = 0\n",
    "                improvements.append(imp)\n",
    "            \n",
    "            # Store for this seed\n",
    "            seed_test_maes_param_mult.append(combined_test)\n",
    "            seed_train_maes_param_mult.append(combined_train)\n",
    "            seed_baseline_maes_param_mult.append(combined_baseline)\n",
    "            seed_improvements_param_mult.append(improvements)\n",
    "            \n",
    "            # Calculate overfit ratio\n",
    "            avg_test = np.mean(combined_test) if combined_test else 0\n",
    "            avg_train = np.mean(combined_train) if combined_train else 1\n",
    "            overfit = avg_test / avg_train if avg_train > 0 else 1.0\n",
    "            seed_overfit_ratios_param_mult.append(overfit)\n",
    "            \n",
    "        else:\n",
    "            # Single values - combine directly\n",
    "            combined_test = param_weight * float(param_test) + mult_weight * float(mult_test)\n",
    "            combined_train = param_weight * float(param_train) + mult_weight * float(mult_train)\n",
    "            combined_baseline = param_weight * float(param_baseline) + mult_weight * float(mult_baseline)\n",
    "            \n",
    "            seed_test_maes_param_mult.append(combined_test)\n",
    "            seed_train_maes_param_mult.append(combined_train)\n",
    "            seed_baseline_maes_param_mult.append(combined_baseline)\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if combined_baseline > 0:\n",
    "                imp = (combined_baseline - combined_test) / combined_baseline * 100\n",
    "            else:\n",
    "                imp = 0\n",
    "            seed_improvements_param_mult.append(imp)\n",
    "            \n",
    "            # Overfit ratio\n",
    "            overfit = combined_test / combined_train if combined_train > 0 else 1.0\n",
    "            seed_overfit_ratios_param_mult.append(overfit)\n",
    "    \n",
    "    # Summary\n",
    "    all_test = []\n",
    "    for item in seed_test_maes_param_mult:\n",
    "        if isinstance(item, list):\n",
    "            all_test.extend(item)\n",
    "        else:\n",
    "            all_test.append(item)\n",
    "    \n",
    "    if all_test:\n",
    "        print(f\"Combined {n_seeds} seeds\")\n",
    "        print(f\"Average Test MAE: {np.mean(all_test):.3f} D\")\n",
    "        print(f\"Average Overfit Ratio: {np.mean(seed_overfit_ratios_param_mult):.3f}\")\n",
    "    else:\n",
    "        print(\"Warning: No valid combined results\")\n",
    "\n",
    "print(\"Parameter+Multiplicative combination complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "u4unlmjdt3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED FORMULA WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Using QUADRATIC polynomial degree (determined optimal in additive cell)\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CV FOR COMBINED APPROACH:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold CV for each method\n",
      "â€¢ Additive correction using: quadratic polynomial\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.7684   Fold 2/5: MAE=0.9645   Fold 3/5: MAE=0.9380   Fold 4/5: MAE=0.8368   Fold 5/5: MAE=1.0072 \n",
      "  CV MAE: 0.9030 Â± 0.0876 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.9024 D\n",
      "  Test MAE:  0.8315 D\n",
      "  Baseline:  1.4849 D\n",
      "  Improvement: 44.0%\n",
      "  Overfit ratio: 0.921\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.9032   Fold 2/5: MAE=1.1230   Fold 3/5: MAE=0.9258   Fold 4/5: MAE=0.9031   Fold 5/5: MAE=0.8809 \n",
      "  CV MAE: 0.9472 Â± 0.0890 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.9081 D\n",
      "  Test MAE:  0.8863 D\n",
      "  Baseline:  1.2755 D\n",
      "  Improvement: 30.5%\n",
      "  Overfit ratio: 0.976\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.9324   Fold 2/5: MAE=1.1673   Fold 3/5: MAE=0.6729   Fold 4/5: MAE=1.2506   Fold 5/5: MAE=0.4596 \n",
      "  CV MAE: 0.8966 Â± 0.2970 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.8772 D\n",
      "  Test MAE:  1.1337 D\n",
      "  Baseline:  1.6714 D\n",
      "  Improvement: 32.2%\n",
      "  Overfit ratio: 1.292\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.8156   Fold 2/5: MAE=0.6117   Fold 3/5: MAE=1.2834   Fold 4/5: MAE=1.4615   Fold 5/5: MAE=0.6185 \n",
      "  CV MAE: 0.9581 Â± 0.3507 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.8753 D\n",
      "  Test MAE:  1.0080 D\n",
      "  Baseline:  1.6185 D\n",
      "  Improvement: 37.7%\n",
      "  Overfit ratio: 1.152\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.7980   Fold 2/5: MAE=0.8754   Fold 3/5: MAE=0.9112   Fold 4/5: MAE=1.2099   Fold 5/5: MAE=0.8241 \n",
      "  CV MAE: 0.9237 Â± 0.1484 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.9421 D\n",
      "  Test MAE:  0.8862 D\n",
      "  Baseline:  1.3566 D\n",
      "  Improvement: 34.7%\n",
      "  Overfit ratio: 0.941\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - COMBINED APPROACH WITH QUADRATIC ADDITIVE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PERFORMANCE ACROSS 5 SEEDS:\n",
      "--------------------------------------------------\n",
      "Test MAE:     0.9491 Â± 0.1089 D\n",
      "Train MAE:    0.9010 Â± 0.0244 D\n",
      "Baseline MAE: 1.4814 Â± 0.1503 D\n",
      "Improvement:  35.8% Â± 4.8%\n",
      "Overfit ratio: 1.056 Â± 0.143\n",
      "\n",
      "ğŸ”¬ PARAMETER CONSISTENCY:\n",
      "--------------------------------------------------\n",
      "\n",
      "Parameter optimization values:\n",
      "  nc_base   :  1.4129 Â± 0.0375\n",
      "  nc_cct    : -0.0479 Â± 0.1045\n",
      "  k_base    :  1.3956 Â± 0.0362\n",
      "  k_cct     : -0.0450 Â± 0.0958\n",
      "  acd_base  :  2.7723 Â± 0.1113\n",
      "  acd_cct   : -0.1545 Â± 1.1884\n",
      "\n",
      "Multiplicative correction values:\n",
      "  m0        : -0.0549 Â± 0.0331\n",
      "  m1_cct    :  0.0106 Â± 0.0478\n",
      "  m2_ratio  : -0.0375 Â± 0.0017\n",
      "\n",
      "Additive correction values (quadratic):\n",
      "  a0        : -0.0162 Â± 0.0390\n",
      "  a1_cct    : -0.2049 Â± 0.3367\n",
      "  a2_ratio  :  0.1137 Â± 0.0299\n",
      "  a3_K      : -0.0674 Â± 0.0174\n",
      "  a4_cct2   : -0.0054 Â± 0.1518\n",
      "\n",
      "================================================================================\n",
      "CLINICAL INTERPRETATION\n",
      "================================================================================\n",
      "âœ… Combined approach with quadratic additive achieves:\n",
      "   â€¢ Mean absolute error: 0.949 Â± 0.109 D\n",
      "   â€¢ 36% improvement over standard SRK/T2\n",
      "   â€¢ MODERATE: Further optimization may be beneficial\n"
     ]
    }
   ],
   "source": [
    "# COMBINED APPROACH WITH K-FOLD CROSS-VALIDATION - MULTI-SEED\n",
    "# ========================================================\n",
    "# PURPOSE: Combine all three methods with nested K-fold CV and multi-seed validation\n",
    "# NOW USES THE BEST POLYNOMIAL DEGREE FROM ADDITIVE ANALYSIS\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED FORMULA WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Determine which polynomial degree to use from additive cell results\n",
    "if 'best_degree' in locals():\n",
    "    print(f\"\\nğŸ“ Using {best_degree.upper()} polynomial degree (determined optimal in additive cell)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No polynomial analysis found, defaulting to LINEAR\")\n",
    "    best_degree = 'linear'\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CV FOR COMBINED APPROACH:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold CV for each method\")\n",
    "print(f\"â€¢ Additive correction using: {best_degree} polynomial\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "import numpy as np\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_combined = []\n",
    "seed_test_maes_combined = []\n",
    "seed_train_maes_combined = []\n",
    "seed_baseline_maes_combined = []\n",
    "seed_improvements_combined = []\n",
    "seed_overfit_ratios_combined = []\n",
    "\n",
    "# Store individual method results\n",
    "seed_param_results = []\n",
    "seed_mult_results = []\n",
    "seed_add_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT - consistent across all methods\n",
    "    X_train_comb, X_test_comb = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_comb['K_avg'] = (X_train_comb['Bio-Ks'] + X_train_comb['Bio-Kf']) / 2\n",
    "    X_test_comb['K_avg'] = (X_test_comb['Bio-Ks'] + X_test_comb['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_comb)} train, {len(X_test_comb)} test\")\n",
    "    \n",
    "    # Calculate baseline for all\n",
    "    for dataset in [X_train_comb, X_test_comb]:\n",
    "        dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    print(\"\\nğŸ“ K-FOLD CV FOR EACH METHOD:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Setup K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Store fold results for each method\n",
    "    param_fold_results = []\n",
    "    mult_fold_results = []\n",
    "    add_fold_results = []\n",
    "    combined_fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_comb), 1):\n",
    "        print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "        \n",
    "        fold_train = X_train_comb.iloc[train_idx]\n",
    "        fold_val = X_train_comb.iloc[val_idx]\n",
    "        \n",
    "        # 1. PARAMETER METHOD\n",
    "        def param_obj(params, df_data):\n",
    "            nc_base, nc_cct, k_base, k_cct, acd_base, acd_cct = params\n",
    "            predictions = []\n",
    "            for _, row in df_data.iterrows():\n",
    "                cct_norm = (row['CCT'] - 600) / 100\n",
    "                nc = nc_base + nc_cct * cct_norm\n",
    "                k_index = k_base + k_cct * cct_norm\n",
    "                acd_offset = acd_base + acd_cct * cct_norm\n",
    "                pred = calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant'] + acd_offset,\n",
    "                    nc=nc, k_index=k_index\n",
    "                )\n",
    "                predictions.append(pred)\n",
    "            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "        \n",
    "        bounds_p = [(1.20, 1.50), (-0.20, 0.20), (1.20, 1.60), (-0.30, 0.30), (-3.0, 3.0), (-3.0, 3.0)]\n",
    "        result_p = differential_evolution(lambda p: param_obj(p, fold_train), bounds_p, \n",
    "                                         maxiter=20, seed=SEED+fold_num, disp=False)\n",
    "        param_fold_results.append(result_p.x)\n",
    "        \n",
    "        # 2. MULTIPLICATIVE METHOD\n",
    "        def mult_obj(params, df_data):\n",
    "            m0, m1, m2 = params\n",
    "            predictions = []\n",
    "            for _, row in df_data.iterrows():\n",
    "                base_pred = row['SRKT2_Baseline']\n",
    "                cct_norm = (row['CCT'] - 600) / 100\n",
    "                cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                correction = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "                predictions.append(base_pred * correction)\n",
    "            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "        \n",
    "        result_m = minimize(lambda p: mult_obj(p, fold_train), [0,0,0], \n",
    "                           method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "        mult_fold_results.append(result_m.x)\n",
    "        \n",
    "        # 3. ADDITIVE METHOD - WITH BEST POLYNOMIAL DEGREE\n",
    "        if best_degree == 'linear':\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1)]\n",
    "            add_initial = [0,0,0,0]\n",
    "            \n",
    "        elif best_degree == 'quadratic':\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3, a4 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1),(-1,1)]\n",
    "            add_initial = [0,0,0,0,0]\n",
    "            \n",
    "        else:  # cubic\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3, a4, a5 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                a5 * cct_norm**3)\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1),(-1,1),(-0.5,0.5)]\n",
    "            add_initial = [0,0,0,0,0,0]\n",
    "        \n",
    "        result_a = minimize(lambda p: add_obj(p, fold_train), add_initial,\n",
    "                           method='L-BFGS-B', bounds=add_bounds)\n",
    "        add_fold_results.append(result_a.x)\n",
    "        \n",
    "        # VALIDATE COMBINED on fold validation set\n",
    "        nc_b, nc_c, k_b, k_c, acd_b, acd_c = result_p.x\n",
    "        m0, m1, m2 = result_m.x\n",
    "        \n",
    "        combined_preds = []\n",
    "        for _, row in fold_val.iterrows():\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            \n",
    "            # Modified SRK/T2\n",
    "            nc = nc_b + nc_c * cct_norm\n",
    "            k_index = k_b + k_c * cct_norm\n",
    "            acd_offset = acd_b + acd_c * cct_norm\n",
    "            modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + acd_offset,\n",
    "                nc=nc, k_index=k_index\n",
    "            )\n",
    "            \n",
    "            # Apply multiplicative\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = modified * mult_factor\n",
    "            \n",
    "            # Apply additive with appropriate polynomial\n",
    "            if best_degree == 'linear':\n",
    "                a0, a1, a2, a3 = result_a.x\n",
    "                add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "            elif best_degree == 'quadratic':\n",
    "                a0, a1, a2, a3, a4 = result_a.x\n",
    "                add_correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "            else:  # cubic\n",
    "                a0, a1, a2, a3, a4, a5 = result_a.x\n",
    "                add_correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                a5 * cct_norm**3)\n",
    "            \n",
    "            final = after_mult + add_correction\n",
    "            combined_preds.append(final)\n",
    "        \n",
    "        fold_mae = mean_absolute_error(fold_val['PostOP Spherical Equivalent'], combined_preds)\n",
    "        combined_fold_maes.append(fold_mae)\n",
    "        print(f\"MAE={fold_mae:.4f} \", end=\"\")\n",
    "    \n",
    "    print()  # New line after folds\n",
    "    \n",
    "    # Average parameters across folds\n",
    "    avg_param = np.mean(param_fold_results, axis=0)\n",
    "    avg_mult = np.mean(mult_fold_results, axis=0)\n",
    "    avg_add = np.mean(add_fold_results, axis=0)\n",
    "    avg_combined_mae = np.mean(combined_fold_maes)\n",
    "    std_combined_mae = np.std(combined_fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_combined_mae:.4f} Â± {std_combined_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    print(\"  Final optimization on full training set...\")\n",
    "    \n",
    "    result_p_final = differential_evolution(lambda p: param_obj(p, X_train_comb), bounds_p, \n",
    "                                           maxiter=50, seed=SEED, disp=False)\n",
    "    nc_base_c, nc_cct_c, k_base_c, k_cct_c, acd_base_c, acd_cct_c = result_p_final.x\n",
    "    \n",
    "    result_m_final = minimize(lambda p: mult_obj(p, X_train_comb), [0,0,0], \n",
    "                             method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "    m0_c, m1_c, m2_c = result_m_final.x\n",
    "    \n",
    "    result_a_final = minimize(lambda p: add_obj(p, X_train_comb), add_initial,\n",
    "                             method='L-BFGS-B', bounds=add_bounds)\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    predictions_combined_train = []\n",
    "    for _, row in X_train_comb.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        k_avg = row['K_avg']\n",
    "        \n",
    "        # Modified SRK/T2 with optimized parameters\n",
    "        nc = nc_base_c + nc_cct_c * cct_norm\n",
    "        k_index = k_base_c + k_cct_c * cct_norm\n",
    "        acd_offset = acd_base_c + acd_cct_c * cct_norm\n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], K_avg=k_avg,\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Apply multiplicative correction\n",
    "        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio\n",
    "        after_mult = modified * mult_factor\n",
    "        \n",
    "        # Apply additive correction with polynomial\n",
    "        if best_degree == 'linear':\n",
    "            a0_c, a1_c, a2_c, a3_c = result_a_final.x\n",
    "            add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg\n",
    "        elif best_degree == 'quadratic':\n",
    "            a0_c, a1_c, a2_c, a3_c, a4_c = result_a_final.x\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2)\n",
    "        else:  # cubic\n",
    "            a0_c, a1_c, a2_c, a3_c, a4_c, a5_c = result_a_final.x\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2 + a5_c * cct_norm**3)\n",
    "        \n",
    "        final = after_mult + add_correction\n",
    "        predictions_combined_train.append(final)\n",
    "    \n",
    "    train_mae_combined = mean_absolute_error(X_train_comb['PostOP Spherical Equivalent'], \n",
    "                                            predictions_combined_train)\n",
    "    \n",
    "    # EVALUATE ON TEST SET\n",
    "    predictions_combined_test = []\n",
    "    for _, row in X_test_comb.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        k_avg = row['K_avg']\n",
    "        \n",
    "        # Modified SRK/T2 with optimized parameters\n",
    "        nc = nc_base_c + nc_cct_c * cct_norm\n",
    "        k_index = k_base_c + k_cct_c * cct_norm\n",
    "        acd_offset = acd_base_c + acd_cct_c * cct_norm\n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], K_avg=k_avg,\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Apply multiplicative correction\n",
    "        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio\n",
    "        after_mult = modified * mult_factor\n",
    "        \n",
    "        # Apply additive correction with polynomial\n",
    "        if best_degree == 'linear':\n",
    "            add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg\n",
    "        elif best_degree == 'quadratic':\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2)\n",
    "        else:  # cubic\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2 + a5_c * cct_norm**3)\n",
    "        \n",
    "        final = after_mult + add_correction\n",
    "        predictions_combined_test.append(final)\n",
    "    \n",
    "    test_mae_combined = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], \n",
    "                                           predictions_combined_test)\n",
    "    baseline_mae_combined = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], \n",
    "                                                X_test_comb['SRKT2_Baseline'])\n",
    "    \n",
    "    improvement_combined = ((baseline_mae_combined - test_mae_combined) / baseline_mae_combined) * 100\n",
    "    overfit_ratio = test_mae_combined / train_mae_combined if train_mae_combined > 0 else float('inf')\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Train MAE: {train_mae_combined:.4f} D\")\n",
    "    print(f\"  Test MAE:  {test_mae_combined:.4f} D\")\n",
    "    print(f\"  Baseline:  {baseline_mae_combined:.4f} D\")\n",
    "    print(f\"  Improvement: {improvement_combined:.1f}%\")\n",
    "    print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_combined.append({\n",
    "        'seed': SEED,\n",
    "        'param_values': result_p_final.x,\n",
    "        'mult_values': result_m_final.x,\n",
    "        'add_values': result_a_final.x,\n",
    "        'train_mae': train_mae_combined,\n",
    "        'test_mae': test_mae_combined,\n",
    "        'baseline_mae': baseline_mae_combined,\n",
    "        'improvement': improvement_combined,\n",
    "        'overfit_ratio': overfit_ratio\n",
    "    })\n",
    "    \n",
    "    seed_test_maes_combined.append(test_mae_combined)\n",
    "    seed_train_maes_combined.append(train_mae_combined)\n",
    "    seed_baseline_maes_combined.append(baseline_mae_combined)\n",
    "    seed_improvements_combined.append(improvement_combined)\n",
    "    seed_overfit_ratios_combined.append(overfit_ratio)\n",
    "    \n",
    "    seed_param_results.append(result_p_final.x)\n",
    "    seed_mult_results.append(result_m_final.x)\n",
    "    seed_add_results.append(result_a_final.x)\n",
    "\n",
    "# SUMMARY STATISTICS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MULTI-SEED SUMMARY - COMBINED APPROACH WITH {best_degree.upper()} ADDITIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Test MAE:     {np.mean(seed_test_maes_combined):.4f} Â± {np.std(seed_test_maes_combined):.4f} D\")\n",
    "print(f\"Train MAE:    {np.mean(seed_train_maes_combined):.4f} Â± {np.std(seed_train_maes_combined):.4f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(seed_baseline_maes_combined):.4f} Â± {np.std(seed_baseline_maes_combined):.4f} D\")\n",
    "print(f\"Improvement:  {np.mean(seed_improvements_combined):.1f}% Â± {np.std(seed_improvements_combined):.1f}%\")\n",
    "print(f\"Overfit ratio: {np.mean(seed_overfit_ratios_combined):.3f} Â± {np.std(seed_overfit_ratios_combined):.3f}\")\n",
    "\n",
    "# Parameter consistency analysis\n",
    "print(\"\\nğŸ”¬ PARAMETER CONSISTENCY:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "param_names = ['nc_base', 'nc_cct', 'k_base', 'k_cct', 'acd_base', 'acd_cct']\n",
    "param_array = np.array(seed_param_results)\n",
    "print(\"\\nParameter optimization values:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    values = param_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "mult_names = ['m0', 'm1_cct', 'm2_ratio']\n",
    "mult_array = np.array(seed_mult_results)\n",
    "print(\"\\nMultiplicative correction values:\")\n",
    "for i, name in enumerate(mult_names):\n",
    "    values = mult_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "add_array = np.array(seed_add_results)\n",
    "print(f\"\\nAdditive correction values ({best_degree}):\")\n",
    "if best_degree == 'linear':\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K']\n",
    "elif best_degree == 'quadratic':\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K', 'a4_cct2']\n",
    "else:  # cubic\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K', 'a4_cct2', 'a5_cct3']\n",
    "\n",
    "for i, name in enumerate(add_names):\n",
    "    values = add_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "# Clinical significance\n",
    "mae_mean = np.mean(seed_test_maes_combined)\n",
    "mae_std = np.std(seed_test_maes_combined)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLINICAL INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Combined approach with {best_degree} additive achieves:\")\n",
    "print(f\"   â€¢ Mean absolute error: {mae_mean:.3f} Â± {mae_std:.3f} D\")\n",
    "print(f\"   â€¢ {np.mean(seed_improvements_combined):.0f}% improvement over standard SRK/T2\")\n",
    "\n",
    "if mae_mean < 0.5:\n",
    "    print(\"   â€¢ EXCELLENT: Within Â±0.50 D target for most patients\")\n",
    "elif mae_mean < 0.75:\n",
    "    print(\"   â€¢ GOOD: Within Â±0.75 D for most patients\")\n",
    "else:\n",
    "    print(\"   â€¢ MODERATE: Further optimization may be beneficial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc887bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FULL COMBINATION WITH ALL METHODS - EXPERIMENTAL\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ WARNING: This combines redundant corrections (Multiplicative AND SVR)\n",
      "Both address CCT error - risk of overcorrection\n",
      "\n",
      "ğŸ”¬ TESTING HYPOTHESIS: Can redundant corrections complement each other?\n",
      "\n",
      "âœ… All prerequisite methods detected. Proceeding with full combination...\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS WITH ALL METHODS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.964, AL=0.133, K=0.830\n",
      "  Multiplicative: m0=-0.119, m1=0.117, m2=-0.035\n",
      "  Additive (Quad): a0=-0.011, a1=0.029, a2=0.093, a3=-0.061, a4=0.101\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 42:\n",
      "  Baseline MAE: 1.4849 D\n",
      "  Test MAE: 1.3844 D\n",
      "  Train MAE: 0.9794 D\n",
      "  Improvement: 6.8%\n",
      "  Overfit ratio: 0.707\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=1.512, AL=0.216, K=0.983\n",
      "  Multiplicative: m0=-0.001, m1=-0.004, m2=-0.041\n",
      "  Additive (Quad): a0=-0.093, a1=-0.148, a2=0.096, a3=-0.057, a4=0.084\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 123:\n",
      "  Baseline MAE: 1.2755 D\n",
      "  Test MAE: 1.0380 D\n",
      "  Train MAE: 0.9492 D\n",
      "  Improvement: 18.6%\n",
      "  Overfit ratio: 0.914\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.393, AL=0.042, K=0.229\n",
      "  Multiplicative: m0=-0.007, m1=0.006, m2=-0.039\n",
      "  Additive (Quad): a0=-0.804, a1=-0.438, a2=0.175, a3=-0.084, a4=0.066\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 456:\n",
      "  Baseline MAE: 1.6714 D\n",
      "  Test MAE: 1.2458 D\n",
      "  Train MAE: 0.8916 D\n",
      "  Improvement: 25.5%\n",
      "  Overfit ratio: 0.716\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.515, AL=0.034, K=0.284\n",
      "  Multiplicative: m0=-0.019, m1=-0.011, m2=-0.036\n",
      "  Additive (Quad): a0=0.041, a1=-0.126, a2=0.103, a3=-0.058, a4=-0.183\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 789:\n",
      "  Baseline MAE: 1.6185 D\n",
      "  Test MAE: 0.9499 D\n",
      "  Train MAE: 0.8813 D\n",
      "  Improvement: 41.3%\n",
      "  Overfit ratio: 0.928\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 72 train, 24 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=1.829, AL=0.144, K=1.538\n",
      "  Multiplicative: m0=-0.028, m1=0.095, m2=-0.039\n",
      "  Additive (Quad): a0=-0.312, a1=-0.425, a2=0.135, a3=-0.075, a4=0.161\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 2025:\n",
      "  Baseline MAE: 1.3566 D\n",
      "  Test MAE: 1.0697 D\n",
      "  Train MAE: 1.0011 D\n",
      "  Improvement: 21.1%\n",
      "  Overfit ratio: 0.936\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - ALL METHODS COMBINED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 1.1376 Â± 0.1564 D\n",
      "  Best MAE: 0.9499 D\n",
      "  Worst MAE: 1.3844 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 22.7 Â± 11.2%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.840\n",
      "  âŒ HIGH OVERFITTING DETECTED (ratio < 0.9)\n",
      "  The model performs much better on training than test data\n",
      "\n",
      "ğŸ”¬ INTERPRETATION:\n",
      "--------------------------------------------------\n",
      "âŒ ALL methods combined is 19.9% WORSE than standard combined\n",
      "   Redundant corrections lead to overcorrection\n",
      "\n",
      "âŒ Adding redundant corrections doesn't help\n",
      "   SVR or Multiplicative alone is sufficient for CCT correction\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION:\n",
      "================================================================================\n",
      "âŒ NOT RECOMMENDED - Too many parameters lead to overfitting\n",
      "\n",
      "This was an experimental test of redundant corrections.\n",
      "Generally, using either Multiplicative OR SVR (not both) is recommended.\n"
     ]
    }
   ],
   "source": [
    "# FULL COMBINATION - ALL METHODS INCLUDING MULTIPLICATIVE AND SVR\n",
    "# ================================================================\n",
    "# PURPOSE: Test if combining ALL correction methods provides additional benefit\n",
    "# This includes: Parameter + Multiplicative + SVR + Additive (Quadratic)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL COMBINATION WITH ALL METHODS - EXPERIMENTAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâš ï¸ WARNING: This combines redundant corrections (Multiplicative AND SVR)\")\n",
    "print(\"Both address CCT error - risk of overcorrection\")\n",
    "print(\"\\nğŸ”¬ TESTING HYPOTHESIS: Can redundant corrections complement each other?\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if previous methods have been run\n",
    "required_vars = ['seed_test_maes_param', 'seed_test_maes_mult', 'seed_test_maes_additive']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "if missing_vars:\n",
    "    print(f\"\\nâŒ ERROR: Missing variables: {missing_vars}\")\n",
    "    print(\"Please run Parameter, Multiplicative, SVR, and Additive cells first!\")\n",
    "else:\n",
    "    print(\"\\nâœ… All prerequisite methods detected. Proceeding with full combination...\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_test_maes_all = []\n",
    "    seed_train_maes_all = []\n",
    "    seed_baseline_maes_all = []\n",
    "    seed_improvements_all = []\n",
    "    seed_overfit_ratios_all = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING MULTI-SEED ANALYSIS WITH ALL METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_all, X_test_all = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        X_train_all['K_avg'] = (X_train_all['Bio-Ks'] + X_train_all['Bio-Kf']) / 2\n",
    "        X_test_all['K_avg'] = (X_test_all['Bio-Ks'] + X_test_all['Bio-Kf']) / 2\n",
    "        \n",
    "        print(f\"ğŸ“Š Split: {len(X_train_all)} train, {len(X_test_all)} test\")\n",
    "        \n",
    "        # Calculate baseline\n",
    "        for dataset in [X_train_all, X_test_all]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_all['PostOP Spherical Equivalent'], \n",
    "                                           X_test_all['SRKT2_Baseline'])\n",
    "        \n",
    "        # Setup K-fold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # Store fold results\n",
    "        param_results = []\n",
    "        mult_results = []\n",
    "        svr_models = []\n",
    "        svr_scalers = []\n",
    "        add_results = []\n",
    "        \n",
    "        print(\"\\nğŸ“ Training each method with 5-fold CV:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_all), 1):\n",
    "            print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "            \n",
    "            fold_train = X_train_all.iloc[train_idx]\n",
    "            fold_val = X_train_all.iloc[val_idx]\n",
    "            \n",
    "            # 1. PARAMETER OPTIMIZATION\n",
    "            def param_objective(params, df_data):\n",
    "                A_mod, AL_mod, K_mod = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    pred = calculate_SRKT2(\n",
    "                        AL=row['Bio-AL'] + AL_mod,\n",
    "                        K_avg=row['K_avg'] + K_mod,\n",
    "                        IOL_power=row['IOL Power'],\n",
    "                        A_constant=row['A-Constant'] + A_mod\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            param_result = minimize(\n",
    "                param_objective,\n",
    "                x0=[0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-2, 2), (-0.5, 0.5), (-2, 2)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            param_results.append(param_result.x)\n",
    "            \n",
    "            # 2. MULTIPLICATIVE CORRECTION\n",
    "            def mult_objective(params, df_data):\n",
    "                m0, m1, m2 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "                    predictions.append(base_pred * mult_factor)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            mult_result = minimize(\n",
    "                mult_objective,\n",
    "                x0=[0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-0.5, 0.5), (-0.5, 0.5), (-10, 10)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            mult_results.append(mult_result.x)\n",
    "            \n",
    "            # 3. SVR CORRECTION\n",
    "            # Prepare features\n",
    "            X_svr = pd.DataFrame()\n",
    "            X_svr['CCT_norm'] = (fold_train['CCT'] - 600) / 100\n",
    "            X_svr['AL'] = fold_train['Bio-AL']\n",
    "            X_svr['ACD'] = fold_train['Bio-ACD']\n",
    "            X_svr['K_mean'] = fold_train['K_avg']\n",
    "            X_svr['CCT_AL_ratio'] = fold_train['CCT'] / fold_train['Bio-AL']\n",
    "            \n",
    "            # Calculate residuals (what SVR needs to correct)\n",
    "            y_svr = fold_train['PostOP Spherical Equivalent'] - fold_train['SRKT2_Baseline']\n",
    "            \n",
    "            # Train SVR\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_svr)\n",
    "            \n",
    "            svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "            svr.fit(X_scaled, y_svr)\n",
    "            \n",
    "            svr_models.append(svr)\n",
    "            svr_scalers.append(scaler)\n",
    "            \n",
    "            # 4. ADDITIVE CORRECTION (Quadratic)\n",
    "            def add_objective(params, df_data):\n",
    "                a0, a1, a2, a3, a4 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    # Quadratic correction\n",
    "                    correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_result = minimize(\n",
    "                add_objective,\n",
    "                x0=[0, 0, 0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-2, 2)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            add_results.append(add_result.x)\n",
    "            \n",
    "            print(\"âœ“ \", end=\"\")\n",
    "        \n",
    "        print(\"\\n\\nğŸ”„ Combining all methods...\")\n",
    "        \n",
    "        # Average parameters across folds\n",
    "        avg_param = np.mean(param_results, axis=0)\n",
    "        avg_mult = np.mean(mult_results, axis=0)\n",
    "        avg_add = np.mean(add_results, axis=0)\n",
    "        \n",
    "        A_mod, AL_mod, K_mod = avg_param\n",
    "        m0, m1, m2 = avg_mult\n",
    "        a0, a1, a2, a3, a4 = avg_add\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Optimized Parameters:\")\n",
    "        print(f\"  Parameter: A={A_mod:.3f}, AL={AL_mod:.3f}, K={K_mod:.3f}\")\n",
    "        print(f\"  Multiplicative: m0={m0:.3f}, m1={m1:.3f}, m2={m2:.3f}\")\n",
    "        print(f\"  Additive (Quad): a0={a0:.3f}, a1={a1:.3f}, a2={a2:.3f}, a3={a3:.3f}, a4={a4:.3f}\")\n",
    "        \n",
    "        # Apply ALL corrections to test set\n",
    "        print(\"\\nğŸ¯ Applying full combination to test set:\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        for _, row in X_test_all.iterrows():\n",
    "            # Step 1: Parameter optimization\n",
    "            param_modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'] + AL_mod,\n",
    "                K_avg=row['K_avg'] + K_mod,\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + A_mod\n",
    "            )\n",
    "            \n",
    "            # Step 2: Multiplicative correction\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = param_modified * mult_factor\n",
    "            \n",
    "            # Step 3: SVR correction (average across fold models)\n",
    "            X_svr_test = pd.DataFrame({\n",
    "                'CCT_norm': [cct_norm],\n",
    "                'AL': [row['Bio-AL']],\n",
    "                'ACD': [row['Bio-ACD']],\n",
    "                'K_mean': [row['K_avg']],\n",
    "                'CCT_AL_ratio': [cct_ratio]\n",
    "            })\n",
    "            \n",
    "            svr_corrections = []\n",
    "            for svr_model, scaler in zip(svr_models, svr_scalers):\n",
    "                X_scaled = scaler.transform(X_svr_test)\n",
    "                svr_pred = svr_model.predict(X_scaled)[0]\n",
    "                svr_corrections.append(svr_pred)\n",
    "            avg_svr_correction = np.mean(svr_corrections)\n",
    "            \n",
    "            after_svr = after_mult + avg_svr_correction\n",
    "            \n",
    "            # Step 4: Additive quadratic correction\n",
    "            add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "            \n",
    "            final_prediction = after_svr + add_correction\n",
    "            all_predictions.append(final_prediction)\n",
    "        \n",
    "        # Calculate performance on test set\n",
    "        test_mae = mean_absolute_error(X_test_all['PostOP Spherical Equivalent'], all_predictions)\n",
    "        \n",
    "        # Calculate on training set for overfitting check\n",
    "        train_predictions = []\n",
    "        for _, row in X_train_all.iterrows():\n",
    "            param_modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'] + AL_mod,\n",
    "                K_avg=row['K_avg'] + K_mod,\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + A_mod\n",
    "            )\n",
    "            \n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = param_modified * mult_factor\n",
    "            \n",
    "            X_svr_test = pd.DataFrame({\n",
    "                'CCT_norm': [cct_norm],\n",
    "                'AL': [row['Bio-AL']],\n",
    "                'ACD': [row['Bio-ACD']],\n",
    "                'K_mean': [row['K_avg']],\n",
    "                'CCT_AL_ratio': [cct_ratio]\n",
    "            })\n",
    "            \n",
    "            svr_corrections = []\n",
    "            for svr_model, scaler in zip(svr_models, svr_scalers):\n",
    "                X_scaled = scaler.transform(X_svr_test)\n",
    "                svr_pred = svr_model.predict(X_scaled)[0]\n",
    "                svr_corrections.append(svr_pred)\n",
    "            avg_svr_correction = np.mean(svr_corrections)\n",
    "            \n",
    "            after_svr = after_mult + avg_svr_correction\n",
    "            add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "            \n",
    "            final_prediction = after_svr + add_correction\n",
    "            train_predictions.append(final_prediction)\n",
    "        \n",
    "        train_mae = mean_absolute_error(X_train_all['PostOP Spherical Equivalent'], train_predictions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "        overfit_ratio = train_mae / test_mae\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ RESULTS FOR SEED {SEED}:\")\n",
    "        print(f\"  Baseline MAE: {baseline_mae:.4f} D\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f} D\")\n",
    "        print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")\n",
    "        print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "        \n",
    "        # Store results\n",
    "        seed_test_maes_all.append(test_mae)\n",
    "        seed_train_maes_all.append(train_mae)\n",
    "        seed_baseline_maes_all.append(baseline_mae)\n",
    "        seed_improvements_all.append(improvement)\n",
    "        seed_overfit_ratios_all.append(overfit_ratio)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-SEED SUMMARY - ALL METHODS COMBINED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TEST SET PERFORMANCE (n={len(SEEDS)} seeds):\")\n",
    "    print(f\"  Mean MAE: {np.mean(seed_test_maes_all):.4f} Â± {np.std(seed_test_maes_all):.4f} D\")\n",
    "    print(f\"  Best MAE: {np.min(seed_test_maes_all):.4f} D\")\n",
    "    print(f\"  Worst MAE: {np.max(seed_test_maes_all):.4f} D\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "    print(f\"  Mean: {np.mean(seed_improvements_all):.1f} Â± {np.std(seed_improvements_all):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ OVERFITTING ANALYSIS:\")\n",
    "    print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_all):.3f}\")\n",
    "    if np.mean(seed_overfit_ratios_all) < 0.9:\n",
    "        print(\"  âŒ HIGH OVERFITTING DETECTED (ratio < 0.9)\")\n",
    "        print(\"  The model performs much better on training than test data\")\n",
    "    elif np.mean(seed_overfit_ratios_all) < 0.95:\n",
    "        print(\"  âš ï¸ MODERATE OVERFITTING (ratio 0.9-0.95)\")\n",
    "    else:\n",
    "        print(\"  âœ… Low overfitting - good generalization\")\n",
    "    \n",
    "    print(\"\\nğŸ”¬ INTERPRETATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare with other methods if available\n",
    "    if 'seed_test_maes_combined' in locals():\n",
    "        standard_combined_mae = np.mean(seed_test_maes_combined)\n",
    "        all_methods_mae = np.mean(seed_test_maes_all)\n",
    "        \n",
    "        if all_methods_mae < standard_combined_mae:\n",
    "            improvement_vs_standard = ((standard_combined_mae - all_methods_mae) / standard_combined_mae) * 100\n",
    "            print(f\"âœ… ALL methods combined BEATS standard combined by {improvement_vs_standard:.1f}%\")\n",
    "            print(\"   Redundant corrections appear to be complementary!\")\n",
    "        else:\n",
    "            worse_by = ((all_methods_mae - standard_combined_mae) / standard_combined_mae) * 100\n",
    "            print(f\"âŒ ALL methods combined is {worse_by:.1f}% WORSE than standard combined\")\n",
    "            print(\"   Redundant corrections lead to overcorrection\")\n",
    "    \n",
    "    if 'seed_test_maes_mult' in locals():\n",
    "        svr_mae = np.mean(seed_test_maes_mult)\n",
    "        all_methods_mae = np.mean(seed_test_maes_all)\n",
    "        \n",
    "        if all_methods_mae < svr_mae:\n",
    "            print(f\"\\nâœ… Full combination outperforms SVR/Mult alone\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ Adding redundant corrections doesn't help\")\n",
    "            print(\"   SVR or Multiplicative alone is sufficient for CCT correction\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if np.mean(seed_overfit_ratios_all) < 0.9:\n",
    "        print(\"âŒ NOT RECOMMENDED - Too many parameters lead to overfitting\")\n",
    "    elif 'seed_test_maes_combined' in locals() and np.mean(seed_test_maes_all) > np.mean(seed_test_maes_combined):\n",
    "        print(\"âŒ NOT RECOMMENDED - Standard combined (without redundancy) performs better\")\n",
    "    else:\n",
    "        print(\"ğŸ¤” FURTHER TESTING NEEDED - Results are inconclusive\")\n",
    "    \n",
    "    print(\"\\nThis was an experimental test of redundant corrections.\")\n",
    "    print(\"Generally, using either Multiplicative OR SVR (not both) is recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3yxaies4nqp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-SEED ANALYSIS - COMPREHENSIVE COMPARISON OF ALL METHODS\n",
      "================================================================================\n",
      "âœ… Included: Multiplicative Correction\n",
      "âœ… Included: Gaussian Process\n",
      "âœ… Included: SVR Correction\n",
      "âœ… Included: Additive Correction\n",
      "âœ… Included: Full Combined\n",
      "âœ… Included: Random Forest\n",
      "âœ… Included: XGBoost\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY ACROSS ALL METHODS\n",
      "================================================================================\n",
      "\n",
      "Method                         Test MAE        Train MAE  Improvement     Overfit   \n",
      "--------------------------------------------------------------------------------\n",
      "SVR                            0.9066 Â± 0.1797 0.6825     38.8%           1.329     \n",
      "Full Combined                  0.9491 Â± 0.1089 0.9010     35.9%           1.056     \n",
      "XGBoost                        0.9553 Â± 0.0498 0.8172     35.5%           0.859     \n",
      "Gaussian Process               0.9729 Â± 0.0407 0.0811     34.3%           0.084     \n",
      "Random Forest                  0.9890 Â± 0.1018 0.6650     33.2%           0.684     \n",
      "Multiplicative                 1.0108 Â± 0.0679 0.9037     31.8%           12.099    \n",
      "Parameter Opt                  1.3205 Â± 0.1738 1.1668     10.9%           14.080    \n",
      "Baseline SRK/T2                1.4814 Â± 0.1503 N/A        0.0%            N/A       \n",
      "Additive                       1.5038 Â± 0.1776 1.2682     -1.5%           1.191     \n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS FROM MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š BEST PERFORMER: SVR\n",
      "   - Test MAE: 0.9066 Â± 0.1797 D\n",
      "   - Improvement over baseline: 38.8%\n",
      "\n",
      "ğŸ“Š SECOND BEST: Full Combined\n",
      "   - Test MAE: 0.9491 Â± 0.1089 D\n",
      "   - Improvement over baseline: 35.9%\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "   âš ï¸ SVR: High overfitting (ratio: 1.329)\n",
      "   âœ… Full Combined: Low overfitting (ratio: 1.056)\n",
      "   âœ… XGBoost: Low overfitting (ratio: 0.859)\n",
      "   âœ… Gaussian Process: Low overfitting (ratio: 0.084)\n",
      "   âœ… Random Forest: Low overfitting (ratio: 0.684)\n",
      "\n",
      "ğŸ¥ CLINICAL RELEVANCE:\n",
      "   Baseline SRK/T2 MAE: 1.4814 D\n",
      "   Best method MAE: 0.9066 D\n",
      "   Absolute improvement: 0.5748 D\n",
      "   Relative improvement: 38.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… MULTI-SEED COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# MULTI-SEED COMPARISON - FINAL COMPREHENSIVE SUMMARY\n",
    "# ====================================================\n",
    "# PURPOSE: Compare ALL methods across multiple seeds for robust conclusions\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-SEED ANALYSIS - COMPREHENSIVE COMPARISON OF ALL METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compile all results into a comparison table\n",
    "all_methods = {}\n",
    "\n",
    "# Using GLOBAL_BASELINE_MAE for all improvement calculations\n",
    "if 'GLOBAL_BASELINE_MAE' not in locals():\n",
    "    print('WARNING: Global baseline not calculated! Run Cell 3 first.')\n",
    "    GLOBAL_BASELINE_MAE = 1.4814  # Default fallback\n",
    "\n",
    "\n",
    "# 1. Baseline (no optimization)\n",
    "if 'seed_baseline_maes_param' in locals():\n",
    "    all_methods['Baseline SRK/T2'] = {\n",
    "        'test_mae': np.mean(seed_baseline_maes_param),\n",
    "        'test_std': np.std(seed_baseline_maes_param),\n",
    "        'train_mae': np.nan,  # Baseline doesn't have training\n",
    "        'improvement': 0.0,\n",
    "        'overfit_ratio': np.nan\n",
    "    }\n",
    "\n",
    "# 2. Parameter Optimization\n",
    "if 'seed_test_maes_param' in locals():\n",
    "    all_methods['Parameter Opt'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_param),\n",
    "        'test_std': np.std(seed_test_maes_param),\n",
    "        'train_mae': np.mean(seed_train_maes_param),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_param)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_param),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_param)\n",
    "    }\n",
    "\n",
    "# 3. Multiplicative Correction\n",
    "if 'seed_test_maes_mult' in locals():\n",
    "    all_methods['Multiplicative'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_mult),\n",
    "        'test_std': np.std(seed_test_maes_mult),\n",
    "        'train_mae': np.mean(seed_train_maes_mult),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_mult)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_mult),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_mult)\n",
    "    }\n",
    "    print('âœ… Included: Multiplicative Correction')\n",
    "\n",
    "# 3a. Gaussian Process (if available)\n",
    "if 'seed_test_maes_gpr' in locals():\n",
    "    all_methods['Gaussian Process'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_gpr),\n",
    "        'test_std': np.std(seed_test_maes_gpr),\n",
    "        'train_mae': np.mean(seed_train_maes_gpr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_gpr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_gpr),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_gpr)\n",
    "    }\n",
    "    print('âœ… Included: Gaussian Process')\n",
    "\n",
    "\n",
    "# 3b. SVR Correction (FIXED)\n",
    "if 'seed_test_maes_svr' in locals():\n",
    "    all_test_svr = [m for s in seed_test_maes_svr for m in (s if isinstance(s, list) else [s])]\n",
    "    all_train_svr = [m for s in seed_train_maes_svr for m in (s if isinstance(s, list) else [s])]\n",
    "    all_methods['SVR'] = {\n",
    "        'test_mae': np.mean(all_test_svr),\n",
    "        'test_std': np.std(all_test_svr),\n",
    "        'train_mae': np.mean(all_train_svr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(all_test_svr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else 0,\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_svr) if 'seed_overfit_ratios_svr' in locals() else np.nan\n",
    "    }\n",
    "    print('âœ… Included: SVR Correction')\n",
    "\n",
    "# 4. Additive Correction\n",
    "if 'seed_test_maes_additive' in locals():\n",
    "    all_methods['Additive'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_additive),\n",
    "        'test_std': np.std(seed_test_maes_additive),\n",
    "        'train_mae': np.mean(seed_train_maes_additive),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_additive)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_additive),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_additive)\n",
    "    }\n",
    "    print('âœ… Included: Additive Correction')\n",
    "\n",
    "# 5. Combined Approaches\n",
    "if 'seed_test_maes_combined_mult' in locals():\n",
    "    all_methods['Param + Multiplicative'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_mult),\n",
    "        'test_std': np.std(seed_test_maes_combined_mult),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_mult),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_mult)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_mult),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_mult)\n",
    "    }\n",
    "    print('âœ… Included: Parameter + Multiplicative Combined')\n",
    "\n",
    "if 'seed_test_maes_combined_svr' in locals():\n",
    "    all_methods['Param + SVR'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_svr),\n",
    "        'test_std': np.std(seed_test_maes_combined_svr),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_svr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_svr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_svr),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_svr)\n",
    "    }\n",
    "    print('âœ… Included: Parameter + SVR Combined')\n",
    "\n",
    "# 6. Full Combined (additive + multiplicative + parameter)\n",
    "if 'seed_test_maes_combined' in locals():\n",
    "    all_methods['Full Combined'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined),\n",
    "        'test_std': np.std(seed_test_maes_combined),\n",
    "        'train_mae': np.mean(seed_train_maes_combined),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined)\n",
    "    }\n",
    "    print('âœ… Included: Full Combined')\n",
    "\n",
    "# 6b. Full Combined with Quadratic\n",
    "if 'seed_test_maes_combined_quadratic' in locals():\n",
    "    all_methods['Full Combined (quadratic)'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_quadratic),\n",
    "        'test_std': np.std(seed_test_maes_combined_quadratic),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_quadratic),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_quadratic)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_quadratic),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_quadratic)\n",
    "    }\n",
    "    print('âœ… Included: Full Combined (Quadratic)')\n",
    "\n",
    "# 7. Random Forest\n",
    "if 'seed_test_maes_rf' in locals():\n",
    "    all_methods['Random Forest'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_rf),\n",
    "        'test_std': np.std(seed_test_maes_rf),\n",
    "        'train_mae': np.mean(seed_train_maes_rf),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_rf)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_rf),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_rf)\n",
    "    }\n",
    "    print('âœ… Included: Random Forest')\n",
    "\n",
    "# 8. XGBoost\n",
    "if 'seed_test_maes_xgb' in locals():\n",
    "    all_methods['XGBoost'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_xgb),\n",
    "        'test_std': np.std(seed_test_maes_xgb),\n",
    "        'train_mae': np.mean(seed_train_maes_xgb),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_xgb)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_xgb),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_xgb)\n",
    "    }\n",
    "    print('âœ… Included: XGBoost')\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY ACROSS ALL METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sort by test MAE\n",
    "sorted_methods = sorted(all_methods.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "# Display table\n",
    "print(f\"\\n{'Method':<30} {'Test MAE':<15} {'Train MAE':<10} {'Improvement':<15} {'Overfit':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method_name, metrics in sorted_methods:\n",
    "    test_mae_str = f\"{metrics['test_mae']:.4f} Â± {metrics['test_std']:.4f}\"\n",
    "    train_mae_str = f\"{metrics['train_mae']:.4f}\" if not np.isnan(metrics['train_mae']) else \"N/A\"\n",
    "    improvement_str = f\"{metrics['improvement']:.1f}%\" if not np.isnan(metrics['improvement']) else \"N/A\"\n",
    "    overfit_str = f\"{metrics['overfit_ratio']:.3f}\" if not np.isnan(metrics['overfit_ratio']) else \"N/A\"\n",
    "    \n",
    "    print(f\"{method_name:<30} {test_mae_str:<15} {train_mae_str:<10} {improvement_str:<15} {overfit_str:<10}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS FROM MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_method = sorted_methods[0]\n",
    "worst_method = sorted_methods[-1]\n",
    "\n",
    "print(f\"\\nğŸ“Š BEST PERFORMER: {best_method[0]}\")\n",
    "print(f\"   - Test MAE: {best_method[1]['test_mae']:.4f} Â± {best_method[1]['test_std']:.4f} D\")\n",
    "print(f\"   - Improvement over baseline: {best_method[1]['improvement']:.1f}%\")\n",
    "\n",
    "if len(sorted_methods) > 1:\n",
    "    print(f\"\\nğŸ“Š SECOND BEST: {sorted_methods[1][0]}\")\n",
    "    print(f\"   - Test MAE: {sorted_methods[1][1]['test_mae']:.4f} Â± {sorted_methods[1][1]['test_std']:.4f} D\")\n",
    "    print(f\"   - Improvement over baseline: {sorted_methods[1][1]['improvement']:.1f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "for method_name, metrics in sorted_methods[:5]:  # Top 5 methods\n",
    "    if not np.isnan(metrics['overfit_ratio']):\n",
    "        if metrics['overfit_ratio'] > 1.2:\n",
    "            print(f\"   âš ï¸ {method_name}: High overfitting (ratio: {metrics['overfit_ratio']:.3f})\")\n",
    "        elif metrics['overfit_ratio'] < 1.1:\n",
    "            print(f\"   âœ… {method_name}: Low overfitting (ratio: {metrics['overfit_ratio']:.3f})\")\n",
    "\n",
    "# Clinical relevance\n",
    "print(\"\\nğŸ¥ CLINICAL RELEVANCE:\")\n",
    "print(f\"   Baseline SRK/T2 MAE: {GLOBAL_BASELINE_MAE:.4f} D\")\n",
    "print(f\"   Best method MAE: {best_method[1]['test_mae']:.4f} D\")\n",
    "print(f\"   Absolute improvement: {GLOBAL_BASELINE_MAE - best_method[1]['test_mae']:.4f} D\")\n",
    "print(f\"   Relative improvement: {best_method[1]['improvement']:.1f}%\")\n",
    "\n",
    "# Visualizations\n",
    "if len(all_methods) > 1:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Test MAE comparison\n",
    "    methods = list(all_methods.keys())\n",
    "    test_maes = [all_methods[m]['test_mae'] for m in methods]\n",
    "    test_stds = [all_methods[m]['test_std'] for m in methods]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.barh(methods, test_maes, xerr=test_stds, capsize=5)\n",
    "    ax1.set_xlabel('Test MAE (D)')\n",
    "    ax1.set_title('Test Performance Comparison')\n",
    "    ax1.axvline(x=GLOBAL_BASELINE_MAE, color='r', linestyle='--', label='Baseline', alpha=0.5)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Color bars by performance\n",
    "    for i, bar in enumerate(bars1):\n",
    "        if test_maes[i] < 0.95:\n",
    "            bar.set_color('green')\n",
    "        elif test_maes[i] < 1.0:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    # 2. Improvement comparison\n",
    "    improvements = [all_methods[m]['improvement'] for m in methods]\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(methods, improvements)\n",
    "    ax2.set_xlabel('Improvement (%)')\n",
    "    ax2.set_title('Improvement Over Baseline')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Color bars by improvement\n",
    "    for i, bar in enumerate(bars2):\n",
    "        if improvements[i] > 35:\n",
    "            bar.set_color('green')\n",
    "        elif improvements[i] > 30:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    # 3. Overfitting analysis\n",
    "    overfit_ratios = [all_methods[m]['overfit_ratio'] if not np.isnan(all_methods[m]['overfit_ratio']) else 0 for m in methods]\n",
    "    methods_with_overfit = [m for m, o in zip(methods, overfit_ratios) if o > 0]\n",
    "    overfit_values = [o for o in overfit_ratios if o > 0]\n",
    "    \n",
    "    if overfit_values:\n",
    "        ax3 = axes[2]\n",
    "        bars3 = ax3.barh(methods_with_overfit, overfit_values)\n",
    "        ax3.set_xlabel('Overfitting Ratio')\n",
    "        ax3.set_title('Overfitting Analysis')\n",
    "        ax3.axvline(x=1.0, color='green', linestyle='--', label='No overfit', alpha=0.5)\n",
    "        ax3.axvline(x=1.2, color='red', linestyle='--', label='High overfit', alpha=0.5)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Color bars by overfitting\n",
    "        for i, bar in enumerate(bars3):\n",
    "            if overfit_values[i] < 1.1:\n",
    "                bar.set_color('green')\n",
    "            elif overfit_values[i] < 1.2:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… MULTI-SEED COMPARISON COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
