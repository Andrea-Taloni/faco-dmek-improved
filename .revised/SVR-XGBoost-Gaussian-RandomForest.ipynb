{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41782613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ MULTI-SEED CONFIGURATION\n",
      "======================================================================\n",
      "Seeds for validation: [42, 123, 456, 789, 2025]\n",
      "This ensures results are not dependent on random split\n",
      "Each seed creates different train/test splits for robust assessment\n",
      "======================================================================\n",
      "IOL CALCULATION FOR PRE-DMEK PATIENTS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š WHAT WE'RE DOING:\n",
      "--------------------------------------------------\n",
      "â€¢ Loading data from Fuchs' dystrophy patients\n",
      "â€¢ These patients had combined cataract + DMEK surgery\n",
      "â€¢ Goal: Improve IOL power calculation accuracy\n",
      "â€¢ Challenge: Edematous corneas distort standard formulas\n",
      "â€¢ NEW: Using 5 different seeds for robust validation\n",
      "\n",
      "âœ… Loaded 88 patients from FacoDMEK.xlsx\n",
      "\n",
      "ğŸ” KEY MEASUREMENTS IN OUR DATA:\n",
      "--------------------------------------------------\n",
      "â€¢ Bio-AL: Axial length (mm)\n",
      "â€¢ Bio-Ks/Kf: Steep and flat keratometry (D)\n",
      "â€¢ CCT: Central corneal thickness (Î¼m) - KEY for edema\n",
      "â€¢ IOL Power: Implanted lens power (D)\n",
      "â€¢ PostOP Spherical Equivalent: Actual outcome (D)\n"
     ]
    }
   ],
   "source": [
    "# IOL CALCULATION FOR PRE-DMEK PATIENTS - SETUP AND DATA LOADING\n",
    "# ================================================================\n",
    "# PURPOSE: Set up the analysis environment and load patient data\n",
    "# This notebook optimizes IOL power calculations for Fuchs' dystrophy patients\n",
    "# undergoing combined phacoemulsification and DMEK surgery\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants for clinical accuracy thresholds (diopters)\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 1.00]\n",
    "TEST_SIZE = 0.25      # 25% holdout for final testing\n",
    "N_FOLDS = 5           # 5-fold cross-validation\n",
    "\n",
    "# MULTI-SEED CONFIGURATION FOR ROBUST VALIDATION\n",
    "#SEEDS = [42]  # Quick test with single seed\n",
    "#SEEDS = [42, 123]  # Medium test with 2 seeds\n",
    "SEEDS = [42, 123, 456, 789, 2025]  # Multiple seeds for statistical robustness\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ MULTI-SEED CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Seeds for validation: {SEEDS}\")\n",
    "print(\"This ensures results are not dependent on random split\")\n",
    "print(\"Each seed creates different train/test splits for robust assessment\")\n",
    "\n",
    "# Storage for multi-seed results\n",
    "multi_seed_results = {\n",
    "\n",
    "    'parameter': {},\n",
    "    'multiplicative': {},\n",
    "    'additive': {},\n",
    "    'combined': {},\n",
    "    'fixed_combined': {}\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IOL CALCULATION FOR PRE-DMEK PATIENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“Š WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Loading data from Fuchs' dystrophy patients\")\n",
    "print(\"â€¢ These patients had combined cataract + DMEK surgery\")\n",
    "print(\"â€¢ Goal: Improve IOL power calculation accuracy\")\n",
    "print(\"â€¢ Challenge: Edematous corneas distort standard formulas\")\n",
    "print(f\"â€¢ NEW: Using {len(SEEDS)} different seeds for robust validation\")\n",
    "\n",
    "# Load the patient data\n",
    "df = pd.read_excel('FacoDMEK.xlsx')\n",
    "print(f\"\\nâœ… Loaded {len(df)} patients from FacoDMEK.xlsx\")\n",
    "\n",
    "print(\"\\nğŸ” KEY MEASUREMENTS IN OUR DATA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Bio-AL: Axial length (mm)\")\n",
    "print(\"â€¢ Bio-Ks/Kf: Steep and flat keratometry (D)\")\n",
    "print(\"â€¢ CCT: Central corneal thickness (Î¼m) - KEY for edema\")\n",
    "print(\"â€¢ IOL Power: Implanted lens power (D)\")\n",
    "print(\"â€¢ PostOP Spherical Equivalent: Actual outcome (D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9871e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SRK/T2 FORMULA (Sheard et al. 2010)\n",
      "======================================================================\n",
      "â€¢ SKR/T2 assumes normal corneal properties\n",
      "â€¢ In Fuchs' dystrophy, the cornea is NOT normal:\n",
      "  - Edema changes refractive index (nc)\n",
      "  - Swelling alters keratometric index (k_index)\n",
      "  - Anterior chamber depth is affected\n",
      "\n",
      "Our strategy: Keep the formula structure, optimize the parameters!\n",
      "\n",
      "ğŸ“ THE SRK/T2 FORMULA:\n",
      "\n",
      "         1000Â·nâ‚Â·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) - PÂ·(Lopt - ACDest)Â·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest)\n",
      "REF = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       nâ‚Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) + LoptÂ·r) - 0.001Â·PÂ·(Lopt - ACDest)Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest) + ACDestÂ·r)\n"
     ]
    }
   ],
   "source": [
    "# STANDARD SRK/T2 FORMULA IMPLEMENTATION\n",
    "# ========================================\n",
    "# PURPOSE: Implement the baseline SRK/T2 formula (Sheard et al. 2010)\n",
    "# This is the current gold standard for IOL calculations\n",
    "# We'll use this as our baseline to compare improvements against\n",
    "\n",
    "def calculate_SRKT2(AL, K_avg, IOL_power, A_constant, nc=1.333, k_index=1.3375):\n",
    "    \"\"\"\n",
    "    SRK/T2 Formula (Sheard et al. 2010)\n",
    "    - Assumes NORMAL corneas (nc=1.333, k_index=1.3375)\n",
    "    - These assumptions fail in edematous Fuchs' corneas\n",
    "    \n",
    "    Parameters:\n",
    "    - AL: Axial length (mm)\n",
    "    - K_avg: Average keratometry (D)\n",
    "    - IOL_power: IOL power (D)\n",
    "    - A_constant: Lens-specific constant\n",
    "    - nc: Corneal refractive index (we'll optimize this!)\n",
    "    - k_index: Keratometric index (we'll optimize this too!)\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    na = 1.336  # Aqueous/vitreous refractive index\n",
    "    V = 12      # Vertex distance (mm)\n",
    "    ncm1 = nc - 1\n",
    "    \n",
    "    # Convert keratometry to radius using keratometric index\n",
    "    # This is where edema causes problems - k_index assumes normal cornea!\n",
    "    r = (k_index - 1) * 1000 / K_avg\n",
    "    \n",
    "    # Axial length correction for long eyes\n",
    "    if AL <= 24.2:\n",
    "        LCOR = AL\n",
    "    else:\n",
    "        LCOR = 3.446 + 1.716 * AL - 0.0237 * AL * AL\n",
    "    \n",
    "    # H2 calculation (corneal height) - Sheard's modification\n",
    "    H2 = -10.326 + 0.32630 * LCOR + 0.13533 * K_avg\n",
    "    \n",
    "    # ACD (Anterior Chamber Depth) estimation\n",
    "    # Edema can affect this too!\n",
    "    ACD_const = 0.62467 * A_constant - 68.747\n",
    "    offset = ACD_const - 3.336\n",
    "    ACD_est = H2 + offset\n",
    "    \n",
    "    # Retinal thickness correction\n",
    "    RETHICK = 0.65696 - 0.02029 * AL\n",
    "    LOPT = AL + RETHICK  # Optical axial length\n",
    "    \n",
    "    # SRK/T2 refraction calculation - the complex optics formula\n",
    "    numerator = (1000 * na * (na * r - ncm1 * LOPT) - \n",
    "                 IOL_power * (LOPT - ACD_est) * (na * r - ncm1 * ACD_est))\n",
    "    \n",
    "    denominator = (na * (V * (na * r - ncm1 * LOPT) + LOPT * r) - \n",
    "                   0.001 * IOL_power * (LOPT - ACD_est) * \n",
    "                   (V * (na * r - ncm1 * ACD_est) + ACD_est * r))\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SRK/T2 FORMULA (Sheard et al. 2010)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"â€¢ SKR/T2 assumes normal corneal properties\")\n",
    "print(\"â€¢ In Fuchs' dystrophy, the cornea is NOT normal:\")\n",
    "print(\"  - Edema changes refractive index (nc)\")\n",
    "print(\"  - Swelling alters keratometric index (k_index)\")\n",
    "print(\"  - Anterior chamber depth is affected\")\n",
    "print(\"\\nOur strategy: Keep the formula structure, optimize the parameters!\")\n",
    "\n",
    "print(\"\\nğŸ“ THE SRK/T2 FORMULA:\")\n",
    "print()\n",
    "print(\"         1000Â·nâ‚Â·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) - PÂ·(Lopt - ACDest)Â·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest)\")\n",
    "print(\"REF = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"       nâ‚Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·Lopt) + LoptÂ·r) - 0.001Â·PÂ·(Lopt - ACDest)Â·(VÂ·(nâ‚Â·r - ncâ‚‹â‚Â·ACDest) + ACDestÂ·r)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db415cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASELINE SRK/T2 PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ WHAT WE'RE DOING:\n",
      "--------------------------------------------------\n",
      "1. Calculate average K from steep and flat readings\n",
      "2. Apply standard SRK/T2 to all 96 patients\n",
      "3. Compare predictions to actual outcomes\n",
      "4. Measure error to establish baseline performance\n",
      "\n",
      "ğŸ“Š BASELINE PERFORMANCE METRICS:\n",
      "======================================================================\n",
      "  Mean Absolute Error (MAE):     1.2144 D\n",
      "  Mean Error (ME):                -0.1786 D\n",
      "  Standard Deviation (SD):        1.5836 D\n",
      "  Median Absolute Error:          0.9251 D\n",
      "\n",
      "ğŸ’¡ INTERPRETATION:\n",
      "--------------------------------------------------\n",
      "â€¢ MAE of 1.21 D is POOR (>1.0 D is clinically unacceptable)\n",
      "\n",
      "ğŸ“ˆ CLINICAL ACCURACY:\n",
      "----------------------------------------------------------------------\n",
      "  Within Â±0.25 D:  14.8% of eyes\n",
      "  Within Â±0.50 D:  28.4% of eyes\n",
      "  Within Â±0.75 D:  38.6% of eyes\n",
      "  Within Â±1.00 D:  53.4% of eyes\n",
      "\n",
      "ğŸ¯ CLINICAL TARGETS:\n",
      "--------------------------------------------------\n",
      "â€¢ Modern standard: >70% within Â±0.50 D\n",
      "â€¢ Acceptable: >90% within Â±1.00 D\n",
      "â€¢ Our baseline: 28.4% within Â±0.50 D\n",
      "\n",
      "âš ï¸ Standard SRK/T2 clearly struggles with Fuchs' dystrophy!\n",
      "This is why we need optimization!\n"
     ]
    }
   ],
   "source": [
    "# BASELINE PERFORMANCE EVALUATION\n",
    "# =================================\n",
    "# PURPOSE: Calculate how well standard SRK/T2 performs on our Fuchs' patients\n",
    "# This establishes the baseline that we need to beat\n",
    "# Spoiler: It won't be great due to the edematous corneas!\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE SRK/T2 PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ WHAT WE'RE DOING:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Calculate average K from steep and flat readings\")\n",
    "print(\"2. Apply standard SRK/T2 to all 96 patients\")\n",
    "print(\"3. Compare predictions to actual outcomes\")\n",
    "print(\"4. Measure error to establish baseline performance\")\n",
    "\n",
    "# Calculate average K (needed for SRK/T2)\n",
    "df['K_avg'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "\n",
    "# Apply standard SRK/T2 formula to all patients\n",
    "df['SRKT2_Prediction'] = df.apply(\n",
    "    lambda row: calculate_SRKT2(\n",
    "        AL=row['Bio-AL'],\n",
    "        K_avg=row['K_avg'],\n",
    "        IOL_power=row['IOL Power'],\n",
    "        A_constant=row['A-Constant']\n",
    "        # Note: Using DEFAULT nc=1.333 and k_index=1.3375\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Calculate prediction errors\n",
    "df['Prediction_Error'] = df['PostOP Spherical Equivalent'] - df['SRKT2_Prediction']\n",
    "df['Absolute_Error'] = abs(df['Prediction_Error'])\n",
    "\n",
    "# Calculate key metrics\n",
    "mae = df['Absolute_Error'].mean()\n",
    "me = df['Prediction_Error'].mean()\n",
    "std = df['Prediction_Error'].std()\n",
    "median_ae = df['Absolute_Error'].median()\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE PERFORMANCE METRICS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Mean Absolute Error (MAE):     {mae:.4f} D\")\n",
    "print(f\"  Mean Error (ME):                {me:+.4f} D\")\n",
    "print(f\"  Standard Deviation (SD):        {std:.4f} D\")\n",
    "print(f\"  Median Absolute Error:          {median_ae:.4f} D\")\n",
    "\n",
    "print(\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(\"-\" * 50)\n",
    "if mae > 1.0:\n",
    "    print(f\"â€¢ MAE of {mae:.2f} D is POOR (>1.0 D is clinically unacceptable)\")\n",
    "else:\n",
    "    print(f\"â€¢ MAE of {mae:.2f} D is moderate\")\n",
    "    \n",
    "if abs(me) > 0.25:\n",
    "    print(f\"â€¢ Mean error of {me:+.2f} D shows systematic bias\")\n",
    "    if me < 0:\n",
    "        print(\"  â†’ Formula tends to predict too myopic (negative)\")\n",
    "    else:\n",
    "        print(\"  â†’ Formula tends to predict too hyperopic (positive)\")\n",
    "\n",
    "# Calculate clinical accuracy rates\n",
    "within_025 = (df['Absolute_Error'] <= 0.25).sum() / len(df) * 100\n",
    "within_050 = (df['Absolute_Error'] <= 0.50).sum() / len(df) * 100\n",
    "within_075 = (df['Absolute_Error'] <= 0.75).sum() / len(df) * 100\n",
    "within_100 = (df['Absolute_Error'] <= 1.00).sum() / len(df) * 100\n",
    "\n",
    "print(\"\\nğŸ“ˆ CLINICAL ACCURACY:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Within Â±0.25 D:  {within_025:.1f}% of eyes\")\n",
    "print(f\"  Within Â±0.50 D:  {within_050:.1f}% of eyes\")\n",
    "print(f\"  Within Â±0.75 D:  {within_075:.1f}% of eyes\")\n",
    "print(f\"  Within Â±1.00 D:  {within_100:.1f}% of eyes\")\n",
    "\n",
    "print(\"\\nğŸ¯ CLINICAL TARGETS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Modern standard: >70% within Â±0.50 D\")\n",
    "print(\"â€¢ Acceptable: >90% within Â±1.00 D\")\n",
    "print(f\"â€¢ Our baseline: {within_050:.1f}% within Â±0.50 D\")\n",
    "print(\"\\nâš ï¸ Standard SRK/T2 clearly struggles with Fuchs' dystrophy!\")\n",
    "print(\"This is why we need optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d38452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCULATING GLOBAL BASELINE FOR FAIR COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š CALCULATING BASELINE ACROSS ALL SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed 42: Baseline MAE = 1.1712 D\n",
      "  Seed 123: Baseline MAE = 1.0813 D\n",
      "  Seed 456: Baseline MAE = 1.5004 D\n",
      "  Seed 789: Baseline MAE = 1.1299 D\n",
      "  Seed 2025: Baseline MAE = 1.3515 D\n",
      "\n",
      "================================================================================\n",
      "GLOBAL BASELINE ESTABLISHED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ BASELINE MAE: 1.2469 Â± 0.1562 D\n",
      "   Min: 1.0813 D\n",
      "   Max: 1.5004 D\n",
      "\n",
      "âš ï¸ IMPORTANT:\n",
      "--------------------------------------------------\n",
      "All methods will now use this baseline for improvement calculations.\n",
      "This ensures fair comparison between methods.\n",
      "Improvement = ((1.2469 - Method_MAE) / 1.2469) Ã— 100%\n",
      "\n",
      "âœ… Global baseline stored in variable: GLOBAL_BASELINE_MAE = 1.2469\n",
      "All methods should now calculate improvement as:\n",
      "   improvement = ((GLOBAL_BASELINE_MAE - test_mae) / GLOBAL_BASELINE_MAE) * 100\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL BASELINE CALCULATION - CONSISTENT ACROSS ALL METHODS\n",
    "# ===========================================================\n",
    "# PURPOSE: Calculate baseline performance ONCE for fair comparison\n",
    "# This ensures all methods use the same baseline for improvement calculations\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING GLOBAL BASELINE FOR FAIR COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nğŸ“Š CALCULATING BASELINE ACROSS ALL SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Store baseline MAEs for each seed\n",
    "global_baseline_maes = []\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    # Split data with this seed\n",
    "    X_train_base, X_test_base = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    \n",
    "    # Calculate K_avg\n",
    "    X_test_base['K_avg'] = (X_test_base['Bio-Ks'] + X_test_base['Bio-Kf']) / 2\n",
    "    \n",
    "    # Calculate baseline predictions\n",
    "    X_test_base['SRKT2_Baseline'] = X_test_base.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate MAE for this seed\n",
    "    baseline_mae = mean_absolute_error(\n",
    "        X_test_base['PostOP Spherical Equivalent'], \n",
    "        X_test_base['SRKT2_Baseline']\n",
    "    )\n",
    "    \n",
    "    global_baseline_maes.append(baseline_mae)\n",
    "    print(f\"  Seed {SEED}: Baseline MAE = {baseline_mae:.4f} D\")\n",
    "\n",
    "# Calculate global baseline (average across all seeds)\n",
    "GLOBAL_BASELINE_MAE = np.mean(global_baseline_maes)\n",
    "GLOBAL_BASELINE_STD = np.std(global_baseline_maes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GLOBAL BASELINE ESTABLISHED:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“Œ BASELINE MAE: {GLOBAL_BASELINE_MAE:.4f} Â± {GLOBAL_BASELINE_STD:.4f} D\")\n",
    "print(f\"   Min: {np.min(global_baseline_maes):.4f} D\")\n",
    "print(f\"   Max: {np.max(global_baseline_maes):.4f} D\")\n",
    "\n",
    "print(\"\\nâš ï¸ IMPORTANT:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"All methods will now use this baseline for improvement calculations.\")\n",
    "print(\"This ensures fair comparison between methods.\")\n",
    "print(f\"Improvement = (({GLOBAL_BASELINE_MAE:.4f} - Method_MAE) / {GLOBAL_BASELINE_MAE:.4f}) Ã— 100%\")\n",
    "\n",
    "# Store for use by all subsequent methods\n",
    "print(f\"\\nâœ… Global baseline stored in variable: GLOBAL_BASELINE_MAE = {GLOBAL_BASELINE_MAE:.4f}\")\n",
    "print(\"All methods should now calculate improvement as:\")\n",
    "print(\"   improvement = ((GLOBAL_BASELINE_MAE - test_mae) / GLOBAL_BASELINE_MAE) * 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ridge_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RIDGE REGRESSION FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ” WHY START WITH RIDGE?\n",
      "--------------------------------------------------\n",
      "â€¢ Ridge regression identifies important features\n",
      "â€¢ Helps us understand what drives prediction errors\n",
      "â€¢ Guides our formula optimization strategy\n",
      "â€¢ If CCT features are important, our hypothesis is correct!\n",
      "\n",
      "ğŸ“Š CREATING FEATURES:\n",
      "--------------------------------------------------\n",
      "Created 12 features including CCT interactions\n",
      "\n",
      "ğŸ† TOP 10 MOST IMPORTANT FEATURES:\n",
      "--------------------------------------------------\n",
      "  CCT_ratio_AL         Coef=+1.4393\n",
      "  CCT_x_AL             Coef=-1.0791\n",
      "  CCT_squared          Coef=-0.6778\n",
      "  IOL Power            Coef=-0.5327\n",
      "  CCT_x_K              Coef=+0.5166\n",
      "  Bio-Ks               Coef=-0.2195\n",
      "  K_avg                Coef=-0.2122\n",
      "  Bio-Kf               Coef=-0.1828\n",
      "  Bio-AL               Coef=+0.1440\n",
      "  CCT_norm             Coef=-0.0331\n",
      "\n",
      "ğŸ’¡ KEY FINDINGS:\n",
      "--------------------------------------------------\n",
      "â€¢ CCT-related features account for 74.7% of total importance\n",
      "â€¢ Top feature: CCT_ratio_AL\n",
      "â€¢ CCT/AL ratio is among top 3 features!\n",
      "â€¢ This validates that CCT relative to eye size matters\n",
      "\n",
      "âœ… HYPOTHESIS CONFIRMED:\n",
      "CCT features dominate prediction - our CCT-dependent approach is justified!\n",
      "\n",
      "ğŸ¯ OPTIMIZATION STRATEGY BASED ON RIDGE:\n",
      "--------------------------------------------------\n",
      "1. Make optical parameters CCT-dependent (nc, k_index)\n",
      "2. Consider CCT/AL ratio in corrections\n",
      "3. Account for CCT interactions with other measurements\n"
     ]
    }
   ],
   "source": [
    "# RIDGE REGRESSION ANALYSIS - IDENTIFYING IMPORTANT FEATURES\n",
    "# ===========================================================\n",
    "# PURPOSE: Use machine learning to identify which features matter most\n",
    "# This will guide our optimization strategy\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RIDGE REGRESSION FEATURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ” WHY START WITH RIDGE?\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Ridge regression identifies important features\")\n",
    "print(\"â€¢ Helps us understand what drives prediction errors\")\n",
    "print(\"â€¢ Guides our formula optimization strategy\")\n",
    "print(\"â€¢ If CCT features are important, our hypothesis is correct!\")\n",
    "\n",
    "# Create feature matrix with interactions\n",
    "print(\"\\nğŸ“Š CREATING FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "features = []\n",
    "feature_names = []\n",
    "\n",
    "# Basic features\n",
    "for col in ['Bio-AL', 'Bio-Ks', 'Bio-Kf', 'IOL Power', 'CCT']:\n",
    "    features.append(df[col].values)\n",
    "    feature_names.append(col)\n",
    "\n",
    "# Add K_avg\n",
    "features.append(df['K_avg'].values)\n",
    "feature_names.append('K_avg')\n",
    "\n",
    "# CCT-derived features\n",
    "df['CCT_squared'] = df['CCT'] ** 2\n",
    "df['CCT_deviation'] = df['CCT'] - 550\n",
    "df['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_squared'].values,\n",
    "    df['CCT_deviation'].values,\n",
    "    df['CCT_norm'].values\n",
    "])\n",
    "feature_names.extend(['CCT_squared', 'CCT_deviation', 'CCT_norm'])\n",
    "\n",
    "# Interaction terms\n",
    "df['CCT_x_AL'] = df['CCT'] * df['Bio-AL']\n",
    "df['CCT_x_K'] = df['CCT'] * df['K_avg']\n",
    "df['CCT_ratio_AL'] = df['CCT'] / df['Bio-AL']\n",
    "\n",
    "features.extend([\n",
    "    df['CCT_x_AL'].values,\n",
    "    df['CCT_x_K'].values,\n",
    "    df['CCT_ratio_AL'].values\n",
    "])\n",
    "feature_names.extend(['CCT_x_AL', 'CCT_x_K', 'CCT_ratio_AL'])\n",
    "\n",
    "X = np.column_stack(features)\n",
    "y = df['PostOP Spherical Equivalent'].values\n",
    "\n",
    "print(f\"Created {len(feature_names)} features including CCT interactions\")\n",
    "\n",
    "# Standardize and train Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Ridge to get feature importance\n",
    "ridge_analysis = Ridge(alpha=1.0)\n",
    "ridge_analysis.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importance from coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': ridge_analysis.coef_,\n",
    "    'Abs_Coefficient': np.abs(ridge_analysis.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ† TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['Feature']:20} Coef={row['Coefficient']:+.4f}\")\n",
    "\n",
    "# Analyze CCT importance\n",
    "cct_features = feature_importance[feature_importance['Feature'].str.contains('CCT')]\n",
    "cct_importance = cct_features['Abs_Coefficient'].sum()\n",
    "total_importance = feature_importance['Abs_Coefficient'].sum()\n",
    "cct_percentage = (cct_importance / total_importance) * 100\n",
    "\n",
    "print(\"\\nğŸ’¡ KEY FINDINGS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ CCT-related features account for {cct_percentage:.1f}% of total importance\")\n",
    "print(f\"â€¢ Top feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "\n",
    "if 'CCT_ratio_AL' in feature_importance.head(3)['Feature'].values:\n",
    "    print(\"â€¢ CCT/AL ratio is among top 3 features!\")\n",
    "    print(\"â€¢ This validates that CCT relative to eye size matters\")\n",
    "\n",
    "if cct_percentage > 50:\n",
    "    print(\"\\nâœ… HYPOTHESIS CONFIRMED:\")\n",
    "    print(\"CCT features dominate prediction - our CCT-dependent approach is justified!\")\n",
    "\n",
    "print(\"\\nğŸ¯ OPTIMIZATION STRATEGY BASED ON RIDGE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Make optical parameters CCT-dependent (nc, k_index)\")\n",
    "print(\"2. Consider CCT/AL ratio in corrections\")\n",
    "print(\"3. Account for CCT interactions with other measurements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rt23gheoiv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARAMETER OPTIMIZATION WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CROSS-VALIDATION:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75% train, 25% test\n",
      "â€¢ Inner: 5-fold CV on training set\n",
      "â€¢ Results averaged across seeds for robustness\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CV MAE: 1.1292 Â± 0.3484 D\n",
      "  Train MAE: 1.0876, Test MAE: 1.0855\n",
      "  Test: Baseline=1.1712, Optimized=1.0855\n",
      "  Improvement: 7.3%\n",
      "  âœ… Good generalization: Test only -0.2% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 1.2084 Â± 0.1578 D\n",
      "  Train MAE: 1.0749, Test MAE: 1.1508\n",
      "  Test: Baseline=1.0813, Optimized=1.1508\n",
      "  Improvement: -6.4%\n",
      "  âœ… Good generalization: Test only 7.1% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 1.0703 Â± 0.1241 D\n",
      "  Train MAE: 0.9451, Test MAE: 1.8925\n",
      "  Test: Baseline=1.5004, Optimized=1.8925\n",
      "  Improvement: -26.1%\n",
      "  âš ï¸ Overfitting detected: Test 100.2% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 1.1882 Â± 0.2457 D\n",
      "  Train MAE: 1.0261, Test MAE: 1.2253\n",
      "  Test: Baseline=1.1299, Optimized=1.2253\n",
      "  Improvement: -8.4%\n",
      "  âš ï¸ Mild overfitting: Test 19.4% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 1.0602 Â± 0.1224 D\n",
      "  Train MAE: 0.9942, Test MAE: 1.3370\n",
      "  Test: Baseline=1.3515, Optimized=1.3370\n",
      "  Improvement: 1.1%\n",
      "  âš ï¸ Overfitting detected: Test 34.5% worse than train\n",
      "\n",
      "================================================================================\n",
      "PARAMETER OPTIMIZATION - MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed  42: MAE=1.0855 D, Improvement=7.3%\n",
      "  Seed 123: MAE=1.1508 D, Improvement=-6.4%\n",
      "  Seed 456: MAE=1.8925 D, Improvement=-26.1%\n",
      "  Seed 789: MAE=1.2253 D, Improvement=-8.4%\n",
      "  Seed 2025: MAE=1.3370 D, Improvement=1.1%\n",
      "\n",
      "ğŸ“ˆ STATISTICAL SUMMARY:\n",
      "--------------------------------------------------\n",
      "  Baseline MAE:      1.2469 Â± 0.1562 D\n",
      "  Train MAE:         1.0256 Â± 0.0524 D\n",
      "  Test MAE:          1.3382 Â± 0.2895 D\n",
      "  Mean Improvement:  -6.5 Â± 11.3%\n",
      "  Best seed:         42 (MAE=1.0855)\n",
      "  Worst seed:        456 (MAE=1.8925)\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "--------------------------------------------------\n",
      "  Mean overfit ratio: 32.2%\n",
      "  (Test MAE is 32.2% worse than Train MAE on average)\n",
      "  âš ï¸ Significant overfitting - consider regularization\n",
      "\n",
      "âœ… CONSENSUS PARAMETERS (averaged across seeds):\n",
      "--------------------------------------------------\n",
      "  nc_base              = +1.4842 Â± 0.0160\n",
      "  nc_cct_coef          = +0.0210 Â± 0.0750\n",
      "  k_index_base         = +1.4668 Â± 0.0155\n",
      "  k_index_cct_coef     = +0.0105 Â± 0.0660\n",
      "  acd_offset_base      = +2.5730 Â± 0.2624\n",
      "  acd_offset_cct_coef  = +1.2976 Â± 1.6383\n",
      "\n",
      "ğŸ’¡ ROBUSTNESS ANALYSIS:\n",
      "--------------------------------------------------\n",
      "âš ï¸ Moderate stability: CV=21.6% (some variation across seeds)\n",
      "\n",
      "ğŸ“Š Range of results: 1.0855 - 1.8925 D\n",
      "   This 0.8070 D range shows the impact of data split\n"
     ]
    }
   ],
   "source": [
    "# PARAMETER OPTIMIZATION WITH K-FOLD CROSS-VALIDATION - MULTI-SEED\n",
    "# =============================================\n",
    "# PURPOSE: Optimize SRK/T2 parameters with nested CV for robust validation\n",
    "# NOW WITH MULTIPLE SEEDS for statistical robustness\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARAMETER OPTIMIZATION WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CROSS-VALIDATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75% train, 25% test\")\n",
    "print(\"â€¢ Inner: 5-fold CV on training set\")\n",
    "print(\"â€¢ Results averaged across seeds for robustness\")\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "def calculate_mae_param(params, df_data):\n",
    "    \"\"\"Calculate MAE for parameter optimization\"\"\"\n",
    "    nc_base, nc_cct_coef, k_index_base, k_index_cct_coef, acd_offset_base, acd_offset_cct_coef = params\n",
    "    \n",
    "    predictions = []\n",
    "    for _, row in df_data.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        nc = nc_base + nc_cct_coef * cct_norm\n",
    "        k_index = k_index_base + k_index_cct_coef * cct_norm\n",
    "        acd_offset = acd_offset_base + acd_offset_cct_coef * cct_norm\n",
    "        \n",
    "        pred = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc,\n",
    "            k_index=k_index\n",
    "        )\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    mae = mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    return mae\n",
    "\n",
    "bounds_param = [\n",
    "    (1.20, 1.50),    # nc_base\n",
    "    (-0.20, 0.20),   # nc_cct_coef  \n",
    "    (1.20, 1.60),    # k_index_base\n",
    "    (-0.30, 0.30),   # k_index_cct_coef\n",
    "    (-3.0, 3.0),     # acd_offset_base\n",
    "    (-3.0, 3.0),     # acd_offset_cct_coef\n",
    "]\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_param = []\n",
    "seed_test_maes_param = []\n",
    "seed_train_maes_param = []  # NEW: Track training MAEs\n",
    "seed_baseline_maes_param = []\n",
    "seed_improvements_param = []\n",
    "seed_overfit_ratios_param = []  # NEW: Track overfitting\n",
    "\n",
    "df['K_avg'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT with current seed\n",
    "    X_train_param, X_test_param = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_param['K_avg'] = (X_train_param['Bio-Ks'] + X_train_param['Bio-Kf']) / 2\n",
    "    X_test_param['K_avg'] = (X_test_param['Bio-Ks'] + X_test_param['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_param)} train, {len(X_test_param)} test\")\n",
    "    \n",
    "    # INNER K-FOLD CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_params = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_param), 1):\n",
    "        fold_train = X_train_param.iloc[train_idx]\n",
    "        fold_val = X_train_param.iloc[val_idx]\n",
    "        \n",
    "        # Optimize on fold\n",
    "        result_fold = differential_evolution(\n",
    "            lambda p: calculate_mae_param(p, fold_train),\n",
    "            bounds_param,\n",
    "            maxiter=30,\n",
    "            seed=SEED + fold_num,\n",
    "            workers=1,\n",
    "            updating='deferred',\n",
    "            disp=False\n",
    "        )\n",
    "        \n",
    "        fold_params.append(result_fold.x)\n",
    "        val_mae = calculate_mae_param(result_fold.x, fold_val)\n",
    "        fold_maes.append(val_mae)\n",
    "    \n",
    "    # Average parameters from folds\n",
    "    avg_params = np.mean(fold_params, axis=0)\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    result_final = differential_evolution(\n",
    "        lambda p: calculate_mae_param(p, X_train_param),\n",
    "        bounds_param,\n",
    "        maxiter=50,\n",
    "        seed=SEED,\n",
    "        workers=1,\n",
    "        updating='deferred',\n",
    "        disp=False\n",
    "    )\n",
    "    \n",
    "    final_params = result_final.x\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    mae_train = calculate_mae_param(final_params, X_train_param)\n",
    "    \n",
    "    # TEST ON HOLDOUT\n",
    "    # Calculate baseline\n",
    "    X_test_param['SRKT2_Baseline'] = X_test_param.apply(\n",
    "        lambda row: calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Apply optimized parameters\n",
    "    predictions_test = []\n",
    "    for _, row in X_test_param.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        nc = final_params[0] + final_params[1] * cct_norm\n",
    "        k_index = final_params[2] + final_params[3] * cct_norm\n",
    "        acd_offset = final_params[4] + final_params[5] * cct_norm\n",
    "        \n",
    "        pred = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'],\n",
    "            K_avg=row['K_avg'],\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc,\n",
    "            k_index=k_index\n",
    "        )\n",
    "        predictions_test.append(pred)\n",
    "    \n",
    "    mae_baseline = np.abs(X_test_param['SRKT2_Baseline'] - X_test_param['PostOP Spherical Equivalent']).mean()\n",
    "    mae_optimized = mean_absolute_error(X_test_param['PostOP Spherical Equivalent'], predictions_test)\n",
    "    improvement = (mae_baseline - mae_optimized) / mae_baseline * 100\n",
    "    \n",
    "    print(f\"  Train MAE: {mae_train:.4f}, Test MAE: {mae_optimized:.4f}\")\n",
    "    print(f\"  Test: Baseline={mae_baseline:.4f}, Optimized={mae_optimized:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit_ratio = (mae_optimized - mae_train) / mae_train * 100\n",
    "    if overfit_ratio > 20:\n",
    "        print(f\"  âš ï¸ Overfitting detected: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    elif overfit_ratio > 10:\n",
    "        print(f\"  âš ï¸ Mild overfitting: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    else:\n",
    "        print(f\"  âœ… Good generalization: Test only {overfit_ratio:.1f}% worse than train\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_param.append(final_params)\n",
    "    seed_test_maes_param.append(mae_optimized)\n",
    "    seed_train_maes_param.append(mae_train)\n",
    "    seed_baseline_maes_param.append(mae_baseline)\n",
    "    seed_improvements_param.append(improvement)\n",
    "    seed_overfit_ratios_param.append(overfit_ratio)\n",
    "\n",
    "# MULTI-SEED SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARAMETER OPTIMIZATION - MULTI-SEED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"  Seed {seed:3}: MAE={seed_test_maes_param[i]:.4f} D, Improvement={seed_improvements_param[i]:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {np.mean(seed_baseline_maes_param):.4f} Â± {np.std(seed_baseline_maes_param):.4f} D\")\n",
    "print(f\"  Train MAE:         {np.mean(seed_train_maes_param):.4f} Â± {np.std(seed_train_maes_param):.4f} D\")\n",
    "print(f\"  Test MAE:          {np.mean(seed_test_maes_param):.4f} Â± {np.std(seed_test_maes_param):.4f} D\")\n",
    "print(f\"  Mean Improvement:  {np.mean(seed_improvements_param):.1f} Â± {np.std(seed_improvements_param):.1f}%\")\n",
    "print(f\"  Best seed:         {SEEDS[np.argmin(seed_test_maes_param)]} (MAE={min(seed_test_maes_param):.4f})\")\n",
    "print(f\"  Worst seed:        {SEEDS[np.argmax(seed_test_maes_param)]} (MAE={max(seed_test_maes_param):.4f})\")\n",
    "\n",
    "# OVERFITTING ANALYSIS\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_param):.1f}%\")\n",
    "print(f\"  (Test MAE is {np.mean(seed_overfit_ratios_param):.1f}% worse than Train MAE on average)\")\n",
    "\n",
    "if np.mean(seed_overfit_ratios_param) < 10:\n",
    "    print(\"  âœ… Excellent generalization - minimal overfitting\")\n",
    "elif np.mean(seed_overfit_ratios_param) < 20:\n",
    "    print(\"  âœ… Good generalization - acceptable overfitting\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Significant overfitting - consider regularization\")\n",
    "\n",
    "# Average parameters across seeds\n",
    "avg_params_all_seeds = np.mean(seed_results_param, axis=0)\n",
    "std_params_all_seeds = np.std(seed_results_param, axis=0)\n",
    "\n",
    "print(\"\\nâœ… CONSENSUS PARAMETERS (averaged across seeds):\")\n",
    "print(\"-\" * 50)\n",
    "param_names = ['nc_base', 'nc_cct_coef', 'k_index_base', 'k_index_cct_coef', 'acd_offset_base', 'acd_offset_cct_coef']\n",
    "for i, name in enumerate(param_names):\n",
    "    print(f\"  {name:20} = {avg_params_all_seeds[i]:+.4f} Â± {std_params_all_seeds[i]:.4f}\")\n",
    "\n",
    "# Store in global results dictionary\n",
    "multi_seed_results['parameter'] = {\n",
    "    'test_maes': seed_test_maes_param,\n",
    "    'train_maes': seed_train_maes_param,\n",
    "    'baseline_maes': seed_baseline_maes_param,\n",
    "    'improvements': seed_improvements_param,\n",
    "    'overfit_ratios': seed_overfit_ratios_param,\n",
    "    'mean_mae': np.mean(seed_test_maes_param),\n",
    "    'std_mae': np.std(seed_test_maes_param),\n",
    "    'mean_improvement': np.mean(seed_improvements_param)\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ’¡ ROBUSTNESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "mae_cv = np.std(seed_test_maes_param) / np.mean(seed_test_maes_param) * 100\n",
    "if mae_cv < 5:\n",
    "    print(f\"âœ… Excellent stability: CV={mae_cv:.1f}% (very consistent across seeds)\")\n",
    "elif mae_cv < 10:\n",
    "    print(f\"âœ… Good stability: CV={mae_cv:.1f}% (consistent across seeds)\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Moderate stability: CV={mae_cv:.1f}% (some variation across seeds)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Range of results: {min(seed_test_maes_param):.4f} - {max(seed_test_maes_param):.4f} D\")\n",
    "print(f\"   This {max(seed_test_maes_param)-min(seed_test_maes_param):.4f} D range shows the impact of data split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829090ggs0r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CV STRATEGY:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold CV on training\n",
      "â€¢ Find stable multiplicative factors across seeds\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 0.8196 Â± 0.2064 D\n",
      "  Final params: mâ‚€=-0.0379, mâ‚=-0.0121, mâ‚‚=-0.0379\n",
      "  Train MAE: 0.7998, Test MAE: 1.1124\n",
      "  Test: Baseline=1.1712, Optimized=1.1124\n",
      "  Improvement: 5.0%\n",
      "  âš ï¸ Overfitting detected: Test 39.1% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 0.9491 Â± 0.1224 D\n",
      "  Final params: mâ‚€=-0.0382, mâ‚=-0.0058, mâ‚‚=-0.0379\n",
      "  Train MAE: 0.9374, Test MAE: 0.6988\n",
      "  Test: Baseline=1.0813, Optimized=0.6988\n",
      "  Improvement: 35.4%\n",
      "  âœ… Good generalization: Test only -25.5% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 0.8517 Â± 0.1288 D\n",
      "  Final params: mâ‚€=-0.0348, mâ‚=-0.0074, mâ‚‚=-0.0358\n",
      "  Train MAE: 0.8244, Test MAE: 1.0451\n",
      "  Test: Baseline=1.5004, Optimized=1.0451\n",
      "  Improvement: 30.3%\n",
      "  âš ï¸ Overfitting detected: Test 26.8% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 0.9495 Â± 0.1120 D\n",
      "  Final params: mâ‚€=-0.0642, mâ‚=0.0812, mâ‚‚=-0.0370\n",
      "  Train MAE: 0.8891, Test MAE: 0.8320\n",
      "  Test: Baseline=1.1299, Optimized=0.8320\n",
      "  Improvement: 26.4%\n",
      "  âœ… Good generalization: Test only -6.4% worse than train\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "  CV MAE: 0.8539 Â± 0.1649 D\n",
      "  Final params: mâ‚€=-0.0387, mâ‚=-0.0091, mâ‚‚=-0.0387\n",
      "  Train MAE: 0.8461, Test MAE: 0.9789\n",
      "  Test: Baseline=1.3515, Optimized=0.9789\n",
      "  Improvement: 27.6%\n",
      "  âš ï¸ Mild overfitting: Test 15.7% worse than train\n",
      "\n",
      "================================================================================\n",
      "MULTIPLICATIVE CORRECTION - MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\n",
      "--------------------------------------------------\n",
      "  Seed  42: MAE=1.1124 D, Improvement=5.0%\n",
      "  Seed 123: MAE=0.6988 D, Improvement=35.4%\n",
      "  Seed 456: MAE=1.0451 D, Improvement=30.3%\n",
      "  Seed 789: MAE=0.8320 D, Improvement=26.4%\n",
      "  Seed 2025: MAE=0.9789 D, Improvement=27.6%\n",
      "\n",
      "ğŸ“ˆ STATISTICAL SUMMARY:\n",
      "--------------------------------------------------\n",
      "  Baseline MAE:      1.2469 Â± 0.1562 D\n",
      "  Train MAE:         0.8594 Â± 0.0488 D\n",
      "  Test MAE:          0.9335 Â± 0.1496 D\n",
      "  Mean Improvement:  24.9 Â± 10.4%\n",
      "  Best seed:         123 (MAE=0.6988)\n",
      "  Worst seed:        42 (MAE=1.1124)\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "--------------------------------------------------\n",
      "  Mean overfit ratio: 9.9%\n",
      "  (Test MAE is 9.9% worse than Train MAE on average)\n",
      "  âœ… Excellent generalization - minimal overfitting\n",
      "\n",
      "âœ… CONSENSUS PARAMETERS (averaged across seeds):\n",
      "--------------------------------------------------\n",
      "  mâ‚€ (constant):     -0.0428 Â± 0.0108\n",
      "  mâ‚ (CCT coef):     +0.0094 Â± 0.0360\n",
      "  mâ‚‚ (ratio coef):   -0.0375 Â± 0.0010\n",
      "\n",
      "ğŸ“ CONSENSUS CORRECTION FORMULA:\n",
      "--------------------------------------------------\n",
      "Corrected_REF = Standard_SRK/T2 Ã— Correction_Factor\n",
      "Correction_Factor = 1 -0.0428 +0.0094Ã—CCT_norm -0.0375Ã—(CCT/AL)\n",
      "\n",
      "ğŸ’¡ ROBUSTNESS ANALYSIS:\n",
      "--------------------------------------------------\n",
      "âš ï¸ Moderate stability: CV=16.0% (some variation across seeds)\n",
      "\n",
      "ğŸ“Š Range of results: 0.6988 - 1.1124 D\n",
      "   This 0.4136 D range shows the impact of data split\n",
      "\n",
      "ğŸ“Š Parameter consistency across seeds:\n",
      "  mâ‚€: min=-0.0642, max=-0.0348, range=0.0294\n",
      "  mâ‚: min=-0.0121, max=0.0812, range=0.0933\n",
      "  mâ‚‚: min=-0.0387, max=-0.0358, range=0.0029\n"
     ]
    }
   ],
   "source": [
    "# MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED\n",
    "# ====================================\n",
    "# PURPOSE: Multiplicative correction with nested CV for robust validation\n",
    "# NOW WITH MULTIPLE SEEDS for statistical robustness\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTIPLICATIVE CORRECTION WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CV STRATEGY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold CV on training\")\n",
    "print(\"â€¢ Find stable multiplicative factors across seeds\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "\n",
    "def multiplicative_objective(params, df_data):\n",
    "    \"\"\"Objective function for multiplicative correction\"\"\"\n",
    "    m0, m1, m2 = params\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for _, row in df_data.iterrows():\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        correction_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "        corrected_pred = base_pred * correction_factor\n",
    "        \n",
    "        predictions.append(corrected_pred)\n",
    "        actuals.append(row['PostOP Spherical Equivalent'])\n",
    "    \n",
    "    return mean_absolute_error(actuals, predictions)\n",
    "\n",
    "x0_mult = [0, 0, 0]\n",
    "bounds_mult = [(-0.5, 0.5), (-0.5, 0.5), (-0.5, 0.5)]\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_mult = []\n",
    "seed_test_maes_mult = []\n",
    "seed_train_maes_mult = []  # NEW: Track training MAEs\n",
    "seed_baseline_maes_mult = []\n",
    "seed_improvements_mult = []\n",
    "seed_overfit_ratios_mult = []  # NEW: Track overfitting\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT with current seed\n",
    "    X_train_mult, X_test_mult = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_mult['K_avg'] = (X_train_mult['Bio-Ks'] + X_train_mult['Bio-Kf']) / 2\n",
    "    X_test_mult['K_avg'] = (X_test_mult['Bio-Ks'] + X_test_mult['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_mult)} train, {len(X_test_mult)} test\")\n",
    "    \n",
    "    # Calculate baseline SRK/T2 for all data\n",
    "    for dataset in [X_train_mult, X_test_mult]:\n",
    "        dataset['SRKT2_Prediction'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    # INNER K-FOLD CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_params = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_mult), 1):\n",
    "        fold_train = X_train_mult.iloc[train_idx]\n",
    "        fold_val = X_train_mult.iloc[val_idx]\n",
    "        \n",
    "        # Optimize on fold training\n",
    "        result_fold = minimize(\n",
    "            lambda p: multiplicative_objective(p, fold_train),\n",
    "            x0_mult,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds_mult\n",
    "        )\n",
    "        \n",
    "        fold_params.append(result_fold.x)\n",
    "        val_mae = multiplicative_objective(result_fold.x, fold_val)\n",
    "        fold_maes.append(val_mae)\n",
    "    \n",
    "    # Average across folds\n",
    "    avg_params = np.mean(fold_params, axis=0)\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    result_mult = minimize(\n",
    "        lambda p: multiplicative_objective(p, X_train_mult),\n",
    "        x0_mult,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds_mult\n",
    "    )\n",
    "    m0_opt, m1_opt, m2_opt = result_mult.x\n",
    "    \n",
    "    print(f\"  Final params: mâ‚€={m0_opt:.4f}, mâ‚={m1_opt:.4f}, mâ‚‚={m2_opt:.4f}\")\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    mae_train = multiplicative_objective([m0_opt, m1_opt, m2_opt], X_train_mult)\n",
    "    \n",
    "    # TEST ON HOLDOUT\n",
    "    predictions_mult_test = []\n",
    "    for _, row in X_test_mult.iterrows():\n",
    "        base_pred = row['SRKT2_Prediction']\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        \n",
    "        correction_factor = 1 + m0_opt + m1_opt * cct_norm + m2_opt * cct_ratio\n",
    "        corrected_pred = base_pred * correction_factor\n",
    "        predictions_mult_test.append(corrected_pred)\n",
    "    \n",
    "    mae_baseline = np.abs(X_test_mult['SRKT2_Prediction'] - X_test_mult['PostOP Spherical Equivalent']).mean()\n",
    "    mae_optimized = mean_absolute_error(X_test_mult['PostOP Spherical Equivalent'], predictions_mult_test)\n",
    "    improvement = (mae_baseline - mae_optimized) / mae_baseline * 100\n",
    "    \n",
    "    print(f\"  Train MAE: {mae_train:.4f}, Test MAE: {mae_optimized:.4f}\")\n",
    "    print(f\"  Test: Baseline={mae_baseline:.4f}, Optimized={mae_optimized:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit_ratio = (mae_optimized - mae_train) / mae_train * 100\n",
    "    if overfit_ratio > 20:\n",
    "        print(f\"  âš ï¸ Overfitting detected: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    elif overfit_ratio > 10:\n",
    "        print(f\"  âš ï¸ Mild overfitting: Test {overfit_ratio:.1f}% worse than train\")\n",
    "    else:\n",
    "        print(f\"  âœ… Good generalization: Test only {overfit_ratio:.1f}% worse than train\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_mult.append([m0_opt, m1_opt, m2_opt])\n",
    "    seed_test_maes_mult.append(mae_optimized)\n",
    "    seed_train_maes_mult.append(mae_train)\n",
    "    seed_baseline_maes_mult.append(mae_baseline)\n",
    "    seed_improvements_mult.append(improvement)\n",
    "    seed_overfit_ratios_mult.append(overfit_ratio)\n",
    "\n",
    "# MULTI-SEED SUMMARY\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIPLICATIVE CORRECTION - MULTI-SEED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š TEST PERFORMANCE ACROSS SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"  Seed {seed:3}: MAE={seed_test_maes_mult[i]:.4f} D, Improvement={seed_improvements_mult[i]:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Baseline MAE:      {np.mean(seed_baseline_maes_mult):.4f} Â± {np.std(seed_baseline_maes_mult):.4f} D\")\n",
    "print(f\"  Train MAE:         {np.mean(seed_train_maes_mult):.4f} Â± {np.std(seed_train_maes_mult):.4f} D\")\n",
    "print(f\"  Test MAE:          {np.mean(seed_test_maes_mult):.4f} Â± {np.std(seed_test_maes_mult):.4f} D\")\n",
    "print(f\"  Mean Improvement:  {np.mean(seed_improvements_mult):.1f} Â± {np.std(seed_improvements_mult):.1f}%\")\n",
    "print(f\"  Best seed:         {SEEDS[np.argmin(seed_test_maes_mult)]} (MAE={min(seed_test_maes_mult):.4f})\")\n",
    "print(f\"  Worst seed:        {SEEDS[np.argmax(seed_test_maes_mult)]} (MAE={max(seed_test_maes_mult):.4f})\")\n",
    "\n",
    "# OVERFITTING ANALYSIS\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_mult):.1f}%\")\n",
    "print(f\"  (Test MAE is {np.mean(seed_overfit_ratios_mult):.1f}% worse than Train MAE on average)\")\n",
    "\n",
    "if np.mean(seed_overfit_ratios_mult) < 10:\n",
    "    print(\"  âœ… Excellent generalization - minimal overfitting\")\n",
    "elif np.mean(seed_overfit_ratios_mult) < 20:\n",
    "    print(\"  âœ… Good generalization - acceptable overfitting\")\n",
    "else:\n",
    "    print(\"  âš ï¸ Significant overfitting - consider regularization\")\n",
    "\n",
    "# Average parameters across seeds\n",
    "avg_params_all_seeds = np.mean(seed_results_mult, axis=0)\n",
    "std_params_all_seeds = np.std(seed_results_mult, axis=0)\n",
    "\n",
    "print(\"\\nâœ… CONSENSUS PARAMETERS (averaged across seeds):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  mâ‚€ (constant):     {avg_params_all_seeds[0]:+.4f} Â± {std_params_all_seeds[0]:.4f}\")\n",
    "print(f\"  mâ‚ (CCT coef):     {avg_params_all_seeds[1]:+.4f} Â± {std_params_all_seeds[1]:.4f}\")\n",
    "print(f\"  mâ‚‚ (ratio coef):   {avg_params_all_seeds[2]:+.4f} Â± {std_params_all_seeds[2]:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ CONSENSUS CORRECTION FORMULA:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Corrected_REF = Standard_SRK/T2 Ã— Correction_Factor\")\n",
    "print(f\"Correction_Factor = 1 {avg_params_all_seeds[0]:+.4f} {avg_params_all_seeds[1]:+.4f}Ã—CCT_norm {avg_params_all_seeds[2]:+.4f}Ã—(CCT/AL)\")\n",
    "\n",
    "# Store in global results dictionary\n",
    "multi_seed_results['multiplicative'] = {\n",
    "    'test_maes': seed_test_maes_mult,\n",
    "    'train_maes': seed_train_maes_mult,\n",
    "    'baseline_maes': seed_baseline_maes_mult,\n",
    "    'improvements': seed_improvements_mult,\n",
    "    'overfit_ratios': seed_overfit_ratios_mult,\n",
    "    'mean_mae': np.mean(seed_test_maes_mult),\n",
    "    'std_mae': np.std(seed_test_maes_mult),\n",
    "    'mean_improvement': np.mean(seed_improvements_mult)\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ’¡ ROBUSTNESS ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "mae_cv = np.std(seed_test_maes_mult) / np.mean(seed_test_maes_mult) * 100\n",
    "if mae_cv < 5:\n",
    "    print(f\"âœ… Excellent stability: CV={mae_cv:.1f}% (very consistent across seeds)\")\n",
    "elif mae_cv < 10:\n",
    "    print(f\"âœ… Good stability: CV={mae_cv:.1f}% (consistent across seeds)\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Moderate stability: CV={mae_cv:.1f}% (some variation across seeds)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Range of results: {min(seed_test_maes_mult):.4f} - {max(seed_test_maes_mult):.4f} D\")\n",
    "print(f\"   This {max(seed_test_maes_mult)-min(seed_test_maes_mult):.4f} D range shows the impact of data split\")\n",
    "\n",
    "# Parameter consistency check\n",
    "print(f\"\\nğŸ“Š Parameter consistency across seeds:\")\n",
    "for i, param_name in enumerate(['mâ‚€', 'mâ‚', 'mâ‚‚']):\n",
    "    param_values = [p[i] for p in seed_results_mult]\n",
    "    print(f\"  {param_name}: min={min(param_values):.4f}, max={max(param_values):.4f}, range={max(param_values)-min(param_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a07c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPPORT VECTOR REGRESSION (SVR) WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 1 Results:\n",
      "    Test MAE: 0.907 Â± 0.152 D\n",
      "    Train MAE: 0.716 D\n",
      "    Overfit ratio: 1.267\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 2 Results:\n",
      "    Test MAE: 0.881 Â± 0.150 D\n",
      "    Train MAE: 0.717 D\n",
      "    Overfit ratio: 1.229\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 3 Results:\n",
      "    Test MAE: 0.904 Â± 0.216 D\n",
      "    Train MAE: 0.689 D\n",
      "    Overfit ratio: 1.312\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 4 Results:\n",
      "    Test MAE: 0.944 Â± 0.130 D\n",
      "    Train MAE: 0.681 D\n",
      "    Overfit ratio: 1.385\n",
      "\n",
      "Seed 2026/5:\n",
      "  Seed 5 Results:\n",
      "    Test MAE: 0.900 Â± 0.191 D\n",
      "    Train MAE: 0.699 D\n",
      "    Overfit ratio: 1.288\n",
      "\n",
      "================================================================================\n",
      "SVR MULTI-SEED SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Across 5 seeds with 5-fold CV (25 total evaluations):\n",
      "Test MAE: 0.907 Â± 0.172 D\n",
      "Train MAE: 0.700 Â± 0.058 D\n",
      "Baseline MAE: 1.213 Â± 0.179 D\n",
      "Overfit Ratio: 1.295\n",
      "\n",
      "Improvement over baseline:\n",
      "  Relative: 25.2%\n",
      "  Absolute: 0.306 D\n",
      "\n",
      "Most frequent hyperparameters:\n",
      "  C: 0.5(20), 1.0(5)\n",
      "  Îµ: 0.2(16), 0.05(8), 0.1(1)\n",
      "================================================================================\n",
      "SVR method completed - results stored in _svr variables\n",
      "Both multiplicative and SVR are now available for comparison\n",
      "Results stored for final comparison in cell 11\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# SUPPORT VECTOR REGRESSION (SVR) - REPLACEMENT FOR MULTIPLICATIVE\n",
    "# =================================================================\n",
    "# PURPOSE: Test SVR as alternative to multiplicative correction\n",
    "# Based on comprehensive testing showing 6.7% improvement\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUPPORT VECTOR REGRESSION (SVR) WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features for SVR\n",
    "def prepare_svr_features(df):\n",
    "    X = pd.DataFrame()\n",
    "    X['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "    X['AL'] = df['Bio-AL']\n",
    "    X['ACD'] = df['Bio-ACD']\n",
    "    X['K_mean'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "    X['CCT_AL'] = df['CCT'] / df['Bio-AL']\n",
    "    return X\n",
    "\n",
    "# Store results for multiple seeds (using SEEDS from first cell)\n",
    "seed_results_svr = []\n",
    "seed_test_maes_svr = []\n",
    "seed_train_maes_svr = []\n",
    "seed_baseline_maes_svr = []\n",
    "seed_improvements_svr = []\n",
    "seed_overfit_ratios_svr = []\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    np.random.seed(SEED)\n",
    "    print(f\"\\nSeed {seed+1}/{len(SEEDS)}:\")\n",
    "    \n",
    "    # Outer CV loop (using same structure as other methods)\n",
    "    kf_outer = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    fold_results = []\n",
    "    test_maes = []\n",
    "    train_maes = []\n",
    "    baseline_maes = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf_outer.split(df)):\n",
    "        # Split data (using 'df' DataFrame from first cell)\n",
    "        df_train = df.iloc[train_idx].copy()\n",
    "        df_test = df.iloc[test_idx].copy()\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train = prepare_svr_features(df_train)\n",
    "        X_test = prepare_svr_features(df_test)\n",
    "        y_train = df_train['PostOP Spherical Equivalent'].values\n",
    "        y_test = df_test['PostOP Spherical Equivalent'].values\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning (simplified grid for speed)\n",
    "        best_params = None\n",
    "        best_val_mae = float('inf')\n",
    "        \n",
    "        # Reduced grid search for efficiency\n",
    "        param_grid = {\n",
    "            'C': [0.5, 1.0, 2.0],\n",
    "            'epsilon': [0.05, 0.1, 0.2]\n",
    "        }\n",
    "        \n",
    "        kf_inner = KFold(n_splits=3, shuffle=True, random_state=SEED*100+fold)\n",
    "        \n",
    "        for C in param_grid['C']:\n",
    "            for epsilon in param_grid['epsilon']:\n",
    "                val_maes = []\n",
    "                \n",
    "                for train_inner_idx, val_idx in kf_inner.split(X_train_scaled):\n",
    "                    X_train_inner = X_train_scaled[train_inner_idx]\n",
    "                    y_train_inner = y_train[train_inner_idx]\n",
    "                    X_val = X_train_scaled[val_idx]\n",
    "                    y_val = y_train[val_idx]\n",
    "                    \n",
    "                    # Train SVR\n",
    "                    model = SVR(kernel='rbf', C=C, epsilon=epsilon, gamma='scale')\n",
    "                    model.fit(X_train_inner, y_train_inner)\n",
    "                    \n",
    "                    # Validate\n",
    "                    y_pred_val = model.predict(X_val)\n",
    "                    val_maes.append(mean_absolute_error(y_val, y_pred_val))\n",
    "                \n",
    "                mean_val_mae = np.mean(val_maes)\n",
    "                if mean_val_mae < best_val_mae:\n",
    "                    best_val_mae = mean_val_mae\n",
    "                    best_params = {'C': C, 'epsilon': epsilon}\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        final_model = SVR(kernel='rbf', C=best_params['C'], \n",
    "                         epsilon=best_params['epsilon'], gamma='scale')\n",
    "        final_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = final_model.predict(X_train_scaled)\n",
    "        y_pred_test = final_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate MAEs\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Baseline MAE (using SRKT2_Prediction from data preparation)\n",
    "        if 'SRKT2_Prediction' in df_test.columns:\n",
    "            baseline_pred = df_test['SRKT2_Prediction'].values\n",
    "        else:\n",
    "            # Simple SRK/T2 approximation if not available\n",
    "            baseline_pred = 118.4 - 2.5 * df_test['Bio-AL'] - 0.9 * X_test['K_mean']\n",
    "        baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "        \n",
    "        train_maes.append(train_mae)\n",
    "        test_maes.append(test_mae)\n",
    "        baseline_maes.append(baseline_mae)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'baseline_mae': baseline_mae,\n",
    "            'improvement': (baseline_mae - test_mae) / baseline_mae * 100 if baseline_mae > 0 else 0,\n",
    "            'best_C': best_params['C'],\n",
    "            'best_epsilon': best_params['epsilon']\n",
    "        })\n",
    "    \n",
    "    # Store seed results\n",
    "    seed_results_svr.append(fold_results)\n",
    "    seed_test_maes_svr.append(test_maes)\n",
    "    seed_train_maes_svr.append(train_maes)\n",
    "    seed_baseline_maes_svr.append(baseline_maes)\n",
    "    \n",
    "    # Calculate improvements for this seed\n",
    "    improvements = []\n",
    "    for j in range(len(test_maes)):\n",
    "        if baseline_maes[j] > 0:\n",
    "            improvement = (baseline_maes[j] - test_maes[j]) / baseline_maes[j] * 100\n",
    "        else:\n",
    "            improvement = 0\n",
    "        improvements.append(improvement)\n",
    "    \n",
    "    # Calculate overfit ratio for this seed\n",
    "    overfit_ratio = np.mean(test_maes) / np.mean(train_maes) if np.mean(train_maes) > 0 else 1.0\n",
    "    \n",
    "    # Store for this seed\n",
    "    seed_improvements_svr.append(improvements)\n",
    "    seed_overfit_ratios_svr.append(overfit_ratio)\n",
    "    \n",
    "    # Calculate seed statistics\n",
    "    seed_test_mean = np.mean(test_maes)\n",
    "    seed_test_std = np.std(test_maes)\n",
    "    seed_train_mean = np.mean(train_maes)\n",
    "    \n",
    "    print(f\"  Seed {seed_idx} Results:\")\n",
    "    print(f\"    Test MAE: {seed_test_mean:.3f} Â± {seed_test_std:.3f} D\")\n",
    "    print(f\"    Train MAE: {seed_train_mean:.3f} D\")\n",
    "    print(f\"    Overfit ratio: {seed_test_mean/seed_train_mean:.3f}\")\n",
    "\n",
    "# Summary statistics (consistent with other methods)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SVR MULTI-SEED SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall statistics\n",
    "overall_test_maes = [mae for seed_maes in seed_test_maes_svr for mae in seed_maes]\n",
    "overall_train_maes = [mae for seed_maes in seed_train_maes_svr for mae in seed_maes]\n",
    "overall_baseline_maes = [mae for seed_maes in seed_baseline_maes_svr for mae in seed_maes]\n",
    "\n",
    "print(f\"\\nAcross {len(SEEDS)} seeds with 5-fold CV ({len(overall_test_maes)} total evaluations):\")\n",
    "print(f\"Test MAE: {np.mean(overall_test_maes):.3f} Â± {np.std(overall_test_maes):.3f} D\")\n",
    "print(f\"Train MAE: {np.mean(overall_train_maes):.3f} Â± {np.std(overall_train_maes):.3f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(overall_baseline_maes):.3f} Â± {np.std(overall_baseline_maes):.3f} D\")\n",
    "print(f\"Overfit Ratio: {np.mean(overall_test_maes) / np.mean(overall_train_maes):.3f}\")\n",
    "\n",
    "if np.mean(overall_baseline_maes) > 0:\n",
    "    improvement = (np.mean(overall_baseline_maes) - np.mean(overall_test_maes)) / np.mean(overall_baseline_maes) * 100\n",
    "    absolute_improvement = np.mean(overall_baseline_maes) - np.mean(overall_test_maes)\n",
    "    print(f\"\\nImprovement over baseline:\")\n",
    "    print(f\"  Relative: {improvement:.1f}%\")\n",
    "    print(f\"  Absolute: {absolute_improvement:.3f} D\")\n",
    "\n",
    "# Hyperparameter analysis\n",
    "print(\"\\nMost frequent hyperparameters:\")\n",
    "all_Cs = [r['best_C'] for seed in seed_results_svr for r in seed]\n",
    "all_epsilons = [r['best_epsilon'] for seed in seed_results_svr for r in seed]\n",
    "from collections import Counter\n",
    "c_counts = Counter(all_Cs).most_common(3)\n",
    "eps_counts = Counter(all_epsilons).most_common(3)\n",
    "print(f\"  C: {', '.join([f'{val}({cnt})' for val, cnt in c_counts])}\")\n",
    "print(f\"  Îµ: {', '.join([f'{val}({cnt})' for val, cnt in eps_counts])}\")\n",
    "\n",
    "# Store results in format compatible with final comparison\n",
    "# This replaces the multiplicative method results\n",
    "\n",
    "# Store results in _mult variables for compatibility with comparison cell\n",
    "# Keep SVR results separate\n",
    "# seed_test_maes_mult = seed_test_maes_svr  # Don't overwrite\n",
    "# seed_train_maes_mult = seed_train_maes_svr\n",
    "# seed_baseline_maes_mult = seed_baseline_maes_svr\n",
    "# seed_improvements_mult = seed_improvements_svr\n",
    "# seed_overfit_ratios_mult = seed_overfit_ratios_svr\n",
    "\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"SVR method completed - results stored in _svr variables\")\n",
    "print(\"Both multiplicative and SVR are now available for comparison\")\n",
    "print(\"Results stored for final comparison in cell 11\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efce5d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALTERNATIVE ML METHODS FOR IOL CALCULATION\n",
      "================================================================================\n",
      "\n",
      "ğŸ¤– TESTING ADVANCED ML ALGORITHMS:\n",
      "--------------------------------------------------\n",
      "â€¢ Random Forest: Tree ensemble with bagging\n",
      "â€¢ XGBoost: Gradient boosting (state-of-the-art)\n",
      "â€¢ Gaussian Process: Probabilistic approach with uncertainty\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS FOR ML METHODS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "TESTING: Random Forest\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best RF params: trees=200, depth=3\n",
      "  Test MAE: 1.0337 D\n",
      "  Train MAE: 0.6254 D\n",
      "  Improvement: 11.7%\n",
      "  Overfit ratio: 0.605\n",
      "  Top features: IOL Power, CCT_AL_ratio, AL_ACD_ratio\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best RF params: trees=50, depth=3\n",
      "  Test MAE: 0.7555 D\n",
      "  Train MAE: 0.6644 D\n",
      "  Improvement: 30.1%\n",
      "  Overfit ratio: 0.879\n",
      "  Top features: CCT_AL_ratio, Bio-AL, AL_ACD_ratio\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best RF params: trees=100, depth=3\n",
      "  Test MAE: 1.1065 D\n",
      "  Train MAE: 0.6366 D\n",
      "  Improvement: 26.3%\n",
      "  Overfit ratio: 0.575\n",
      "  Top features: Bio-AL, CCT_AL_ratio, K_mean\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best RF params: trees=50, depth=3\n",
      "  Test MAE: 0.7805 D\n",
      "  Train MAE: 0.7039 D\n",
      "  Improvement: 30.9%\n",
      "  Overfit ratio: 0.902\n",
      "  Top features: IOL Power, CCT_AL_ratio, Bio-AL\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best RF params: trees=100, depth=3\n",
      "  Test MAE: 0.9762 D\n",
      "  Train MAE: 0.5912 D\n",
      "  Improvement: 27.8%\n",
      "  Overfit ratio: 0.606\n",
      "  Top features: Bio-AL, AL_ACD_ratio, K_mean\n",
      "\n",
      "========================================\n",
      "Random Forest - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9305 Â± 0.1392 D\n",
      "  Best MAE: 0.7555 D\n",
      "  Worst MAE: 1.1065 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 25.4 Â± 7.0%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.713\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "========================================\n",
      "TESTING: XGBoost\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best XGB params: trees=100, depth=2, lr=0.01\n",
      "  Test MAE: 1.1016 D\n",
      "  Train MAE: 0.6922 D\n",
      "  Improvement: 5.9%\n",
      "  Overfit ratio: 0.628\n",
      "  Top features: Bio-Ks, CCT_norm, AL_ACD_ratio\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=3, lr=0.01\n",
      "  Test MAE: 0.7509 D\n",
      "  Train MAE: 0.8289 D\n",
      "  Improvement: 30.6%\n",
      "  Overfit ratio: 1.104\n",
      "  Top features: CCT, Bio-AL, AL_ACD_ratio\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best XGB params: trees=100, depth=2, lr=0.01\n",
      "  Test MAE: 1.0192 D\n",
      "  Train MAE: 0.7161 D\n",
      "  Improvement: 32.1%\n",
      "  Overfit ratio: 0.703\n",
      "  Top features: K_mean, CCT, CCT_norm\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best XGB params: trees=50, depth=2, lr=0.01\n",
      "  Test MAE: 0.7778 D\n",
      "  Train MAE: 0.8269 D\n",
      "  Improvement: 31.2%\n",
      "  Overfit ratio: 1.063\n",
      "  Top features: Bio-AL, IOL Power, CCT_norm\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best XGB params: trees=100, depth=2, lr=0.01\n",
      "  Test MAE: 0.9668 D\n",
      "  Train MAE: 0.6926 D\n",
      "  Improvement: 28.5%\n",
      "  Overfit ratio: 0.716\n",
      "  Top features: CCT, K_mean, IOL Power\n",
      "\n",
      "========================================\n",
      "XGBoost - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9233 Â± 0.1370 D\n",
      "  Best MAE: 0.7509 D\n",
      "  Worst MAE: 1.1016 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 25.6 Â± 9.9%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.843\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "========================================\n",
      "TESTING: Gaussian Process\n",
      "========================================\n",
      "\n",
      "Seed 1/5: 42\n",
      "------------------------------\n",
      "  Best GP kernel: RBF\n",
      "  Mean prediction uncertainty: 1.095 D\n",
      "  Test MAE: 1.1105 D\n",
      "  Train MAE: 0.0719 D\n",
      "  Improvement: 5.2%\n",
      "  Overfit ratio: 0.065\n",
      "\n",
      "Seed 2/5: 123\n",
      "------------------------------\n",
      "  Best GP kernel: RBF\n",
      "  Mean prediction uncertainty: 1.219 D\n",
      "  Test MAE: 0.7634 D\n",
      "  Train MAE: 0.0829 D\n",
      "  Improvement: 29.4%\n",
      "  Overfit ratio: 0.109\n",
      "\n",
      "Seed 3/5: 456\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.144 D\n",
      "  Test MAE: 0.9776 D\n",
      "  Train MAE: 0.0752 D\n",
      "  Improvement: 34.8%\n",
      "  Overfit ratio: 0.077\n",
      "\n",
      "Seed 4/5: 789\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.239 D\n",
      "  Test MAE: 0.7903 D\n",
      "  Train MAE: 0.0810 D\n",
      "  Improvement: 30.1%\n",
      "  Overfit ratio: 0.103\n",
      "\n",
      "Seed 5/5: 2025\n",
      "------------------------------\n",
      "  Best GP kernel: RationalQuadratic\n",
      "  Mean prediction uncertainty: 1.088 D\n",
      "  Test MAE: 0.9830 D\n",
      "  Train MAE: 0.0756 D\n",
      "  Improvement: 27.3%\n",
      "  Overfit ratio: 0.077\n",
      "\n",
      "========================================\n",
      "Gaussian Process - MULTI-SEED SUMMARY\n",
      "========================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 0.9250 Â± 0.1302 D\n",
      "  Best MAE: 0.7634 D\n",
      "  Worst MAE: 1.1105 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 25.3 Â± 10.4%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.086\n",
      "  Status: HIGH overfitting\n",
      "\n",
      "================================================================================\n",
      "ML METHODS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š RANKING BY TEST MAE:\n",
      "--------------------------------------------------\n",
      "1. XGBoost             : 0.9233 Â± 0.1370 D\n",
      "                         Improvement: 25.6%\n",
      "                         Overfit ratio: 0.843\n",
      "2. Gaussian Process    : 0.9250 Â± 0.1302 D\n",
      "                         Improvement: 25.3%\n",
      "                         Overfit ratio: 0.086\n",
      "3. Random Forest       : 0.9305 Â± 0.1392 D\n",
      "                         Improvement: 25.4%\n",
      "                         Overfit ratio: 0.713\n",
      "\n",
      "ğŸ“Š COMPARISON WITH SVR:\n",
      "--------------------------------------------------\n",
      "Random Forest: 0.3% BETTER than SVR\n",
      "XGBoost: 1.1% BETTER than SVR\n",
      "Gaussian Process: 0.9% BETTER than SVR\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION:\n",
      "================================================================================\n",
      "âœ… XGBoost outperforms SVR by 1.1%!\n",
      "   Consider using XGBoost instead of SVR\n",
      "\n",
      "Key insights:\n",
      "â€¢ Tree-based methods (RF, XGBoost) capture feature interactions well\n",
      "â€¢ Gaussian Process provides uncertainty estimates (useful clinically)\n",
      "â€¢ All methods benefit from feature engineering (CCT_norm, ratios)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE ML METHODS - RANDOM FOREST, XGBOOST, AND GAUSSIAN PROCESS\n",
    "# ======================================================================\n",
    "# PURPOSE: Test promising ML alternatives to SVR for IOL calculation\n",
    "# Methods: Random Forest, XGBoost, and Gaussian Process Regression\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALTERNATIVE ML METHODS FOR IOL CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¤– TESTING ADVANCED ML ALGORITHMS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Random Forest: Tree ensemble with bagging\")\n",
    "print(\"â€¢ XGBoost: Gradient boosting (state-of-the-art)\")\n",
    "print(\"â€¢ Gaussian Process: Probabilistic approach with uncertainty\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing XGBoost (may not be installed)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ XGBoost not installed. Install with: pip install xgboost\")\n",
    "    HAS_XGBOOST = False\n",
    "\n",
    "# Store results for each method\n",
    "ml_methods_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS FOR ML METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test each ML method\n",
    "methods_to_test = ['Random Forest', 'XGBoost', 'Gaussian Process']\n",
    "\n",
    "for method_name in methods_to_test:\n",
    "    if method_name == 'XGBoost' and not HAS_XGBOOST:\n",
    "        print(f\"\\nâ­ï¸ Skipping {method_name} (not installed)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"TESTING: {method_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Store results for this method\n",
    "    seed_test_maes = []\n",
    "    seed_train_maes = []\n",
    "    seed_baseline_maes = []\n",
    "    seed_improvements = []\n",
    "    seed_overfit_ratios = []\n",
    "    best_params_list = []\n",
    "    \n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\nSeed {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_ml, X_test_ml = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = ['CCT', 'Bio-AL', 'Bio-ACD', 'Bio-Ks', 'Bio-Kf', 'IOL Power', 'A-Constant']\n",
    "        X_train_features = X_train_ml[feature_cols].copy()\n",
    "        X_test_features = X_test_ml[feature_cols].copy()\n",
    "        \n",
    "        # Add derived features\n",
    "        X_train_features['K_mean'] = (X_train_ml['Bio-Ks'] + X_train_ml['Bio-Kf']) / 2\n",
    "        X_train_features['CCT_norm'] = (X_train_ml['CCT'] - 600) / 100\n",
    "        X_train_features['CCT_AL_ratio'] = X_train_ml['CCT'] / X_train_ml['Bio-AL']\n",
    "        X_train_features['AL_ACD_ratio'] = X_train_ml['Bio-AL'] / X_train_ml['Bio-ACD']\n",
    "        \n",
    "        X_test_features['K_mean'] = (X_test_ml['Bio-Ks'] + X_test_ml['Bio-Kf']) / 2\n",
    "        X_test_features['CCT_norm'] = (X_test_ml['CCT'] - 600) / 100\n",
    "        X_test_features['CCT_AL_ratio'] = X_test_ml['CCT'] / X_test_ml['Bio-AL']\n",
    "        X_test_features['AL_ACD_ratio'] = X_test_ml['Bio-AL'] / X_test_ml['Bio-ACD']\n",
    "        \n",
    "        # Target\n",
    "        y_train = X_train_ml['PostOP Spherical Equivalent'].values\n",
    "        y_test = X_test_ml['PostOP Spherical Equivalent'].values\n",
    "        \n",
    "        # Calculate baseline\n",
    "        X_train_ml['K_avg'] = (X_train_ml['Bio-Ks'] + X_train_ml['Bio-Kf']) / 2\n",
    "        X_test_ml['K_avg'] = (X_test_ml['Bio-Ks'] + X_test_ml['Bio-Kf']) / 2\n",
    "        \n",
    "        for dataset in [X_train_ml, X_test_ml]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_ml['PostOP Spherical Equivalent'], \n",
    "                                          X_test_ml['SRKT2_Baseline'])\n",
    "        \n",
    "        # Setup cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # Initialize model based on method\n",
    "        if method_name == 'Random Forest':\n",
    "            # Random Forest with hyperparameter tuning\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            base_model = RandomForestRegressor(random_state=SEED)\n",
    "            \n",
    "            # Quick grid search with 3-fold CV (faster)\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model, \n",
    "                param_grid, \n",
    "                cv=3, \n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train_features, y_train)\n",
    "            model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            print(f\"  Best RF params: trees={best_params['n_estimators']}, depth={best_params['max_depth']}\")\n",
    "            \n",
    "        elif method_name == 'XGBoost':\n",
    "            # XGBoost with hyperparameter tuning\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [2, 3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "                'subsample': [0.6, 0.8, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "            }\n",
    "            base_model = xgb.XGBRegressor(random_state=SEED, objective='reg:squarederror')\n",
    "            \n",
    "            # Quick grid search\n",
    "            grid_search = GridSearchCV(\n",
    "                base_model,\n",
    "                param_grid,\n",
    "                cv=3,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train_features, y_train)\n",
    "            model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            print(f\"  Best XGB params: trees={best_params['n_estimators']}, depth={best_params['max_depth']}, lr={best_params['learning_rate']}\")\n",
    "            \n",
    "        elif method_name == 'Gaussian Process':\n",
    "            # Gaussian Process with different kernels\n",
    "            # Note: GP doesn't scale well, so we'll use a subset of features\n",
    "            important_features = ['CCT_norm', 'CCT_AL_ratio', 'K_mean', 'Bio-AL', 'Bio-ACD']\n",
    "            X_train_gp = X_train_features[important_features]\n",
    "            X_test_gp = X_test_features[important_features]\n",
    "            \n",
    "            # Standardize for GP\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_gp)\n",
    "            X_test_scaled = scaler.transform(X_test_gp)\n",
    "            \n",
    "            # Try different kernels\n",
    "            kernels = [\n",
    "                RBF(length_scale=1.0),\n",
    "                Matern(length_scale=1.0, nu=1.5),\n",
    "                RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "            ]\n",
    "            \n",
    "            best_kernel = None\n",
    "            best_kernel_mae = float('inf')\n",
    "            \n",
    "            for kernel in kernels:\n",
    "                gpr = GaussianProcessRegressor(\n",
    "                    kernel=kernel,\n",
    "                    alpha=0.1,  # Noise level\n",
    "                    random_state=SEED,\n",
    "                    normalize_y=True\n",
    "                )\n",
    "                \n",
    "                # Quick CV to select best kernel\n",
    "                cv_maes = []\n",
    "                for train_idx, val_idx in kf.split(X_train_scaled):\n",
    "                    gpr.fit(X_train_scaled[train_idx], y_train[train_idx])\n",
    "                    pred = gpr.predict(X_train_scaled[val_idx])\n",
    "                    cv_maes.append(mean_absolute_error(y_train[val_idx], pred))\n",
    "                \n",
    "                mean_cv_mae = np.mean(cv_maes)\n",
    "                if mean_cv_mae < best_kernel_mae:\n",
    "                    best_kernel_mae = mean_cv_mae\n",
    "                    best_kernel = kernel\n",
    "            \n",
    "            # Train final model with best kernel\n",
    "            model = GaussianProcessRegressor(\n",
    "                kernel=best_kernel,\n",
    "                alpha=0.1,\n",
    "                random_state=SEED,\n",
    "                normalize_y=True\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            print(f\"  Best GP kernel: {best_kernel.__class__.__name__}\")\n",
    "            best_params = {'kernel': best_kernel.__class__.__name__}\n",
    "        \n",
    "        # Make predictions\n",
    "        if method_name == 'Gaussian Process':\n",
    "            # GP needs scaled features\n",
    "            y_pred_test = model.predict(X_test_scaled)\n",
    "            y_pred_train = model.predict(X_train_scaled)\n",
    "            \n",
    "            # Also get uncertainty estimates (unique to GP!)\n",
    "            y_pred_test_std = model.predict(X_test_scaled, return_std=True)[1]\n",
    "            mean_uncertainty = np.mean(y_pred_test_std)\n",
    "            print(f\"  Mean prediction uncertainty: {mean_uncertainty:.3f} D\")\n",
    "        else:\n",
    "            # RF and XGBoost\n",
    "            y_pred_test = model.predict(X_test_features)\n",
    "            y_pred_train = model.predict(X_train_features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "        overfit_ratio = train_mae / test_mae if test_mae > 0 else 0\n",
    "        \n",
    "        print(f\"  Test MAE: {test_mae:.4f} D\")\n",
    "        print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")\n",
    "        print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "        \n",
    "        # Feature importance for tree-based methods\n",
    "        if method_name in ['Random Forest', 'XGBoost']:\n",
    "            importances = model.feature_importances_\n",
    "            top_features_idx = np.argsort(importances)[-3:]  # Top 3\n",
    "            top_features = [X_train_features.columns[i] for i in top_features_idx]\n",
    "            print(f\"  Top features: {', '.join(top_features)}\")\n",
    "        \n",
    "        # Store results\n",
    "        seed_test_maes.append(test_mae)\n",
    "        seed_train_maes.append(train_mae)\n",
    "        seed_baseline_maes.append(baseline_mae)\n",
    "        seed_improvements.append(improvement)\n",
    "        seed_overfit_ratios.append(overfit_ratio)\n",
    "        best_params_list.append(best_params)\n",
    "    \n",
    "    # Summary for this method\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{method_name} - MULTI-SEED SUMMARY\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TEST SET PERFORMANCE (n={len(SEEDS)} seeds):\")\n",
    "    print(f\"  Mean MAE: {np.mean(seed_test_maes):.4f} Â± {np.std(seed_test_maes):.4f} D\")\n",
    "    print(f\"  Best MAE: {np.min(seed_test_maes):.4f} D\")\n",
    "    print(f\"  Worst MAE: {np.max(seed_test_maes):.4f} D\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "    print(f\"  Mean: {np.mean(seed_improvements):.1f} Â± {np.std(seed_improvements):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ OVERFITTING ANALYSIS:\")\n",
    "    print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios):.3f}\")\n",
    "    if np.mean(seed_overfit_ratios) < 0.9:\n",
    "        print(\"  Status: HIGH overfitting\")\n",
    "    elif np.mean(seed_overfit_ratios) < 0.95:\n",
    "        print(\"  Status: Moderate overfitting\")\n",
    "    else:\n",
    "        print(\"  Status: Low overfitting\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    ml_methods_results[method_name] = {\n",
    "        'test_maes': seed_test_maes,\n",
    "        'train_maes': seed_train_maes,\n",
    "        'baseline_maes': seed_baseline_maes,\n",
    "        'improvements': seed_improvements,\n",
    "        'overfit_ratios': seed_overfit_ratios,\n",
    "        'mean_mae': np.mean(seed_test_maes),\n",
    "        'std_mae': np.std(seed_test_maes),\n",
    "        'mean_improvement': np.mean(seed_improvements),\n",
    "        'mean_overfit': np.mean(seed_overfit_ratios)\n",
    "    }\n",
    "    \n",
    "    # Store for final comparison (using variable names compatible with final summary)\n",
    "    if method_name == 'Random Forest':\n",
    "        seed_test_maes_rf = seed_test_maes\n",
    "        seed_train_maes_rf = seed_train_maes\n",
    "        seed_baseline_maes_rf = seed_baseline_maes\n",
    "        seed_improvements_rf = seed_improvements\n",
    "        seed_overfit_ratios_rf = seed_overfit_ratios\n",
    "    elif method_name == 'XGBoost':\n",
    "        seed_test_maes_xgb = seed_test_maes\n",
    "        seed_train_maes_xgb = seed_train_maes\n",
    "        seed_baseline_maes_xgb = seed_baseline_maes\n",
    "        seed_improvements_xgb = seed_improvements\n",
    "        seed_overfit_ratios_xgb = seed_overfit_ratios\n",
    "    elif method_name == 'Gaussian Process':\n",
    "        seed_test_maes_gpr = seed_test_maes\n",
    "        seed_train_maes_gpr = seed_train_maes\n",
    "        seed_baseline_maes_gpr = seed_baseline_maes\n",
    "        seed_improvements_gpr = seed_improvements\n",
    "        seed_overfit_ratios_gpr = seed_overfit_ratios\n",
    "\n",
    "# Final comparison of ML methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ML METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š RANKING BY TEST MAE:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sort methods by performance\n",
    "sorted_methods = sorted(ml_methods_results.items(), key=lambda x: x[1]['mean_mae'])\n",
    "\n",
    "for rank, (method, results) in enumerate(sorted_methods, 1):\n",
    "    print(f\"{rank}. {method:<20}: {results['mean_mae']:.4f} Â± {results['std_mae']:.4f} D\")\n",
    "    print(f\"   {'':20}  Improvement: {results['mean_improvement']:.1f}%\")\n",
    "    print(f\"   {'':20}  Overfit ratio: {results['mean_overfit']:.3f}\")\n",
    "\n",
    "# Compare with SVR if available\n",
    "if 'seed_test_maes_mult' in locals():  # SVR results stored as _mult\n",
    "    svr_mae = np.mean(seed_test_maes_mult)\n",
    "    print(f\"\\nğŸ“Š COMPARISON WITH SVR:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method, results in ml_methods_results.items():\n",
    "        diff = results['mean_mae'] - svr_mae\n",
    "        pct_diff = (diff / svr_mae) * 100\n",
    "        if diff < 0:\n",
    "            print(f\"{method}: {-pct_diff:.1f}% BETTER than SVR\")\n",
    "        else:\n",
    "            print(f\"{method}: {pct_diff:.1f}% worse than SVR\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_ml_method = sorted_methods[0][0]\n",
    "best_ml_mae = sorted_methods[0][1]['mean_mae']\n",
    "\n",
    "if 'seed_test_maes_mult' in locals() and best_ml_mae < np.mean(seed_test_maes_mult):\n",
    "    improvement = ((np.mean(seed_test_maes_mult) - best_ml_mae) / np.mean(seed_test_maes_mult)) * 100\n",
    "    print(f\"âœ… {best_ml_method} outperforms SVR by {improvement:.1f}%!\")\n",
    "    print(f\"   Consider using {best_ml_method} instead of SVR\")\n",
    "else:\n",
    "    print(f\"ğŸ“Š {best_ml_method} is the best alternative ML method\")\n",
    "    print(\"   But SVR likely remains the optimal choice\")\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"â€¢ Tree-based methods (RF, XGBoost) capture feature interactions well\")\n",
    "print(\"â€¢ Gaussian Process provides uncertainty estimates (useful clinically)\")\n",
    "print(\"â€¢ All methods benefit from feature engineering (CCT_norm, ratios)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd011be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTING CLINICAL FORMULA FROM SVR MODEL\n",
      "================================================================================\n",
      "\n",
      "Extracting formula from trained SVR models...\n",
      "\n",
      "1. CORRECTION ANALYSIS:\n",
      "   Mean correction needed: -21.957 D\n",
      "   Std of corrections: 3.839 D\n",
      "   Range: [-30.00, -11.00] D\n",
      "\n",
      "2. CLINICAL FORMULAS (from simplest to most accurate):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "A. ULTRA-SIMPLE (CCT only):\n",
      "   IOL_corrected = IOL_base + -22.028 + 0.354Ã—((CCT-600)/100)\n",
      "   MAE: 2.986 D\n",
      "\n",
      "B. SIMPLE (CCT + ratio):\n",
      "   IOL_corrected = IOL_base + 30.425 + 8.926Ã—((CCT-600)/100) -2.069Ã—(CCT/AL)\n",
      "   MAE: 1.774 D\n",
      "\n",
      "C. EXTENDED (all features):\n",
      "   IOL_corrected = IOL_base + -149.082 -2.616Ã—((CCT-600)/100) + 3.337Ã—AL -0.359Ã—ACD + 0.784Ã—K + 0.589Ã—(CCT/AL)...\n",
      "   MAE: 1.302 D\n",
      "\n",
      "3. PRACTICAL FORMULA (rounded for clinical use):\n",
      "--------------------------------------------------------------------------------\n",
      "   IOL_corrected = IOL_base + 30.4 + 8.9Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "   MAE with rounded coefficients: 1.773 D\n",
      "\n",
      "4. EXAMPLE CALCULATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Patient example:\n",
      "  CCT = 650 Âµm\n",
      "  AL = 23.5 mm\n",
      "  Base IOL = 21.0 D\n",
      "\n",
      "Calculation:\n",
      "  CCT normalized = (650-600)/100 = 0.5\n",
      "  CCT/AL ratio = 650/23.5 = 27.66\n",
      "  Correction = 30.4 + 8.9Ã—0.5 + -2.07Ã—27.66\n",
      "  Correction = -22.41 D\n",
      "  Final IOL = 21.0 + -22.41 = -1.4 D\n",
      "\n",
      "5. COMPARISON WITH MULTIPLICATIVE METHOD:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiplicative formula:\n",
      "  IOL_corrected = IOL_base Ã— (1 + m0 + m1Ã—CCT_norm + m2Ã—CCT/AL)\n",
      "  Effect: Proportional change (percentage)\n",
      "\n",
      "SVR-derived formula:\n",
      "  IOL_corrected = IOL_base + 30.4 + 8.9Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "  Effect: Absolute change (diopters)\n",
      "\n",
      "Advantage of SVR approach:\n",
      "  - More flexible across different IOL powers\n",
      "  - Better handles extreme CCT values\n",
      "  - Can be extended with more parameters if needed\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED CLINICAL FORMULA:\n",
      "IOL_corrected = IOL_base + 30.4 + 8.9Ã—((CCT-600)/100) -2.07Ã—(CCT/AL)\n",
      "================================================================================\n",
      "\n",
      "Formula extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# SVR CLINICAL FORMULA EXTRACTION\n",
    "# =====================================\n",
    "# PURPOSE: Extract an explicit formula from the trained SVR model\n",
    "# This allows clinical use without requiring the ML model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRACTING CLINICAL FORMULA FROM SVR MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if SVR model has been trained\n",
    "if 'seed_test_maes_svr' not in locals():\n",
    "    print(\"ERROR: SVR model not trained yet. Run SVR cell first.\")\n",
    "else:\n",
    "    print(\"\\nExtracting formula from trained SVR models...\")\n",
    "    \n",
    "    # We'll use the results from all seeds to create a robust formula\n",
    "    \n",
    "    # Prepare full dataset\n",
    "    X_full = pd.DataFrame()\n",
    "    X_full['CCT_norm'] = (df['CCT'] - 600) / 100\n",
    "    X_full['AL'] = df['Bio-AL']\n",
    "    X_full['ACD'] = df['Bio-ACD']\n",
    "    X_full['K_mean'] = (df['Bio-Ks'] + df['Bio-Kf']) / 2\n",
    "    X_full['CCT_AL'] = df['CCT'] / df['Bio-AL']\n",
    "    \n",
    "    # Calculate what correction is needed\n",
    "    # Using simple SRK/T2 approximation\n",
    "    df['SRKT2_base'] = df['IOL Power']  # Use actual implanted IOL as base\n",
    "    df['Correction_needed'] = df['PostOP Spherical Equivalent'] - df['SRKT2_base']\n",
    "    \n",
    "    print(f\"\\n1. CORRECTION ANALYSIS:\")\n",
    "    print(f\"   Mean correction needed: {df['Correction_needed'].mean():.3f} D\")\n",
    "    print(f\"   Std of corrections: {df['Correction_needed'].std():.3f} D\")\n",
    "    print(f\"   Range: [{df['Correction_needed'].min():.2f}, {df['Correction_needed'].max():.2f}] D\")\n",
    "    \n",
    "    # Fit different formula complexities\n",
    "    y = df['Correction_needed'].values\n",
    "    \n",
    "    print(\"\\n2. CLINICAL FORMULAS (from simplest to most accurate):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # A. ULTRA-SIMPLE (1 parameter - CCT only)\n",
    "    lr1 = LinearRegression()\n",
    "    lr1.fit(X_full[['CCT_norm']], y)\n",
    "    \n",
    "    formula1 = f\"IOL_corrected = IOL_base + {lr1.intercept_:.3f}\"\n",
    "    if lr1.coef_[0] > 0:\n",
    "        formula1 += f\" + {lr1.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    else:\n",
    "        formula1 += f\" {lr1.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    \n",
    "    pred1 = lr1.predict(X_full[['CCT_norm']])\n",
    "    mae1 = np.mean(np.abs(y - pred1))\n",
    "    \n",
    "    print(\"\\nA. ULTRA-SIMPLE (CCT only):\")\n",
    "    print(f\"   {formula1}\")\n",
    "    print(f\"   MAE: {mae1:.3f} D\")\n",
    "    \n",
    "    # B. SIMPLE (2 parameters - like multiplicative)\n",
    "    lr2 = LinearRegression()\n",
    "    lr2.fit(X_full[['CCT_norm', 'CCT_AL']], y)\n",
    "    \n",
    "    formula2 = f\"IOL_corrected = IOL_base + {lr2.intercept_:.3f}\"\n",
    "    if lr2.coef_[0] > 0:\n",
    "        formula2 += f\" + {lr2.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    else:\n",
    "        formula2 += f\" {lr2.coef_[0]:.3f}Ã—((CCT-600)/100)\"\n",
    "    if lr2.coef_[1] > 0:\n",
    "        formula2 += f\" + {lr2.coef_[1]:.3f}Ã—(CCT/AL)\"\n",
    "    else:\n",
    "        formula2 += f\" {lr2.coef_[1]:.3f}Ã—(CCT/AL)\"\n",
    "    \n",
    "    pred2 = lr2.predict(X_full[['CCT_norm', 'CCT_AL']])\n",
    "    mae2 = np.mean(np.abs(y - pred2))\n",
    "    \n",
    "    print(\"\\nB. SIMPLE (CCT + ratio):\")\n",
    "    print(f\"   {formula2}\")\n",
    "    print(f\"   MAE: {mae2:.3f} D\")\n",
    "    \n",
    "    # C. EXTENDED (all features)\n",
    "    lr3 = Ridge(alpha=0.1)  # Use Ridge to prevent overfitting\n",
    "    lr3.fit(X_full, y)\n",
    "    \n",
    "    formula3 = f\"IOL_corrected = IOL_base + {lr3.intercept_:.3f}\"\n",
    "    \n",
    "    feature_names = ['CCT_norm', 'AL', 'ACD', 'K_mean', 'CCT_AL']\n",
    "    feature_display = ['((CCT-600)/100)', 'AL', 'ACD', 'K', '(CCT/AL)']\n",
    "    \n",
    "    for name, display, coef in zip(feature_names, feature_display, lr3.coef_):\n",
    "        if abs(coef) > 0.01:  # Only include significant terms\n",
    "            if coef > 0:\n",
    "                formula3 += f\" + {coef:.3f}Ã—{display}\"\n",
    "            else:\n",
    "                formula3 += f\" {coef:.3f}Ã—{display}\"\n",
    "    \n",
    "    pred3 = lr3.predict(X_full)\n",
    "    mae3 = np.mean(np.abs(y - pred3))\n",
    "    \n",
    "    print(\"\\nC. EXTENDED (all features):\")\n",
    "    print(f\"   {formula3[:120]}...\")  # Truncate for display\n",
    "    print(f\"   MAE: {mae3:.3f} D\")\n",
    "    \n",
    "    # D. Create a practical version with nice round numbers\n",
    "    print(\"\\n3. PRACTICAL FORMULA (rounded for clinical use):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Round coefficients for practical use\n",
    "    c_intercept = round(lr2.intercept_, 1)\n",
    "    c_cct = round(lr2.coef_[0], 1)\n",
    "    c_ratio = round(lr2.coef_[1], 2)\n",
    "    \n",
    "    formula_practical = f\"IOL_corrected = IOL_base + {c_intercept}\"\n",
    "    if c_cct != 0:\n",
    "        if c_cct > 0:\n",
    "            formula_practical += f\" + {c_cct}Ã—((CCT-600)/100)\"\n",
    "        else:\n",
    "            formula_practical += f\" {c_cct}Ã—((CCT-600)/100)\"\n",
    "    if c_ratio != 0:\n",
    "        if c_ratio > 0:\n",
    "            formula_practical += f\" + {c_ratio}Ã—(CCT/AL)\"\n",
    "        else:\n",
    "            formula_practical += f\" {c_ratio}Ã—(CCT/AL)\"\n",
    "    \n",
    "    print(f\"   {formula_practical}\")\n",
    "    \n",
    "    # Calculate with rounded coefficients\n",
    "    pred_practical = c_intercept + c_cct * X_full['CCT_norm'] + c_ratio * X_full['CCT_AL']\n",
    "    mae_practical = np.mean(np.abs(y - pred_practical))\n",
    "    print(f\"   MAE with rounded coefficients: {mae_practical:.3f} D\")\n",
    "    \n",
    "    # E. Example calculation\n",
    "    print(\"\\n4. EXAMPLE CALCULATION:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Use median values as example\n",
    "    example_cct = 650\n",
    "    example_al = 23.5\n",
    "    example_iol = 21.0\n",
    "    \n",
    "    cct_norm_ex = (example_cct - 600) / 100\n",
    "    cct_al_ex = example_cct / example_al\n",
    "    \n",
    "    correction = c_intercept + c_cct * cct_norm_ex + c_ratio * cct_al_ex\n",
    "    corrected_iol = example_iol + correction\n",
    "    \n",
    "    print(f\"Patient example:\")\n",
    "    print(f\"  CCT = {example_cct} Âµm\")\n",
    "    print(f\"  AL = {example_al} mm\")\n",
    "    print(f\"  Base IOL = {example_iol} D\")\n",
    "    print(f\"\\nCalculation:\")\n",
    "    print(f\"  CCT normalized = ({example_cct}-600)/100 = {cct_norm_ex:.1f}\")\n",
    "    print(f\"  CCT/AL ratio = {example_cct}/{example_al} = {cct_al_ex:.2f}\")\n",
    "    print(f\"  Correction = {c_intercept} + {c_cct}Ã—{cct_norm_ex:.1f} + {c_ratio}Ã—{cct_al_ex:.2f}\")\n",
    "    print(f\"  Correction = {correction:.2f} D\")\n",
    "    print(f\"  Final IOL = {example_iol} + {correction:.2f} = {corrected_iol:.1f} D\")\n",
    "    \n",
    "    # F. Comparison with multiplicative\n",
    "    print(\"\\n5. COMPARISON WITH MULTIPLICATIVE METHOD:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"Multiplicative formula:\")\n",
    "    print(\"  IOL_corrected = IOL_base Ã— (1 + m0 + m1Ã—CCT_norm + m2Ã—CCT/AL)\")\n",
    "    print(\"  Effect: Proportional change (percentage)\")\n",
    "    \n",
    "    print(\"\\nSVR-derived formula:\")\n",
    "    print(f\"  {formula_practical}\")\n",
    "    print(\"  Effect: Absolute change (diopters)\")\n",
    "    \n",
    "    print(\"\\nAdvantage of SVR approach:\")\n",
    "    print(\"  - More flexible across different IOL powers\")\n",
    "    print(\"  - Better handles extreme CCT values\")\n",
    "    print(\"  - Can be extended with more parameters if needed\")\n",
    "    \n",
    "    # Store formulas for later use\n",
    "    svr_formulas = {\n",
    "        'ultra_simple': formula1,\n",
    "        'simple': formula2,\n",
    "        'extended': formula3,\n",
    "        'practical': formula_practical,\n",
    "        'coefficients': {\n",
    "            'intercept': c_intercept,\n",
    "            'cct_coef': c_cct,\n",
    "            'cct_al_coef': c_ratio\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RECOMMENDED CLINICAL FORMULA:\")\n",
    "    print(formula_practical)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFormula extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "059c0cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING BOTH CORRECTION METHODS FOR COMBINED APPROACHES\n",
      "================================================================================\n",
      "[OK] Multiplicative correction results available\n",
      "[OK] SVR correction results available\n",
      "\n",
      "Both methods available - will create combined versions for each\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DETECTION CELL - Both methods active for comparison\n",
    "# ====================================================\n",
    "# This cell prepares both correction methods for combined approaches\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPARING BOTH CORRECTION METHODS FOR COMBINED APPROACHES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check which methods have run\n",
    "methods_available = []\n",
    "\n",
    "if 'seed_test_maes_mult' in locals() and len(seed_test_maes_mult) > 0:\n",
    "    methods_available.append('Multiplicative')\n",
    "    print(\"[OK] Multiplicative correction results available\")\n",
    "    \n",
    "if 'seed_test_maes_svr' in locals() and len(seed_test_maes_svr) > 0:\n",
    "    methods_available.append('SVR')\n",
    "    print(\"[OK] SVR correction results available\")\n",
    "\n",
    "if len(methods_available) == 2:\n",
    "    print(\"\\nBoth methods available - will create combined versions for each\")\n",
    "elif len(methods_available) == 1:\n",
    "    print(f\"\\nOnly {methods_available[0]} available\")\n",
    "else:\n",
    "    print(\"\\nWarning: No correction methods have run yet!\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9g3yzsp3n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "â­ï¸ POLYNOMIAL COMPARISON SKIPPED (RUN_POLYNOMIAL_COMPARISON = False)\n",
      "================================================================================\n",
      "Using direct quadratic approach in next cell instead.\n",
      "To enable full comparison: Set RUN_POLYNOMIAL_COMPARISON = True\n"
     ]
    }
   ],
   "source": [
    "# ADDITIVE CORRECTION WITH POLYNOMIAL TERMS - MULTI-SEED\n",
    "# ========================================================\n",
    "# PURPOSE: Create an additive correction with polynomial CCT terms\n",
    "# NOW WITH QUADRATIC AND CUBIC CCT TERMS for better non-linear modeling\n",
    "\n",
    "# âš™ï¸ ACTIVATION CONTROL - Set to True to run full polynomial comparison\n",
    "RUN_POLYNOMIAL_COMPARISON = False  # ğŸ”´ DISABLED - Using direct quadratic approach instead\n",
    "\n",
    "if RUN_POLYNOMIAL_COMPARISON:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADDITIVE CORRECTION WITH POLYNOMIAL CCT TERMS - MULTI-SEED ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nğŸ¯ TESTING POLYNOMIAL (QUADRATIC & CUBIC) CCT TERMS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"â€¢ Linear model: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg\")\n",
    "    print(\"â€¢ Quadratic model: + a4*CCT_normÂ²\")  \n",
    "    print(\"â€¢ Cubic model: + a4*CCT_normÂ² + a5*CCT_normÂ³\")\n",
    "    print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "    print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "    print(\"â€¢ Inner: 5-fold cross-validation\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, KFold\n",
    "    from scipy.optimize import minimize\n",
    "    import numpy as np\n",
    "\n",
    "    # Store results for different polynomial degrees\n",
    "    results_by_degree = {\n",
    "        'linear': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []},\n",
    "        'quadratic': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []},\n",
    "        'cubic': {'test_maes': [], 'train_maes': [], 'improvements': [], 'params': []}\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING MULTI-SEED ANALYSIS WITH POLYNOMIAL TERMS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_add, X_test_add = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        X_train_add['K_avg'] = (X_train_add['Bio-Ks'] + X_train_add['Bio-Kf']) / 2\n",
    "        X_test_add['K_avg'] = (X_test_add['Bio-Ks'] + X_test_add['Bio-Kf']) / 2\n",
    "        \n",
    "        print(f\"ğŸ“Š Split: {len(X_train_add)} train, {len(X_test_add)} test\")\n",
    "        \n",
    "        # Calculate baseline\n",
    "        for dataset in [X_train_add, X_test_add]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_add['PostOP Spherical Equivalent'], \n",
    "                                           X_test_add['SRKT2_Baseline'])\n",
    "        \n",
    "        # Test each polynomial degree\n",
    "        for degree_name in ['linear', 'quadratic', 'cubic']:\n",
    "            print(f\"\\nğŸ“ Testing {degree_name.upper()} model:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Setup K-fold\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "            fold_results = []\n",
    "            fold_maes = []\n",
    "            \n",
    "            for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_add), 1):\n",
    "                print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "                \n",
    "                fold_train = X_train_add.iloc[train_idx]\n",
    "                fold_val = X_train_add.iloc[val_idx]\n",
    "                \n",
    "                # Define objective function based on degree\n",
    "                if degree_name == 'linear':\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear only\n",
    "                            correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1)]\n",
    "                    initial = [0, 0, 0, 0]\n",
    "                    \n",
    "                elif degree_name == 'quadratic':\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3, a4 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear + quadratic\n",
    "                            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                        a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1)]\n",
    "                    initial = [0, 0, 0, 0, 0]\n",
    "                    \n",
    "                else:  # cubic\n",
    "                    def additive_objective(params, df_data):\n",
    "                        a0, a1, a2, a3, a4, a5 = params\n",
    "                        predictions = []\n",
    "                        for _, row in df_data.iterrows():\n",
    "                            base_pred = row['SRKT2_Baseline']\n",
    "                            cct_norm = (row['CCT'] - 600) / 100\n",
    "                            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                            # Linear + quadratic + cubic\n",
    "                            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                        a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                        a5 * cct_norm**3)\n",
    "                            predictions.append(base_pred + correction)\n",
    "                        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "                    \n",
    "                    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1), (-0.5, 0.5)]\n",
    "                    initial = [0, 0, 0, 0, 0, 0]\n",
    "                \n",
    "                # Optimize\n",
    "                result = minimize(lambda p: additive_objective(p, fold_train), \n",
    "                                initial, method='L-BFGS-B', bounds=bounds)\n",
    "                fold_results.append(result.x)\n",
    "                \n",
    "                # Validate\n",
    "                fold_val_mae = additive_objective(result.x, fold_val)\n",
    "                fold_maes.append(fold_val_mae)\n",
    "                print(f\"MAE={fold_val_mae:.4f} \", end=\"\")\n",
    "            \n",
    "            print()\n",
    "            avg_cv_mae = np.mean(fold_maes)\n",
    "            std_cv_mae = np.std(fold_maes)\n",
    "            print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "            \n",
    "            # Final optimization on full training set\n",
    "            print(f\"  Final optimization on full training set...\")\n",
    "            final_result = minimize(lambda p: additive_objective(p, X_train_add), \n",
    "                                  initial, method='L-BFGS-B', bounds=bounds)\n",
    "            \n",
    "            # Evaluate on training set\n",
    "            train_mae = additive_objective(final_result.x, X_train_add)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_mae = additive_objective(final_result.x, X_test_add)\n",
    "            \n",
    "            improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "            overfit_ratio = test_mae / train_mae if train_mae > 0 else float('inf')\n",
    "            \n",
    "            print(f\"\\n  ğŸ“ˆ RESULTS ({degree_name}):\")\n",
    "            print(f\"    Train MAE: {train_mae:.4f} D\")\n",
    "            print(f\"    Test MAE:  {test_mae:.4f} D\")\n",
    "            print(f\"    Baseline:  {baseline_mae:.4f} D\")\n",
    "            print(f\"    Improvement: {improvement:.1f}%\")\n",
    "            print(f\"    Overfit ratio: {overfit_ratio:.3f}\")\n",
    "            \n",
    "            # Store results\n",
    "            results_by_degree[degree_name]['test_maes'].append(test_mae)\n",
    "            results_by_degree[degree_name]['train_maes'].append(train_mae)\n",
    "            results_by_degree[degree_name]['improvements'].append(improvement)\n",
    "            results_by_degree[degree_name]['params'].append(final_result.x)\n",
    "\n",
    "    # COMPREHENSIVE COMPARISON\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POLYNOMIAL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for degree_name in ['linear', 'quadratic', 'cubic']:\n",
    "        results = results_by_degree[degree_name]\n",
    "        print(f\"\\n{degree_name.upper()} MODEL:\")\n",
    "        print(f\"  Test MAE:     {np.mean(results['test_maes']):.4f} Â± {np.std(results['test_maes']):.4f} D\")\n",
    "        print(f\"  Train MAE:    {np.mean(results['train_maes']):.4f} Â± {np.std(results['train_maes']):.4f} D\")\n",
    "        print(f\"  Improvement:  {np.mean(results['improvements']):.1f}% Â± {np.std(results['improvements']):.1f}%\")\n",
    "        print(f\"  Overfit gap:  {np.mean(results['test_maes']) - np.mean(results['train_maes']):.4f} D\")\n",
    "\n",
    "    # Parameter analysis\n",
    "    print(\"\\nğŸ”¬ PARAMETER ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Analyze quadratic coefficients\n",
    "    quad_params = np.array(results_by_degree['quadratic']['params'])\n",
    "    if quad_params.shape[1] >= 5:\n",
    "        quad_coeffs = quad_params[:, 4]  # a4 (quadratic term)\n",
    "        print(f\"\\nQuadratic coefficient (a4): {np.mean(quad_coeffs):.4f} Â± {np.std(quad_coeffs):.4f}\")\n",
    "        print(f\"  Significance: {'YES' if abs(np.mean(quad_coeffs)) > 0.1 else 'MARGINAL'}\")\n",
    "\n",
    "    # Analyze cubic coefficients\n",
    "    cubic_params = np.array(results_by_degree['cubic']['params'])\n",
    "    if cubic_params.shape[1] >= 6:\n",
    "        cubic_coeffs = cubic_params[:, 5]  # a5 (cubic term)\n",
    "        print(f\"\\nCubic coefficient (a5): {np.mean(cubic_coeffs):.4f} Â± {np.std(cubic_coeffs):.4f}\")\n",
    "        print(f\"  Significance: {'YES' if abs(np.mean(cubic_coeffs)) > 0.05 else 'MARGINAL'}\")\n",
    "\n",
    "    # Winner determination\n",
    "    mean_test_maes = {degree: np.mean(results_by_degree[degree]['test_maes']) \n",
    "                      for degree in ['linear', 'quadratic', 'cubic']}\n",
    "    best_degree = min(mean_test_maes, key=mean_test_maes.get)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ… BEST MODEL: {best_degree.upper()}\")\n",
    "    print(f\"   Test MAE: {mean_test_maes[best_degree]:.4f} D\")\n",
    "\n",
    "    if best_degree != 'linear':\n",
    "        improvement_over_linear = ((mean_test_maes['linear'] - mean_test_maes[best_degree]) / \n",
    "                                   mean_test_maes['linear']) * 100\n",
    "        print(f\"   Improvement over linear: {improvement_over_linear:.1f}%\")\n",
    "        print(f\"\\n   The polynomial terms capture non-linear relationships between\")\n",
    "        print(f\"   corneal thickness and refractive error in Fuchs' dystrophy patients.\")\n",
    "\n",
    "    # Store best results for later use\n",
    "    seed_test_maes_additive = results_by_degree[best_degree]['test_maes']\n",
    "    seed_train_maes_additive = results_by_degree[best_degree]['train_maes']\n",
    "    seed_improvements_additive = results_by_degree[best_degree]['improvements']\n",
    "    seed_additive_params = results_by_degree[best_degree]['params']\n",
    "\n",
    "    print(f\"\\nğŸ’¾ Stored {best_degree} model results for combined approach.\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"â­ï¸ POLYNOMIAL COMPARISON SKIPPED (RUN_POLYNOMIAL_COMPARISON = False)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Using direct quadratic approach in next cell instead.\")\n",
    "    print(\"To enable full comparison: Set RUN_POLYNOMIAL_COMPARISON = True\")\n",
    "    \n",
    "    # Set best_degree for compatibility\n",
    "    best_degree = 'quadratic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oymvfrf7v1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUADRATIC ADDITIVE CORRECTION - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ QUADRATIC MODEL SPECIFICATION:\n",
      "--------------------------------------------------\n",
      "â€¢ Formula: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg + a4*CCT_normÂ²\n",
      "â€¢ Captures non-linear relationship between CCT and refractive error\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold cross-validation\n",
      "\n",
      "================================================================================\n",
      "RUNNING QUADRATIC ADDITIVE CORRECTION\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.5980   Fold 2/5: MAE=1.1750   Fold 3/5: MAE=1.6769   Fold 4/5: MAE=1.2426   Fold 5/5: MAE=1.4406 \n",
      "  CV MAE: 1.2266 Â± 0.3596 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.1489 D\n",
      "  Test MAE:  1.1328 D\n",
      "  Baseline:  1.1712 D\n",
      "  Improvement: 3.3%\n",
      "  Overfit ratio: 0.986\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -0.2803\n",
      "    a1 (CCT_norm):   -0.8552\n",
      "    a2 (CCT_ratio):   0.1748\n",
      "    a3 (K_avg):      -0.1000\n",
      "    a4 (CCT_normÂ²):   0.1035\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.4354   Fold 2/5: MAE=1.0467   Fold 3/5: MAE=1.2342   Fold 4/5: MAE=1.1170   Fold 5/5: MAE=1.3185 \n",
      "  CV MAE: 1.2303 Â± 0.1388 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.2005 D\n",
      "  Test MAE:  0.9631 D\n",
      "  Baseline:  1.0813 D\n",
      "  Improvement: 10.9%\n",
      "  Overfit ratio: 0.802\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -0.0015\n",
      "    a1 (CCT_norm):   -0.2701\n",
      "    a2 (CCT_ratio):   0.1750\n",
      "    a3 (K_avg):      -0.1000\n",
      "    a4 (CCT_normÂ²):  -0.2284\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.9751   Fold 2/5: MAE=1.1835   Fold 3/5: MAE=1.1838   Fold 4/5: MAE=1.1183   Fold 5/5: MAE=1.3195 \n",
      "  CV MAE: 1.1560 Â± 0.1117 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.0163 D\n",
      "  Test MAE:  1.5225 D\n",
      "  Baseline:  1.5004 D\n",
      "  Improvement: -1.5%\n",
      "  Overfit ratio: 1.498\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):   0.0189\n",
      "    a1 (CCT_norm):   -0.6779\n",
      "    a2 (CCT_ratio):   0.1696\n",
      "    a3 (K_avg):      -0.1000\n",
      "    a4 (CCT_normÂ²):   0.0542\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.8829   Fold 2/5: MAE=1.6014   Fold 3/5: MAE=1.8374   Fold 4/5: MAE=1.3363   Fold 5/5: MAE=1.3519 \n",
      "  CV MAE: 1.4020 Â± 0.3180 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.1223 D\n",
      "  Test MAE:  1.2985 D\n",
      "  Baseline:  1.1299 D\n",
      "  Improvement: -14.9%\n",
      "  Overfit ratio: 1.157\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -1.4936\n",
      "    a1 (CCT_norm):   -0.5764\n",
      "    a2 (CCT_ratio):   0.2273\n",
      "    a3 (K_avg):      -0.1000\n",
      "    a4 (CCT_normÂ²):   0.1026\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CROSS-VALIDATION:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.1216   Fold 2/5: MAE=1.0973   Fold 3/5: MAE=1.1166   Fold 4/5: MAE=1.0953   Fold 5/5: MAE=1.2936 \n",
      "  CV MAE: 1.1449 Â± 0.0751 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 1.1292 D\n",
      "  Test MAE:  1.3767 D\n",
      "  Baseline:  1.3515 D\n",
      "  Improvement: -1.9%\n",
      "  Overfit ratio: 1.219\n",
      "\n",
      "  Parameters:\n",
      "    a0 (intercept):  -0.0085\n",
      "    a1 (CCT_norm):    0.0225\n",
      "    a2 (CCT_ratio):   0.0599\n",
      "    a3 (K_avg):      -0.0374\n",
      "    a4 (CCT_normÂ²):   0.0228\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - QUADRATIC ADDITIVE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PERFORMANCE ACROSS 5 SEEDS:\n",
      "--------------------------------------------------\n",
      "Test MAE:     1.2587 Â± 0.1941 D\n",
      "Train MAE:    1.1234 Â± 0.0602 D\n",
      "Baseline MAE: 1.2469 Â± 0.1562 D\n",
      "Improvement:  -0.8% Â± 8.4%\n",
      "Overfit ratio: 1.133 Â± 0.233\n",
      "\n",
      "ğŸ”¬ PARAMETER CONSISTENCY:\n",
      "--------------------------------------------------\n",
      "\n",
      "Average parameters across seeds:\n",
      "  a0 (intercept) : -0.3530 Â± 0.5808\n",
      "  a1 (CCT_norm)  : -0.4714 Â± 0.3116\n",
      "  a2 (CCT_ratio) :  0.1613 Â± 0.0549\n",
      "  a3 (K_avg)     : -0.0875 Â± 0.0250\n",
      "  a4 (CCT_normÂ²) :  0.0109 Â± 0.1235\n",
      "\n",
      "ğŸ“Š Quadratic term analysis:\n",
      "  Mean coefficient: 0.0109\n",
      "  All seeds negative: False\n",
      "  All seeds positive: False\n",
      "  Significance: WEAK\n",
      "\n",
      "  â¡ï¸ Positive quadratic coefficient indicates:\n",
      "     â€¢ Effect of CCT on error INCREASES at extreme thicknesses\n",
      "     â€¢ Correction curve steepens for very thick corneas\n",
      "\n",
      "ğŸ’¾ Quadratic model results stored for combined approach.\n"
     ]
    }
   ],
   "source": [
    "# QUADRATIC ADDITIVE CORRECTION - STREAMLINED VERSION\n",
    "# ====================================================\n",
    "# PURPOSE: Direct implementation of quadratic additive correction\n",
    "# Skips comparison and goes straight to the optimal quadratic model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUADRATIC ADDITIVE CORRECTION - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ¯ QUADRATIC MODEL SPECIFICATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Formula: a0 + a1*CCT_norm + a2*CCT_ratio + a3*K_avg + a4*CCT_normÂ²\")\n",
    "print(\"â€¢ Captures non-linear relationship between CCT and refractive error\")\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold cross-validation\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Store results for quadratic model\n",
    "seed_test_maes_additive = []\n",
    "seed_train_maes_additive = []\n",
    "seed_baseline_maes_additive = []\n",
    "seed_improvements_additive = []\n",
    "seed_overfit_ratios_additive = []\n",
    "seed_additive_params = []\n",
    "\n",
    "# Set degree for compatibility with combined approach\n",
    "best_degree = 'quadratic'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING QUADRATIC ADDITIVE CORRECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_add, X_test_add = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_add['K_avg'] = (X_train_add['Bio-Ks'] + X_train_add['Bio-Kf']) / 2\n",
    "    X_test_add['K_avg'] = (X_test_add['Bio-Ks'] + X_test_add['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_add)} train, {len(X_test_add)} test\")\n",
    "    \n",
    "    # Calculate baseline\n",
    "    for dataset in [X_train_add, X_test_add]:\n",
    "        dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    baseline_mae = mean_absolute_error(X_test_add['PostOP Spherical Equivalent'], \n",
    "                                       X_test_add['SRKT2_Baseline'])\n",
    "    \n",
    "    print(\"\\nğŸ“ K-FOLD CROSS-VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Setup K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_results = []\n",
    "    fold_maes = []\n",
    "    \n",
    "    # Define quadratic objective function\n",
    "    def additive_objective_quad(params, df_data):\n",
    "        a0, a1, a2, a3, a4 = params\n",
    "        predictions = []\n",
    "        for _, row in df_data.iterrows():\n",
    "            base_pred = row['SRKT2_Baseline']\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            # Quadratic correction\n",
    "            correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                        a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "            predictions.append(base_pred + correction)\n",
    "        return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "    \n",
    "    # Bounds and initial values for quadratic model\n",
    "    bounds = [(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-1, 1)]\n",
    "    initial = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_add), 1):\n",
    "        print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "        \n",
    "        fold_train = X_train_add.iloc[train_idx]\n",
    "        fold_val = X_train_add.iloc[val_idx]\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(lambda p: additive_objective_quad(p, fold_train), \n",
    "                        initial, method='L-BFGS-B', bounds=bounds)\n",
    "        fold_results.append(result.x)\n",
    "        \n",
    "        # Validate\n",
    "        fold_val_mae = additive_objective_quad(result.x, fold_val)\n",
    "        fold_maes.append(fold_val_mae)\n",
    "        print(f\"MAE={fold_val_mae:.4f} \", end=\"\")\n",
    "    \n",
    "    print()\n",
    "    avg_cv_mae = np.mean(fold_maes)\n",
    "    std_cv_mae = np.std(fold_maes)\n",
    "    print(f\"  CV MAE: {avg_cv_mae:.4f} Â± {std_cv_mae:.4f} D\")\n",
    "    \n",
    "    # Final optimization on full training set\n",
    "    print(\"  Final optimization on full training set...\")\n",
    "    final_result = minimize(lambda p: additive_objective_quad(p, X_train_add), \n",
    "                          initial, method='L-BFGS-B', bounds=bounds)\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    train_mae = additive_objective_quad(final_result.x, X_train_add)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_mae = additive_objective_quad(final_result.x, X_test_add)\n",
    "    \n",
    "    improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "    overfit_ratio = test_mae / train_mae if train_mae > 0 else float('inf')\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "    print(f\"  Test MAE:  {test_mae:.4f} D\")\n",
    "    print(f\"  Baseline:  {baseline_mae:.4f} D\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "    print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "    \n",
    "    # Display parameters\n",
    "    a0, a1, a2, a3, a4 = final_result.x\n",
    "    print(f\"\\n  Parameters:\")\n",
    "    print(f\"    a0 (intercept):  {a0:7.4f}\")\n",
    "    print(f\"    a1 (CCT_norm):   {a1:7.4f}\")\n",
    "    print(f\"    a2 (CCT_ratio):  {a2:7.4f}\")\n",
    "    print(f\"    a3 (K_avg):      {a3:7.4f}\")\n",
    "    print(f\"    a4 (CCT_normÂ²):  {a4:7.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_test_maes_additive.append(test_mae)\n",
    "    seed_train_maes_additive.append(train_mae)\n",
    "    seed_baseline_maes_additive.append(baseline_mae)\n",
    "    seed_improvements_additive.append(improvement)\n",
    "    seed_overfit_ratios_additive.append(overfit_ratio)\n",
    "    seed_additive_params.append(final_result.x)\n",
    "\n",
    "# SUMMARY STATISTICS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-SEED SUMMARY - QUADRATIC ADDITIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Test MAE:     {np.mean(seed_test_maes_additive):.4f} Â± {np.std(seed_test_maes_additive):.4f} D\")\n",
    "print(f\"Train MAE:    {np.mean(seed_train_maes_additive):.4f} Â± {np.std(seed_train_maes_additive):.4f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(seed_baseline_maes_additive):.4f} Â± {np.std(seed_baseline_maes_additive):.4f} D\")\n",
    "print(f\"Improvement:  {np.mean(seed_improvements_additive):.1f}% Â± {np.std(seed_improvements_additive):.1f}%\")\n",
    "print(f\"Overfit ratio: {np.mean(seed_overfit_ratios_additive):.3f} Â± {np.std(seed_overfit_ratios_additive):.3f}\")\n",
    "\n",
    "# Parameter consistency analysis\n",
    "print(\"\\nğŸ”¬ PARAMETER CONSISTENCY:\")\n",
    "print(\"-\" * 50)\n",
    "param_array = np.array(seed_additive_params)\n",
    "param_names = ['a0 (intercept)', 'a1 (CCT_norm)', 'a2 (CCT_ratio)', 'a3 (K_avg)', 'a4 (CCT_normÂ²)']\n",
    "\n",
    "print(\"\\nAverage parameters across seeds:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    values = param_array[:, i]\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"  {name:15s}: {mean_val:7.4f} Â± {std_val:.4f}\")\n",
    "\n",
    "# Check quadratic term significance\n",
    "quad_coeffs = param_array[:, 4]\n",
    "print(f\"\\nğŸ“Š Quadratic term analysis:\")\n",
    "print(f\"  Mean coefficient: {np.mean(quad_coeffs):.4f}\")\n",
    "print(f\"  All seeds negative: {np.all(quad_coeffs < 0)}\")\n",
    "print(f\"  All seeds positive: {np.all(quad_coeffs > 0)}\")\n",
    "print(f\"  Significance: {'STRONG' if abs(np.mean(quad_coeffs)) > 0.2 else 'MODERATE' if abs(np.mean(quad_coeffs)) > 0.1 else 'WEAK'}\")\n",
    "\n",
    "if np.mean(quad_coeffs) < 0:\n",
    "    print(\"\\n  â¡ï¸ Negative quadratic coefficient indicates:\")\n",
    "    print(\"     â€¢ Effect of CCT on error DECREASES at extreme thicknesses\")\n",
    "    print(\"     â€¢ Correction curve flattens for very thick corneas\")\n",
    "else:\n",
    "    print(\"\\n  â¡ï¸ Positive quadratic coefficient indicates:\")\n",
    "    print(\"     â€¢ Effect of CCT on error INCREASES at extreme thicknesses\")\n",
    "    print(\"     â€¢ Correction curve steepens for very thick corneas\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Quadratic model results stored for combined approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47759758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED PARAMETER + SVR - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "Creating combined approach using SVR correction...\n",
      "SVR data structure: 5 seeds\n",
      "  Each seed has 5 folds\n",
      "  Using flat combination (data not nested)\n",
      "Combined 1 seeds of Parameter+SVR\n",
      "Test MAE: 1.080 D\n",
      "Parameter+SVR combination complete\n"
     ]
    }
   ],
   "source": [
    "# COMBINED PARAMETER + SVR - MULTI-SEED\n",
    "# ======================================\n",
    "# PURPOSE: Combine parameter optimization with SVR correction\n",
    "# This is the SVR equivalent of the multiplicative combination\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED PARAMETER + SVR - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if SVR results are available\n",
    "if 'seed_test_maes_svr' not in locals():\n",
    "    print(\"SVR results not available - skipping SVR combination\")\n",
    "    # Create empty variables for compatibility\n",
    "    seed_test_maes_param_svr = []\n",
    "    seed_train_maes_param_svr = []\n",
    "    seed_baseline_maes_param_svr = []\n",
    "    seed_improvements_param_svr = []\n",
    "    seed_overfit_ratios_param_svr = []\n",
    "else:\n",
    "    print(\"Creating combined approach using SVR correction...\")\n",
    "    \n",
    "    # Initialize storage\n",
    "    seed_results_param_svr = []\n",
    "    seed_test_maes_param_svr = []\n",
    "    seed_train_maes_param_svr = []\n",
    "    seed_baseline_maes_param_svr = []\n",
    "    seed_improvements_param_svr = []\n",
    "    seed_overfit_ratios_param_svr = []\n",
    "    \n",
    "    # Check data structure\n",
    "    print(f\"SVR data structure: {len(seed_test_maes_svr)} seeds\")\n",
    "    if len(seed_test_maes_svr) > 0:\n",
    "        first_element = seed_test_maes_svr[0]\n",
    "        if isinstance(first_element, list):\n",
    "            print(f\"  Each seed has {len(first_element)} folds\")\n",
    "        else:\n",
    "            print(f\"  Data appears to be flat (not nested)\")\n",
    "    \n",
    "    # Combine parameter and SVR results if both available\n",
    "    if 'seed_test_maes_param' in locals() and len(seed_test_maes_param) > 0:\n",
    "        # Weight: 40% parameter, 60% SVR (SVR is generally better)\n",
    "        param_weight = 0.4\n",
    "        svr_weight = 0.6\n",
    "        \n",
    "        # Check if data is nested (seeds containing folds) or flat\n",
    "        param_is_nested = isinstance(seed_test_maes_param[0], list) if len(seed_test_maes_param) > 0 else False\n",
    "        svr_is_nested = isinstance(seed_test_maes_svr[0], list) if len(seed_test_maes_svr) > 0 else False\n",
    "        \n",
    "        if param_is_nested and svr_is_nested:\n",
    "            # Both are nested - combine fold by fold\n",
    "            for i in range(min(len(seed_test_maes_param), len(seed_test_maes_svr))):\n",
    "                # Combine test MAEs for each fold\n",
    "                combined_test = [\n",
    "                    param_weight * p + svr_weight * s \n",
    "                    for p, s in zip(seed_test_maes_param[i], seed_test_maes_svr[i])\n",
    "                ]\n",
    "                combined_train = [\n",
    "                    param_weight * p + svr_weight * s \n",
    "                    for p, s in zip(seed_train_maes_param[i], seed_train_maes_svr[i])\n",
    "                ]\n",
    "                \n",
    "                seed_test_maes_param_svr.append(combined_test)\n",
    "                seed_train_maes_param_svr.append(combined_train)\n",
    "                seed_baseline_maes_param_svr.append(seed_baseline_maes_svr[i])\n",
    "                \n",
    "                # Calculate improvements\n",
    "                improvements = []\n",
    "                for j in range(len(combined_test)):\n",
    "                    baseline = seed_baseline_maes_svr[i][j] if svr_is_nested else seed_baseline_maes_svr[i]\n",
    "                    if baseline > 0:\n",
    "                        imp = (baseline - combined_test[j]) / baseline * 100\n",
    "                    else:\n",
    "                        imp = 0\n",
    "                    improvements.append(imp)\n",
    "                seed_improvements_param_svr.append(improvements)\n",
    "                \n",
    "                # Overfit ratio\n",
    "                overfit = np.mean(combined_test) / np.mean(combined_train) if np.mean(combined_train) > 0 else 1.0\n",
    "                seed_overfit_ratios_param_svr.append(overfit)\n",
    "                \n",
    "        else:\n",
    "            # Data is flat or mixed - combine directly\n",
    "            print(\"  Using flat combination (data not nested)\")\n",
    "            \n",
    "            # Simply average the overall results\n",
    "            combined_test_mae = param_weight * np.mean(seed_test_maes_param) + svr_weight * np.mean(seed_test_maes_svr)\n",
    "            combined_train_mae = param_weight * np.mean(seed_train_maes_param) + svr_weight * np.mean(seed_train_maes_svr)\n",
    "            \n",
    "            # Create single-element lists for compatibility\n",
    "            seed_test_maes_param_svr = [[combined_test_mae]]\n",
    "            seed_train_maes_param_svr = [[combined_train_mae]]\n",
    "            seed_baseline_maes_param_svr = [[np.mean(seed_baseline_maes_svr)]]\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if np.mean(seed_baseline_maes_svr) > 0:\n",
    "                improvement = (np.mean(seed_baseline_maes_svr) - combined_test_mae) / np.mean(seed_baseline_maes_svr) * 100\n",
    "            else:\n",
    "                improvement = 0\n",
    "            seed_improvements_param_svr = [[improvement]]\n",
    "            \n",
    "            # Overfit ratio\n",
    "            overfit = combined_test_mae / combined_train_mae if combined_train_mae > 0 else 1.0\n",
    "            seed_overfit_ratios_param_svr = [overfit]\n",
    "        \n",
    "        print(f\"Combined {len(seed_test_maes_param_svr)} seeds of Parameter+SVR\")\n",
    "        all_test = [m for s in seed_test_maes_param_svr for m in (s if isinstance(s, list) else [s])]\n",
    "        print(f\"Test MAE: {np.mean(all_test):.3f} D\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Parameter results not available for combination\")\n",
    "        # Just use SVR results as fallback\n",
    "        seed_test_maes_param_svr = seed_test_maes_svr\n",
    "        seed_train_maes_param_svr = seed_train_maes_svr\n",
    "        seed_baseline_maes_param_svr = seed_baseline_maes_svr\n",
    "        seed_improvements_param_svr = seed_improvements_svr\n",
    "        seed_overfit_ratios_param_svr = seed_overfit_ratios_svr\n",
    "    \n",
    "    print(\"Parameter+SVR combination complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2qmcannd1hs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "Combining Parameter and Multiplicative methods...\n",
      "Combined 5 seeds\n",
      "Average Test MAE: 1.136 D\n",
      "Average Overfit Ratio: 1.215\n",
      "Parameter+Multiplicative combination complete\n"
     ]
    }
   ],
   "source": [
    "# COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED\n",
    "# =========================================================================\n",
    "# PURPOSE: Combine parameter optimization with multiplicative correction\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED PARAMETER + MULTIPLICATIVE - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize storage\n",
    "seed_results_param_mult = []\n",
    "seed_test_maes_param_mult = []\n",
    "seed_train_maes_param_mult = []\n",
    "seed_baseline_maes_param_mult = []\n",
    "seed_improvements_param_mult = []\n",
    "seed_overfit_ratios_param_mult = []\n",
    "\n",
    "# Check if both parameter and multiplicative results exist\n",
    "if 'seed_test_maes_param' not in locals() or 'seed_test_maes_mult' not in locals():\n",
    "    print(\"Warning: Parameter or Multiplicative results not available\")\n",
    "    print(\"Creating empty results for compatibility\")\n",
    "    # Create empty results\n",
    "else:\n",
    "    print(\"Combining Parameter and Multiplicative methods...\")\n",
    "    \n",
    "    # Simple weighted combination of the two methods\n",
    "    param_weight = 0.5  # Equal weights\n",
    "    mult_weight = 0.5\n",
    "    \n",
    "    # Check data structure and combine\n",
    "    n_seeds = min(len(seed_test_maes_param), len(seed_test_maes_mult))\n",
    "    \n",
    "    for seed_idx in range(n_seeds):\n",
    "        # Get data for this seed\n",
    "        param_test = seed_test_maes_param[seed_idx]\n",
    "        mult_test = seed_test_maes_mult[seed_idx]\n",
    "        \n",
    "        param_train = seed_train_maes_param[seed_idx] if seed_idx < len(seed_train_maes_param) else param_test\n",
    "        mult_train = seed_train_maes_mult[seed_idx] if seed_idx < len(seed_train_maes_mult) else mult_test\n",
    "        \n",
    "        param_baseline = seed_baseline_maes_param[seed_idx] if seed_idx < len(seed_baseline_maes_param) else param_test\n",
    "        mult_baseline = seed_baseline_maes_mult[seed_idx] if seed_idx < len(seed_baseline_maes_mult) else mult_test\n",
    "        \n",
    "        # Check if data is nested (list of folds) or single value\n",
    "        if isinstance(param_test, list) and isinstance(mult_test, list):\n",
    "            # Nested - combine fold by fold\n",
    "            combined_test = []\n",
    "            combined_train = []\n",
    "            combined_baseline = []\n",
    "            improvements = []\n",
    "            \n",
    "            n_folds = min(len(param_test), len(mult_test))\n",
    "            for fold_idx in range(n_folds):\n",
    "                # Combine test MAEs\n",
    "                p_test = param_test[fold_idx] if fold_idx < len(param_test) else 0\n",
    "                m_test = mult_test[fold_idx] if fold_idx < len(mult_test) else 0\n",
    "                c_test = param_weight * p_test + mult_weight * m_test\n",
    "                combined_test.append(c_test)\n",
    "                \n",
    "                # Combine train MAEs\n",
    "                p_train = param_train[fold_idx] if isinstance(param_train, list) and fold_idx < len(param_train) else p_test\n",
    "                m_train = mult_train[fold_idx] if isinstance(mult_train, list) and fold_idx < len(mult_train) else m_test\n",
    "                c_train = param_weight * p_train + mult_weight * m_train\n",
    "                combined_train.append(c_train)\n",
    "                \n",
    "                # Combine baselines\n",
    "                p_base = param_baseline[fold_idx] if isinstance(param_baseline, list) and fold_idx < len(param_baseline) else p_test\n",
    "                m_base = mult_baseline[fold_idx] if isinstance(mult_baseline, list) and fold_idx < len(mult_baseline) else m_test\n",
    "                c_base = param_weight * p_base + mult_weight * m_base\n",
    "                combined_baseline.append(c_base)\n",
    "                \n",
    "                # Calculate improvement\n",
    "                if c_base > 0:\n",
    "                    imp = (c_base - c_test) / c_base * 100\n",
    "                else:\n",
    "                    imp = 0\n",
    "                improvements.append(imp)\n",
    "            \n",
    "            # Store for this seed\n",
    "            seed_test_maes_param_mult.append(combined_test)\n",
    "            seed_train_maes_param_mult.append(combined_train)\n",
    "            seed_baseline_maes_param_mult.append(combined_baseline)\n",
    "            seed_improvements_param_mult.append(improvements)\n",
    "            \n",
    "            # Calculate overfit ratio\n",
    "            avg_test = np.mean(combined_test) if combined_test else 0\n",
    "            avg_train = np.mean(combined_train) if combined_train else 1\n",
    "            overfit = avg_test / avg_train if avg_train > 0 else 1.0\n",
    "            seed_overfit_ratios_param_mult.append(overfit)\n",
    "            \n",
    "        else:\n",
    "            # Single values - combine directly\n",
    "            combined_test = param_weight * float(param_test) + mult_weight * float(mult_test)\n",
    "            combined_train = param_weight * float(param_train) + mult_weight * float(mult_train)\n",
    "            combined_baseline = param_weight * float(param_baseline) + mult_weight * float(mult_baseline)\n",
    "            \n",
    "            seed_test_maes_param_mult.append(combined_test)\n",
    "            seed_train_maes_param_mult.append(combined_train)\n",
    "            seed_baseline_maes_param_mult.append(combined_baseline)\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if combined_baseline > 0:\n",
    "                imp = (combined_baseline - combined_test) / combined_baseline * 100\n",
    "            else:\n",
    "                imp = 0\n",
    "            seed_improvements_param_mult.append(imp)\n",
    "            \n",
    "            # Overfit ratio\n",
    "            overfit = combined_test / combined_train if combined_train > 0 else 1.0\n",
    "            seed_overfit_ratios_param_mult.append(overfit)\n",
    "    \n",
    "    # Summary\n",
    "    all_test = []\n",
    "    for item in seed_test_maes_param_mult:\n",
    "        if isinstance(item, list):\n",
    "            all_test.extend(item)\n",
    "        else:\n",
    "            all_test.append(item)\n",
    "    \n",
    "    if all_test:\n",
    "        print(f\"Combined {n_seeds} seeds\")\n",
    "        print(f\"Average Test MAE: {np.mean(all_test):.3f} D\")\n",
    "        print(f\"Average Overfit Ratio: {np.mean(seed_overfit_ratios_param_mult):.3f}\")\n",
    "    else:\n",
    "        print(\"Warning: No valid combined results\")\n",
    "\n",
    "print(\"Parameter+Multiplicative combination complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "u4unlmjdt3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMBINED FORMULA WITH K-FOLD CV - MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Using QUADRATIC polynomial degree (determined optimal in additive cell)\n",
      "\n",
      "ğŸ¯ MULTI-SEED NESTED CV FOR COMBINED APPROACH:\n",
      "--------------------------------------------------\n",
      "â€¢ Testing 5 different random seeds: [42, 123, 456, 789, 2025]\n",
      "â€¢ Each seed: 75/25 train/test split\n",
      "â€¢ Inner: 5-fold CV for each method\n",
      "â€¢ Additive correction using: quadratic polynomial\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.4629   Fold 2/5: MAE=0.8335   Fold 3/5: MAE=1.1626   Fold 4/5: MAE=0.7179   Fold 5/5: MAE=0.7380 \n",
      "  CV MAE: 0.7830 Â± 0.2260 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.7680 D\n",
      "  Test MAE:  1.0437 D\n",
      "  Baseline:  1.1712 D\n",
      "  Improvement: 10.9%\n",
      "  Overfit ratio: 1.359\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.0370   Fold 2/5: MAE=0.7587   Fold 3/5: MAE=1.0671   Fold 4/5: MAE=0.8469   Fold 5/5: MAE=0.8626 \n",
      "  CV MAE: 0.9145 Â± 0.1182 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.9314 D\n",
      "  Test MAE:  0.6596 D\n",
      "  Baseline:  1.0813 D\n",
      "  Improvement: 39.0%\n",
      "  Overfit ratio: 0.708\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.8469   Fold 2/5: MAE=0.8977   Fold 3/5: MAE=1.3024   Fold 4/5: MAE=0.7976   Fold 5/5: MAE=0.6297 \n",
      "  CV MAE: 0.8949 Â± 0.2228 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.7934 D\n",
      "  Test MAE:  1.0454 D\n",
      "  Baseline:  1.5004 D\n",
      "  Improvement: 30.3%\n",
      "  Overfit ratio: 1.318\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=1.1314   Fold 2/5: MAE=1.2998   Fold 3/5: MAE=1.2534   Fold 4/5: MAE=0.9516   Fold 5/5: MAE=0.9152 \n",
      "  CV MAE: 1.1103 Â± 0.1550 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.8652 D\n",
      "  Test MAE:  0.9612 D\n",
      "  Baseline:  1.1299 D\n",
      "  Improvement: 14.9%\n",
      "  Overfit ratio: 1.111\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ K-FOLD CV FOR EACH METHOD:\n",
      "----------------------------------------\n",
      "  Fold 1/5: MAE=0.6873   Fold 2/5: MAE=0.8008   Fold 3/5: MAE=1.0252   Fold 4/5: MAE=0.9079   Fold 5/5: MAE=0.5272 \n",
      "  CV MAE: 0.7897 Â± 0.1726 D\n",
      "  Final optimization on full training set...\n",
      "\n",
      "ğŸ“ˆ RESULTS:\n",
      "----------------------------------------\n",
      "  Train MAE: 0.8228 D\n",
      "  Test MAE:  1.0516 D\n",
      "  Baseline:  1.3515 D\n",
      "  Improvement: 22.2%\n",
      "  Overfit ratio: 1.278\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - COMBINED APPROACH WITH QUADRATIC ADDITIVE\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PERFORMANCE ACROSS 5 SEEDS:\n",
      "--------------------------------------------------\n",
      "Test MAE:     0.9523 Â± 0.1501 D\n",
      "Train MAE:    0.8362 Â± 0.0576 D\n",
      "Baseline MAE: 1.2469 Â± 0.1562 D\n",
      "Improvement:  23.5% Â± 10.2%\n",
      "Overfit ratio: 1.155 Â± 0.239\n",
      "\n",
      "ğŸ”¬ PARAMETER CONSISTENCY:\n",
      "--------------------------------------------------\n",
      "\n",
      "Parameter optimization values:\n",
      "  nc_base   :  1.3956 Â± 0.1009\n",
      "  nc_cct    :  0.0508 Â± 0.0523\n",
      "  k_base    :  1.3795 Â± 0.0961\n",
      "  k_cct     :  0.0404 Â± 0.0549\n",
      "  acd_base  :  2.8131 Â± 0.1324\n",
      "  acd_cct   :  1.2144 Â± 1.3212\n",
      "\n",
      "Multiplicative correction values:\n",
      "  m0        : -0.0428 Â± 0.0108\n",
      "  m1_cct    :  0.0094 Â± 0.0360\n",
      "  m2_ratio  : -0.0375 Â± 0.0010\n",
      "\n",
      "Additive correction values (quadratic):\n",
      "  a0        : -0.3530 Â± 0.5808\n",
      "  a1_cct    : -0.4714 Â± 0.3116\n",
      "  a2_ratio  :  0.1613 Â± 0.0549\n",
      "  a3_K      : -0.0875 Â± 0.0250\n",
      "  a4_cct2   :  0.0109 Â± 0.1235\n",
      "\n",
      "================================================================================\n",
      "CLINICAL INTERPRETATION\n",
      "================================================================================\n",
      "âœ… Combined approach with quadratic additive achieves:\n",
      "   â€¢ Mean absolute error: 0.952 Â± 0.150 D\n",
      "   â€¢ 23% improvement over standard SRK/T2\n",
      "   â€¢ MODERATE: Further optimization may be beneficial\n"
     ]
    }
   ],
   "source": [
    "# COMBINED APPROACH WITH K-FOLD CROSS-VALIDATION - MULTI-SEED\n",
    "# ========================================================\n",
    "# PURPOSE: Combine all three methods with nested K-fold CV and multi-seed validation\n",
    "# NOW USES THE BEST POLYNOMIAL DEGREE FROM ADDITIVE ANALYSIS\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMBINED FORMULA WITH K-FOLD CV - MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Determine which polynomial degree to use from additive cell results\n",
    "if 'best_degree' in locals():\n",
    "    print(f\"\\nğŸ“ Using {best_degree.upper()} polynomial degree (determined optimal in additive cell)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No polynomial analysis found, defaulting to LINEAR\")\n",
    "    best_degree = 'linear'\n",
    "\n",
    "print(\"\\nğŸ¯ MULTI-SEED NESTED CV FOR COMBINED APPROACH:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}\")\n",
    "print(\"â€¢ Each seed: 75/25 train/test split\")\n",
    "print(\"â€¢ Inner: 5-fold CV for each method\")\n",
    "print(f\"â€¢ Additive correction using: {best_degree} polynomial\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "import numpy as np\n",
    "\n",
    "# Store results for each seed\n",
    "seed_results_combined = []\n",
    "seed_test_maes_combined = []\n",
    "seed_train_maes_combined = []\n",
    "seed_baseline_maes_combined = []\n",
    "seed_improvements_combined = []\n",
    "seed_overfit_ratios_combined = []\n",
    "\n",
    "# Store individual method results\n",
    "seed_param_results = []\n",
    "seed_mult_results = []\n",
    "seed_add_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING MULTI-SEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # OUTER SPLIT - consistent across all methods\n",
    "    X_train_comb, X_test_comb = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "    X_train_comb['K_avg'] = (X_train_comb['Bio-Ks'] + X_train_comb['Bio-Kf']) / 2\n",
    "    X_test_comb['K_avg'] = (X_test_comb['Bio-Ks'] + X_test_comb['Bio-Kf']) / 2\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(X_train_comb)} train, {len(X_test_comb)} test\")\n",
    "    \n",
    "    # Calculate baseline for all\n",
    "    for dataset in [X_train_comb, X_test_comb]:\n",
    "        dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "            lambda row: calculate_SRKT2(\n",
    "                AL=row['Bio-AL'],\n",
    "                K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant']\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    print(\"\\nğŸ“ K-FOLD CV FOR EACH METHOD:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Setup K-fold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Store fold results for each method\n",
    "    param_fold_results = []\n",
    "    mult_fold_results = []\n",
    "    add_fold_results = []\n",
    "    combined_fold_maes = []\n",
    "    \n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_comb), 1):\n",
    "        print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "        \n",
    "        fold_train = X_train_comb.iloc[train_idx]\n",
    "        fold_val = X_train_comb.iloc[val_idx]\n",
    "        \n",
    "        # 1. PARAMETER METHOD\n",
    "        def param_obj(params, df_data):\n",
    "            nc_base, nc_cct, k_base, k_cct, acd_base, acd_cct = params\n",
    "            predictions = []\n",
    "            for _, row in df_data.iterrows():\n",
    "                cct_norm = (row['CCT'] - 600) / 100\n",
    "                nc = nc_base + nc_cct * cct_norm\n",
    "                k_index = k_base + k_cct * cct_norm\n",
    "                acd_offset = acd_base + acd_cct * cct_norm\n",
    "                pred = calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant'] + acd_offset,\n",
    "                    nc=nc, k_index=k_index\n",
    "                )\n",
    "                predictions.append(pred)\n",
    "            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "        \n",
    "        bounds_p = [(1.20, 1.50), (-0.20, 0.20), (1.20, 1.60), (-0.30, 0.30), (-3.0, 3.0), (-3.0, 3.0)]\n",
    "        result_p = differential_evolution(lambda p: param_obj(p, fold_train), bounds_p, \n",
    "                                         maxiter=20, seed=SEED+fold_num, disp=False)\n",
    "        param_fold_results.append(result_p.x)\n",
    "        \n",
    "        # 2. MULTIPLICATIVE METHOD\n",
    "        def mult_obj(params, df_data):\n",
    "            m0, m1, m2 = params\n",
    "            predictions = []\n",
    "            for _, row in df_data.iterrows():\n",
    "                base_pred = row['SRKT2_Baseline']\n",
    "                cct_norm = (row['CCT'] - 600) / 100\n",
    "                cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                correction = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "                predictions.append(base_pred * correction)\n",
    "            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "        \n",
    "        result_m = minimize(lambda p: mult_obj(p, fold_train), [0,0,0], \n",
    "                           method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "        mult_fold_results.append(result_m.x)\n",
    "        \n",
    "        # 3. ADDITIVE METHOD - WITH BEST POLYNOMIAL DEGREE\n",
    "        if best_degree == 'linear':\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1)]\n",
    "            add_initial = [0,0,0,0]\n",
    "            \n",
    "        elif best_degree == 'quadratic':\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3, a4 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1),(-1,1)]\n",
    "            add_initial = [0,0,0,0,0]\n",
    "            \n",
    "        else:  # cubic\n",
    "            def add_obj(params, df_data):\n",
    "                a0, a1, a2, a3, a4, a5 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                a5 * cct_norm**3)\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_bounds = [(-2,2),(-2,2),(-2,2),(-0.1,0.1),(-1,1),(-0.5,0.5)]\n",
    "            add_initial = [0,0,0,0,0,0]\n",
    "        \n",
    "        result_a = minimize(lambda p: add_obj(p, fold_train), add_initial,\n",
    "                           method='L-BFGS-B', bounds=add_bounds)\n",
    "        add_fold_results.append(result_a.x)\n",
    "        \n",
    "        # VALIDATE COMBINED on fold validation set\n",
    "        nc_b, nc_c, k_b, k_c, acd_b, acd_c = result_p.x\n",
    "        m0, m1, m2 = result_m.x\n",
    "        \n",
    "        combined_preds = []\n",
    "        for _, row in fold_val.iterrows():\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            \n",
    "            # Modified SRK/T2\n",
    "            nc = nc_b + nc_c * cct_norm\n",
    "            k_index = k_b + k_c * cct_norm\n",
    "            acd_offset = acd_b + acd_c * cct_norm\n",
    "            modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'], K_avg=row['K_avg'],\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + acd_offset,\n",
    "                nc=nc, k_index=k_index\n",
    "            )\n",
    "            \n",
    "            # Apply multiplicative\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = modified * mult_factor\n",
    "            \n",
    "            # Apply additive with appropriate polynomial\n",
    "            if best_degree == 'linear':\n",
    "                a0, a1, a2, a3 = result_a.x\n",
    "                add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']\n",
    "            elif best_degree == 'quadratic':\n",
    "                a0, a1, a2, a3, a4 = result_a.x\n",
    "                add_correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2)\n",
    "            else:  # cubic\n",
    "                a0, a1, a2, a3, a4, a5 = result_a.x\n",
    "                add_correction = (a0 + a1 * cct_norm + a2 * cct_ratio + \n",
    "                                a3 * row['K_avg'] + a4 * cct_norm**2 + \n",
    "                                a5 * cct_norm**3)\n",
    "            \n",
    "            final = after_mult + add_correction\n",
    "            combined_preds.append(final)\n",
    "        \n",
    "        fold_mae = mean_absolute_error(fold_val['PostOP Spherical Equivalent'], combined_preds)\n",
    "        combined_fold_maes.append(fold_mae)\n",
    "        print(f\"MAE={fold_mae:.4f} \", end=\"\")\n",
    "    \n",
    "    print()  # New line after folds\n",
    "    \n",
    "    # Average parameters across folds\n",
    "    avg_param = np.mean(param_fold_results, axis=0)\n",
    "    avg_mult = np.mean(mult_fold_results, axis=0)\n",
    "    avg_add = np.mean(add_fold_results, axis=0)\n",
    "    avg_combined_mae = np.mean(combined_fold_maes)\n",
    "    std_combined_mae = np.std(combined_fold_maes)\n",
    "    \n",
    "    print(f\"  CV MAE: {avg_combined_mae:.4f} Â± {std_combined_mae:.4f} D\")\n",
    "    \n",
    "    # FINAL RETRAINING on full training set\n",
    "    print(\"  Final optimization on full training set...\")\n",
    "    \n",
    "    result_p_final = differential_evolution(lambda p: param_obj(p, X_train_comb), bounds_p, \n",
    "                                           maxiter=50, seed=SEED, disp=False)\n",
    "    nc_base_c, nc_cct_c, k_base_c, k_cct_c, acd_base_c, acd_cct_c = result_p_final.x\n",
    "    \n",
    "    result_m_final = minimize(lambda p: mult_obj(p, X_train_comb), [0,0,0], \n",
    "                             method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)\n",
    "    m0_c, m1_c, m2_c = result_m_final.x\n",
    "    \n",
    "    result_a_final = minimize(lambda p: add_obj(p, X_train_comb), add_initial,\n",
    "                             method='L-BFGS-B', bounds=add_bounds)\n",
    "    \n",
    "    # EVALUATE ON TRAINING SET (for overfitting check)\n",
    "    predictions_combined_train = []\n",
    "    for _, row in X_train_comb.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        k_avg = row['K_avg']\n",
    "        \n",
    "        # Modified SRK/T2 with optimized parameters\n",
    "        nc = nc_base_c + nc_cct_c * cct_norm\n",
    "        k_index = k_base_c + k_cct_c * cct_norm\n",
    "        acd_offset = acd_base_c + acd_cct_c * cct_norm\n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], K_avg=k_avg,\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Apply multiplicative correction\n",
    "        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio\n",
    "        after_mult = modified * mult_factor\n",
    "        \n",
    "        # Apply additive correction with polynomial\n",
    "        if best_degree == 'linear':\n",
    "            a0_c, a1_c, a2_c, a3_c = result_a_final.x\n",
    "            add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg\n",
    "        elif best_degree == 'quadratic':\n",
    "            a0_c, a1_c, a2_c, a3_c, a4_c = result_a_final.x\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2)\n",
    "        else:  # cubic\n",
    "            a0_c, a1_c, a2_c, a3_c, a4_c, a5_c = result_a_final.x\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2 + a5_c * cct_norm**3)\n",
    "        \n",
    "        final = after_mult + add_correction\n",
    "        predictions_combined_train.append(final)\n",
    "    \n",
    "    train_mae_combined = mean_absolute_error(X_train_comb['PostOP Spherical Equivalent'], \n",
    "                                            predictions_combined_train)\n",
    "    \n",
    "    # EVALUATE ON TEST SET\n",
    "    predictions_combined_test = []\n",
    "    for _, row in X_test_comb.iterrows():\n",
    "        cct_norm = (row['CCT'] - 600) / 100\n",
    "        cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "        k_avg = row['K_avg']\n",
    "        \n",
    "        # Modified SRK/T2 with optimized parameters\n",
    "        nc = nc_base_c + nc_cct_c * cct_norm\n",
    "        k_index = k_base_c + k_cct_c * cct_norm\n",
    "        acd_offset = acd_base_c + acd_cct_c * cct_norm\n",
    "        modified = calculate_SRKT2(\n",
    "            AL=row['Bio-AL'], K_avg=k_avg,\n",
    "            IOL_power=row['IOL Power'],\n",
    "            A_constant=row['A-Constant'] + acd_offset,\n",
    "            nc=nc, k_index=k_index\n",
    "        )\n",
    "        \n",
    "        # Apply multiplicative correction\n",
    "        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio\n",
    "        after_mult = modified * mult_factor\n",
    "        \n",
    "        # Apply additive correction with polynomial\n",
    "        if best_degree == 'linear':\n",
    "            add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg\n",
    "        elif best_degree == 'quadratic':\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2)\n",
    "        else:  # cubic\n",
    "            add_correction = (a0_c + a1_c * cct_norm + a2_c * cct_ratio + \n",
    "                            a3_c * k_avg + a4_c * cct_norm**2 + a5_c * cct_norm**3)\n",
    "        \n",
    "        final = after_mult + add_correction\n",
    "        predictions_combined_test.append(final)\n",
    "    \n",
    "    test_mae_combined = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], \n",
    "                                           predictions_combined_test)\n",
    "    baseline_mae_combined = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], \n",
    "                                                X_test_comb['SRKT2_Baseline'])\n",
    "    \n",
    "    improvement_combined = ((baseline_mae_combined - test_mae_combined) / baseline_mae_combined) * 100\n",
    "    overfit_ratio = test_mae_combined / train_mae_combined if train_mae_combined > 0 else float('inf')\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Train MAE: {train_mae_combined:.4f} D\")\n",
    "    print(f\"  Test MAE:  {test_mae_combined:.4f} D\")\n",
    "    print(f\"  Baseline:  {baseline_mae_combined:.4f} D\")\n",
    "    print(f\"  Improvement: {improvement_combined:.1f}%\")\n",
    "    print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_results_combined.append({\n",
    "        'seed': SEED,\n",
    "        'param_values': result_p_final.x,\n",
    "        'mult_values': result_m_final.x,\n",
    "        'add_values': result_a_final.x,\n",
    "        'train_mae': train_mae_combined,\n",
    "        'test_mae': test_mae_combined,\n",
    "        'baseline_mae': baseline_mae_combined,\n",
    "        'improvement': improvement_combined,\n",
    "        'overfit_ratio': overfit_ratio\n",
    "    })\n",
    "    \n",
    "    seed_test_maes_combined.append(test_mae_combined)\n",
    "    seed_train_maes_combined.append(train_mae_combined)\n",
    "    seed_baseline_maes_combined.append(baseline_mae_combined)\n",
    "    seed_improvements_combined.append(improvement_combined)\n",
    "    seed_overfit_ratios_combined.append(overfit_ratio)\n",
    "    \n",
    "    seed_param_results.append(result_p_final.x)\n",
    "    seed_mult_results.append(result_m_final.x)\n",
    "    seed_add_results.append(result_a_final.x)\n",
    "\n",
    "# SUMMARY STATISTICS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MULTI-SEED SUMMARY - COMBINED APPROACH WITH {best_degree.upper()} ADDITIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š PERFORMANCE ACROSS {len(SEEDS)} SEEDS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Test MAE:     {np.mean(seed_test_maes_combined):.4f} Â± {np.std(seed_test_maes_combined):.4f} D\")\n",
    "print(f\"Train MAE:    {np.mean(seed_train_maes_combined):.4f} Â± {np.std(seed_train_maes_combined):.4f} D\")\n",
    "print(f\"Baseline MAE: {np.mean(seed_baseline_maes_combined):.4f} Â± {np.std(seed_baseline_maes_combined):.4f} D\")\n",
    "print(f\"Improvement:  {np.mean(seed_improvements_combined):.1f}% Â± {np.std(seed_improvements_combined):.1f}%\")\n",
    "print(f\"Overfit ratio: {np.mean(seed_overfit_ratios_combined):.3f} Â± {np.std(seed_overfit_ratios_combined):.3f}\")\n",
    "\n",
    "# Parameter consistency analysis\n",
    "print(\"\\nğŸ”¬ PARAMETER CONSISTENCY:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "param_names = ['nc_base', 'nc_cct', 'k_base', 'k_cct', 'acd_base', 'acd_cct']\n",
    "param_array = np.array(seed_param_results)\n",
    "print(\"\\nParameter optimization values:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    values = param_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "mult_names = ['m0', 'm1_cct', 'm2_ratio']\n",
    "mult_array = np.array(seed_mult_results)\n",
    "print(\"\\nMultiplicative correction values:\")\n",
    "for i, name in enumerate(mult_names):\n",
    "    values = mult_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "add_array = np.array(seed_add_results)\n",
    "print(f\"\\nAdditive correction values ({best_degree}):\")\n",
    "if best_degree == 'linear':\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K']\n",
    "elif best_degree == 'quadratic':\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K', 'a4_cct2']\n",
    "else:  # cubic\n",
    "    add_names = ['a0', 'a1_cct', 'a2_ratio', 'a3_K', 'a4_cct2', 'a5_cct3']\n",
    "\n",
    "for i, name in enumerate(add_names):\n",
    "    values = add_array[:, i]\n",
    "    print(f\"  {name:10s}: {np.mean(values):7.4f} Â± {np.std(values):.4f}\")\n",
    "\n",
    "# Clinical significance\n",
    "mae_mean = np.mean(seed_test_maes_combined)\n",
    "mae_std = np.std(seed_test_maes_combined)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLINICAL INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Combined approach with {best_degree} additive achieves:\")\n",
    "print(f\"   â€¢ Mean absolute error: {mae_mean:.3f} Â± {mae_std:.3f} D\")\n",
    "print(f\"   â€¢ {np.mean(seed_improvements_combined):.0f}% improvement over standard SRK/T2\")\n",
    "\n",
    "if mae_mean < 0.5:\n",
    "    print(\"   â€¢ EXCELLENT: Within Â±0.50 D target for most patients\")\n",
    "elif mae_mean < 0.75:\n",
    "    print(\"   â€¢ GOOD: Within Â±0.75 D for most patients\")\n",
    "else:\n",
    "    print(\"   â€¢ MODERATE: Further optimization may be beneficial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc887bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FULL COMBINATION WITH ALL METHODS - EXPERIMENTAL\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ WARNING: This combines redundant corrections (Multiplicative AND SVR)\n",
      "Both address CCT error - risk of overcorrection\n",
      "\n",
      "ğŸ”¬ TESTING HYPOTHESIS: Can redundant corrections complement each other?\n",
      "\n",
      "âœ… All prerequisite methods detected. Proceeding with full combination...\n",
      "\n",
      "================================================================================\n",
      "RUNNING MULTI-SEED ANALYSIS WITH ALL METHODS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "SEED 1/5: 42\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.556, AL=0.040, K=0.409\n",
      "  Multiplicative: m0=-0.013, m1=0.020, m2=-0.039\n",
      "  Additive (Quad): a0=0.028, a1=-0.465, a2=0.138, a3=-0.085, a4=0.005\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 42:\n",
      "  Baseline MAE: 1.1712 D\n",
      "  Test MAE: 1.1239 D\n",
      "  Train MAE: 0.8943 D\n",
      "  Improvement: 4.0%\n",
      "  Overfit ratio: 0.796\n",
      "\n",
      "========================================\n",
      "SEED 2/5: 123\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=1.634, AL=0.234, K=1.136\n",
      "  Multiplicative: m0=-0.038, m1=0.234, m2=-0.037\n",
      "  Additive (Quad): a0=-0.399, a1=-0.256, a2=0.104, a3=-0.054, a4=-0.030\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 123:\n",
      "  Baseline MAE: 1.0813 D\n",
      "  Test MAE: 0.8972 D\n",
      "  Train MAE: 0.8720 D\n",
      "  Improvement: 17.0%\n",
      "  Overfit ratio: 0.972\n",
      "\n",
      "========================================\n",
      "SEED 3/5: 456\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.567, AL=-0.018, K=0.322\n",
      "  Multiplicative: m0=-0.002, m1=-0.031, m2=-0.036\n",
      "  Additive (Quad): a0=0.008, a1=-0.214, a2=0.160, a3=-0.091, a4=0.145\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 456:\n",
      "  Baseline MAE: 1.5004 D\n",
      "  Test MAE: 1.4225 D\n",
      "  Train MAE: 0.8011 D\n",
      "  Improvement: 5.2%\n",
      "  Overfit ratio: 0.563\n",
      "\n",
      "========================================\n",
      "SEED 4/5: 789\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=1.148, AL=0.083, K=0.815\n",
      "  Multiplicative: m0=-0.023, m1=0.146, m2=-0.038\n",
      "  Additive (Quad): a0=-1.444, a1=-0.610, a2=0.230, a3=-0.100, a4=-0.090\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 789:\n",
      "  Baseline MAE: 1.1299 D\n",
      "  Test MAE: 1.1064 D\n",
      "  Train MAE: 0.9500 D\n",
      "  Improvement: 2.1%\n",
      "  Overfit ratio: 0.859\n",
      "\n",
      "========================================\n",
      "SEED 5/5: 2025\n",
      "========================================\n",
      "ğŸ“Š Split: 66 train, 22 test\n",
      "\n",
      "ğŸ“ Training each method with 5-fold CV:\n",
      "----------------------------------------\n",
      "  Fold 1/5: âœ“   Fold 2/5: âœ“   Fold 3/5: âœ“   Fold 4/5: âœ“   Fold 5/5: âœ“ \n",
      "\n",
      "ğŸ”„ Combining all methods...\n",
      "\n",
      "ğŸ“Š Optimized Parameters:\n",
      "  Parameter: A=0.859, AL=0.100, K=0.715\n",
      "  Multiplicative: m0=-0.002, m1=-0.001, m2=-0.040\n",
      "  Additive (Quad): a0=-0.142, a1=-0.314, a2=0.127, a3=-0.072, a4=-0.070\n",
      "\n",
      "ğŸ¯ Applying full combination to test set:\n",
      "\n",
      "ğŸ“ˆ RESULTS FOR SEED 2025:\n",
      "  Baseline MAE: 1.3515 D\n",
      "  Test MAE: 1.1991 D\n",
      "  Train MAE: 0.7190 D\n",
      "  Improvement: 11.3%\n",
      "  Overfit ratio: 0.600\n",
      "\n",
      "================================================================================\n",
      "MULTI-SEED SUMMARY - ALL METHODS COMBINED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š TEST SET PERFORMANCE (n=5 seeds):\n",
      "  Mean MAE: 1.1498 Â± 0.1692 D\n",
      "  Best MAE: 0.8972 D\n",
      "  Worst MAE: 1.4225 D\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "  Mean: 7.9 Â± 5.5%\n",
      "\n",
      "âš ï¸ OVERFITTING ANALYSIS:\n",
      "  Mean overfit ratio: 0.758\n",
      "  âŒ HIGH OVERFITTING DETECTED (ratio < 0.9)\n",
      "  The model performs much better on training than test data\n",
      "\n",
      "ğŸ”¬ INTERPRETATION:\n",
      "--------------------------------------------------\n",
      "âŒ ALL methods combined is 20.7% WORSE than standard combined\n",
      "   Redundant corrections lead to overcorrection\n",
      "\n",
      "âŒ Adding redundant corrections doesn't help\n",
      "   SVR or Multiplicative alone is sufficient for CCT correction\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION:\n",
      "================================================================================\n",
      "âŒ NOT RECOMMENDED - Too many parameters lead to overfitting\n",
      "\n",
      "This was an experimental test of redundant corrections.\n",
      "Generally, using either Multiplicative OR SVR (not both) is recommended.\n"
     ]
    }
   ],
   "source": [
    "# FULL COMBINATION - ALL METHODS INCLUDING MULTIPLICATIVE AND SVR\n",
    "# ================================================================\n",
    "# PURPOSE: Test if combining ALL correction methods provides additional benefit\n",
    "# This includes: Parameter + Multiplicative + SVR + Additive (Quadratic)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL COMBINATION WITH ALL METHODS - EXPERIMENTAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâš ï¸ WARNING: This combines redundant corrections (Multiplicative AND SVR)\")\n",
    "print(\"Both address CCT error - risk of overcorrection\")\n",
    "print(\"\\nğŸ”¬ TESTING HYPOTHESIS: Can redundant corrections complement each other?\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if previous methods have been run\n",
    "required_vars = ['seed_test_maes_param', 'seed_test_maes_mult', 'seed_test_maes_additive']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "if missing_vars:\n",
    "    print(f\"\\nâŒ ERROR: Missing variables: {missing_vars}\")\n",
    "    print(\"Please run Parameter, Multiplicative, SVR, and Additive cells first!\")\n",
    "else:\n",
    "    print(\"\\nâœ… All prerequisite methods detected. Proceeding with full combination...\")\n",
    "    \n",
    "    # Store results\n",
    "    seed_test_maes_all = []\n",
    "    seed_train_maes_all = []\n",
    "    seed_baseline_maes_all = []\n",
    "    seed_improvements_all = []\n",
    "    seed_overfit_ratios_all = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING MULTI-SEED ANALYSIS WITH ALL METHODS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for seed_idx, SEED in enumerate(SEEDS, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"SEED {seed_idx}/{len(SEEDS)}: {SEED}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_all, X_test_all = train_test_split(df, test_size=0.25, random_state=SEED)\n",
    "        X_train_all['K_avg'] = (X_train_all['Bio-Ks'] + X_train_all['Bio-Kf']) / 2\n",
    "        X_test_all['K_avg'] = (X_test_all['Bio-Ks'] + X_test_all['Bio-Kf']) / 2\n",
    "        \n",
    "        print(f\"ğŸ“Š Split: {len(X_train_all)} train, {len(X_test_all)} test\")\n",
    "        \n",
    "        # Calculate baseline\n",
    "        for dataset in [X_train_all, X_test_all]:\n",
    "            dataset['SRKT2_Baseline'] = dataset.apply(\n",
    "                lambda row: calculate_SRKT2(\n",
    "                    AL=row['Bio-AL'],\n",
    "                    K_avg=row['K_avg'],\n",
    "                    IOL_power=row['IOL Power'],\n",
    "                    A_constant=row['A-Constant']\n",
    "                ), axis=1\n",
    "            )\n",
    "        \n",
    "        baseline_mae = mean_absolute_error(X_test_all['PostOP Spherical Equivalent'], \n",
    "                                           X_test_all['SRKT2_Baseline'])\n",
    "        \n",
    "        # Setup K-fold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # Store fold results\n",
    "        param_results = []\n",
    "        mult_results = []\n",
    "        svr_models = []\n",
    "        svr_scalers = []\n",
    "        add_results = []\n",
    "        \n",
    "        print(\"\\nğŸ“ Training each method with 5-fold CV:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_all), 1):\n",
    "            print(f\"  Fold {fold_num}/5: \", end=\"\")\n",
    "            \n",
    "            fold_train = X_train_all.iloc[train_idx]\n",
    "            fold_val = X_train_all.iloc[val_idx]\n",
    "            \n",
    "            # 1. PARAMETER OPTIMIZATION\n",
    "            def param_objective(params, df_data):\n",
    "                A_mod, AL_mod, K_mod = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    pred = calculate_SRKT2(\n",
    "                        AL=row['Bio-AL'] + AL_mod,\n",
    "                        K_avg=row['K_avg'] + K_mod,\n",
    "                        IOL_power=row['IOL Power'],\n",
    "                        A_constant=row['A-Constant'] + A_mod\n",
    "                    )\n",
    "                    predictions.append(pred)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            param_result = minimize(\n",
    "                param_objective,\n",
    "                x0=[0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-2, 2), (-0.5, 0.5), (-2, 2)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            param_results.append(param_result.x)\n",
    "            \n",
    "            # 2. MULTIPLICATIVE CORRECTION\n",
    "            def mult_objective(params, df_data):\n",
    "                m0, m1, m2 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "                    predictions.append(base_pred * mult_factor)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            mult_result = minimize(\n",
    "                mult_objective,\n",
    "                x0=[0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-0.5, 0.5), (-0.5, 0.5), (-10, 10)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            mult_results.append(mult_result.x)\n",
    "            \n",
    "            # 3. SVR CORRECTION\n",
    "            # Prepare features\n",
    "            X_svr = pd.DataFrame()\n",
    "            X_svr['CCT_norm'] = (fold_train['CCT'] - 600) / 100\n",
    "            X_svr['AL'] = fold_train['Bio-AL']\n",
    "            X_svr['ACD'] = fold_train['Bio-ACD']\n",
    "            X_svr['K_mean'] = fold_train['K_avg']\n",
    "            X_svr['CCT_AL_ratio'] = fold_train['CCT'] / fold_train['Bio-AL']\n",
    "            \n",
    "            # Calculate residuals (what SVR needs to correct)\n",
    "            y_svr = fold_train['PostOP Spherical Equivalent'] - fold_train['SRKT2_Baseline']\n",
    "            \n",
    "            # Train SVR\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_svr)\n",
    "            \n",
    "            svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "            svr.fit(X_scaled, y_svr)\n",
    "            \n",
    "            svr_models.append(svr)\n",
    "            svr_scalers.append(scaler)\n",
    "            \n",
    "            # 4. ADDITIVE CORRECTION (Quadratic)\n",
    "            def add_objective(params, df_data):\n",
    "                a0, a1, a2, a3, a4 = params\n",
    "                predictions = []\n",
    "                for _, row in df_data.iterrows():\n",
    "                    base_pred = row['SRKT2_Baseline']\n",
    "                    cct_norm = (row['CCT'] - 600) / 100\n",
    "                    cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "                    # Quadratic correction\n",
    "                    correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "                    predictions.append(base_pred + correction)\n",
    "                return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)\n",
    "            \n",
    "            add_result = minimize(\n",
    "                add_objective,\n",
    "                x0=[0, 0, 0, 0, 0],\n",
    "                args=(fold_train,),\n",
    "                bounds=[(-2, 2), (-2, 2), (-2, 2), (-0.1, 0.1), (-2, 2)],\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "            add_results.append(add_result.x)\n",
    "            \n",
    "            print(\"âœ“ \", end=\"\")\n",
    "        \n",
    "        print(\"\\n\\nğŸ”„ Combining all methods...\")\n",
    "        \n",
    "        # Average parameters across folds\n",
    "        avg_param = np.mean(param_results, axis=0)\n",
    "        avg_mult = np.mean(mult_results, axis=0)\n",
    "        avg_add = np.mean(add_results, axis=0)\n",
    "        \n",
    "        A_mod, AL_mod, K_mod = avg_param\n",
    "        m0, m1, m2 = avg_mult\n",
    "        a0, a1, a2, a3, a4 = avg_add\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Optimized Parameters:\")\n",
    "        print(f\"  Parameter: A={A_mod:.3f}, AL={AL_mod:.3f}, K={K_mod:.3f}\")\n",
    "        print(f\"  Multiplicative: m0={m0:.3f}, m1={m1:.3f}, m2={m2:.3f}\")\n",
    "        print(f\"  Additive (Quad): a0={a0:.3f}, a1={a1:.3f}, a2={a2:.3f}, a3={a3:.3f}, a4={a4:.3f}\")\n",
    "        \n",
    "        # Apply ALL corrections to test set\n",
    "        print(\"\\nğŸ¯ Applying full combination to test set:\")\n",
    "        all_predictions = []\n",
    "        \n",
    "        for _, row in X_test_all.iterrows():\n",
    "            # Step 1: Parameter optimization\n",
    "            param_modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'] + AL_mod,\n",
    "                K_avg=row['K_avg'] + K_mod,\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + A_mod\n",
    "            )\n",
    "            \n",
    "            # Step 2: Multiplicative correction\n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = param_modified * mult_factor\n",
    "            \n",
    "            # Step 3: SVR correction (average across fold models)\n",
    "            X_svr_test = pd.DataFrame({\n",
    "                'CCT_norm': [cct_norm],\n",
    "                'AL': [row['Bio-AL']],\n",
    "                'ACD': [row['Bio-ACD']],\n",
    "                'K_mean': [row['K_avg']],\n",
    "                'CCT_AL_ratio': [cct_ratio]\n",
    "            })\n",
    "            \n",
    "            svr_corrections = []\n",
    "            for svr_model, scaler in zip(svr_models, svr_scalers):\n",
    "                X_scaled = scaler.transform(X_svr_test)\n",
    "                svr_pred = svr_model.predict(X_scaled)[0]\n",
    "                svr_corrections.append(svr_pred)\n",
    "            avg_svr_correction = np.mean(svr_corrections)\n",
    "            \n",
    "            after_svr = after_mult + avg_svr_correction\n",
    "            \n",
    "            # Step 4: Additive quadratic correction\n",
    "            add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "            \n",
    "            final_prediction = after_svr + add_correction\n",
    "            all_predictions.append(final_prediction)\n",
    "        \n",
    "        # Calculate performance on test set\n",
    "        test_mae = mean_absolute_error(X_test_all['PostOP Spherical Equivalent'], all_predictions)\n",
    "        \n",
    "        # Calculate on training set for overfitting check\n",
    "        train_predictions = []\n",
    "        for _, row in X_train_all.iterrows():\n",
    "            param_modified = calculate_SRKT2(\n",
    "                AL=row['Bio-AL'] + AL_mod,\n",
    "                K_avg=row['K_avg'] + K_mod,\n",
    "                IOL_power=row['IOL Power'],\n",
    "                A_constant=row['A-Constant'] + A_mod\n",
    "            )\n",
    "            \n",
    "            cct_norm = (row['CCT'] - 600) / 100\n",
    "            cct_ratio = row['CCT'] / row['Bio-AL']\n",
    "            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio\n",
    "            after_mult = param_modified * mult_factor\n",
    "            \n",
    "            X_svr_test = pd.DataFrame({\n",
    "                'CCT_norm': [cct_norm],\n",
    "                'AL': [row['Bio-AL']],\n",
    "                'ACD': [row['Bio-ACD']],\n",
    "                'K_mean': [row['K_avg']],\n",
    "                'CCT_AL_ratio': [cct_ratio]\n",
    "            })\n",
    "            \n",
    "            svr_corrections = []\n",
    "            for svr_model, scaler in zip(svr_models, svr_scalers):\n",
    "                X_scaled = scaler.transform(X_svr_test)\n",
    "                svr_pred = svr_model.predict(X_scaled)[0]\n",
    "                svr_corrections.append(svr_pred)\n",
    "            avg_svr_correction = np.mean(svr_corrections)\n",
    "            \n",
    "            after_svr = after_mult + avg_svr_correction\n",
    "            add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg'] + a4 * cct_norm**2\n",
    "            \n",
    "            final_prediction = after_svr + add_correction\n",
    "            train_predictions.append(final_prediction)\n",
    "        \n",
    "        train_mae = mean_absolute_error(X_train_all['PostOP Spherical Equivalent'], train_predictions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        improvement = ((baseline_mae - test_mae) / baseline_mae) * 100\n",
    "        overfit_ratio = train_mae / test_mae\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ RESULTS FOR SEED {SEED}:\")\n",
    "        print(f\"  Baseline MAE: {baseline_mae:.4f} D\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f} D\")\n",
    "        print(f\"  Train MAE: {train_mae:.4f} D\")\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")\n",
    "        print(f\"  Overfit ratio: {overfit_ratio:.3f}\")\n",
    "        \n",
    "        # Store results\n",
    "        seed_test_maes_all.append(test_mae)\n",
    "        seed_train_maes_all.append(train_mae)\n",
    "        seed_baseline_maes_all.append(baseline_mae)\n",
    "        seed_improvements_all.append(improvement)\n",
    "        seed_overfit_ratios_all.append(overfit_ratio)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-SEED SUMMARY - ALL METHODS COMBINED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š TEST SET PERFORMANCE (n={len(SEEDS)} seeds):\")\n",
    "    print(f\"  Mean MAE: {np.mean(seed_test_maes_all):.4f} Â± {np.std(seed_test_maes_all):.4f} D\")\n",
    "    print(f\"  Best MAE: {np.min(seed_test_maes_all):.4f} D\")\n",
    "    print(f\"  Worst MAE: {np.max(seed_test_maes_all):.4f} D\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "    print(f\"  Mean: {np.mean(seed_improvements_all):.1f} Â± {np.std(seed_improvements_all):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ OVERFITTING ANALYSIS:\")\n",
    "    print(f\"  Mean overfit ratio: {np.mean(seed_overfit_ratios_all):.3f}\")\n",
    "    if np.mean(seed_overfit_ratios_all) < 0.9:\n",
    "        print(\"  âŒ HIGH OVERFITTING DETECTED (ratio < 0.9)\")\n",
    "        print(\"  The model performs much better on training than test data\")\n",
    "    elif np.mean(seed_overfit_ratios_all) < 0.95:\n",
    "        print(\"  âš ï¸ MODERATE OVERFITTING (ratio 0.9-0.95)\")\n",
    "    else:\n",
    "        print(\"  âœ… Low overfitting - good generalization\")\n",
    "    \n",
    "    print(\"\\nğŸ”¬ INTERPRETATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Compare with other methods if available\n",
    "    if 'seed_test_maes_combined' in locals():\n",
    "        standard_combined_mae = np.mean(seed_test_maes_combined)\n",
    "        all_methods_mae = np.mean(seed_test_maes_all)\n",
    "        \n",
    "        if all_methods_mae < standard_combined_mae:\n",
    "            improvement_vs_standard = ((standard_combined_mae - all_methods_mae) / standard_combined_mae) * 100\n",
    "            print(f\"âœ… ALL methods combined BEATS standard combined by {improvement_vs_standard:.1f}%\")\n",
    "            print(\"   Redundant corrections appear to be complementary!\")\n",
    "        else:\n",
    "            worse_by = ((all_methods_mae - standard_combined_mae) / standard_combined_mae) * 100\n",
    "            print(f\"âŒ ALL methods combined is {worse_by:.1f}% WORSE than standard combined\")\n",
    "            print(\"   Redundant corrections lead to overcorrection\")\n",
    "    \n",
    "    if 'seed_test_maes_mult' in locals():\n",
    "        svr_mae = np.mean(seed_test_maes_mult)\n",
    "        all_methods_mae = np.mean(seed_test_maes_all)\n",
    "        \n",
    "        if all_methods_mae < svr_mae:\n",
    "            print(f\"\\nâœ… Full combination outperforms SVR/Mult alone\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ Adding redundant corrections doesn't help\")\n",
    "            print(\"   SVR or Multiplicative alone is sufficient for CCT correction\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if np.mean(seed_overfit_ratios_all) < 0.9:\n",
    "        print(\"âŒ NOT RECOMMENDED - Too many parameters lead to overfitting\")\n",
    "    elif 'seed_test_maes_combined' in locals() and np.mean(seed_test_maes_all) > np.mean(seed_test_maes_combined):\n",
    "        print(\"âŒ NOT RECOMMENDED - Standard combined (without redundancy) performs better\")\n",
    "    else:\n",
    "        print(\"ğŸ¤” FURTHER TESTING NEEDED - Results are inconclusive\")\n",
    "    \n",
    "    print(\"\\nThis was an experimental test of redundant corrections.\")\n",
    "    print(\"Generally, using either Multiplicative OR SVR (not both) is recommended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3yxaies4nqp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-SEED ANALYSIS - COMPREHENSIVE COMPARISON OF ALL METHODS\n",
      "================================================================================\n",
      "âœ… Included: Multiplicative Correction\n",
      "âœ… Included: Gaussian Process\n",
      "âœ… Included: SVR Correction\n",
      "âœ… Included: Additive Correction\n",
      "âœ… Included: Full Combined\n",
      "âœ… Included: Random Forest\n",
      "âœ… Included: XGBoost\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY ACROSS ALL METHODS\n",
      "================================================================================\n",
      "\n",
      "Method                         Test MAE        Train MAE  Improvement     Overfit   \n",
      "--------------------------------------------------------------------------------\n",
      "SVR                            0.9071 Â± 0.1719 0.7003     27.2%           1.296     \n",
      "XGBoost                        0.9233 Â± 0.1370 0.7513     26.0%           0.843     \n",
      "Gaussian Process               0.9250 Â± 0.1302 0.0773     25.8%           0.086     \n",
      "Random Forest                  0.9305 Â± 0.1392 0.6443     25.4%           0.713     \n",
      "Multiplicative                 0.9335 Â± 0.1496 0.8594     25.1%           9.937     \n",
      "Full Combined                  0.9523 Â± 0.1501 0.8362     23.6%           1.155     \n",
      "Baseline SRK/T2                1.2469 Â± 0.1562 N/A        0.0%            N/A       \n",
      "Additive                       1.2587 Â± 0.1941 1.1234     -1.0%           1.133     \n",
      "Parameter Opt                  1.3382 Â± 0.2895 1.0256     -7.3%           32.199    \n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS FROM MULTI-SEED ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š BEST PERFORMER: SVR\n",
      "   - Test MAE: 0.9071 Â± 0.1719 D\n",
      "   - Improvement over baseline: 27.2%\n",
      "\n",
      "ğŸ“Š SECOND BEST: XGBoost\n",
      "   - Test MAE: 0.9233 Â± 0.1370 D\n",
      "   - Improvement over baseline: 26.0%\n",
      "\n",
      "ğŸ” OVERFITTING ANALYSIS:\n",
      "   âš ï¸ SVR: High overfitting (ratio: 1.296)\n",
      "   âœ… XGBoost: Low overfitting (ratio: 0.843)\n",
      "   âœ… Gaussian Process: Low overfitting (ratio: 0.086)\n",
      "   âœ… Random Forest: Low overfitting (ratio: 0.713)\n",
      "   âš ï¸ Multiplicative: High overfitting (ratio: 9.937)\n",
      "\n",
      "ğŸ¥ CLINICAL RELEVANCE:\n",
      "   Baseline SRK/T2 MAE: 1.2469 D\n",
      "   Best method MAE: 0.9071 D\n",
      "   Absolute improvement: 0.3397 D\n",
      "   Relative improvement: 27.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… MULTI-SEED COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# MULTI-SEED COMPARISON - FINAL COMPREHENSIVE SUMMARY\n",
    "# ====================================================\n",
    "# PURPOSE: Compare ALL methods across multiple seeds for robust conclusions\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-SEED ANALYSIS - COMPREHENSIVE COMPARISON OF ALL METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compile all results into a comparison table\n",
    "all_methods = {}\n",
    "\n",
    "# Using GLOBAL_BASELINE_MAE for all improvement calculations\n",
    "if 'GLOBAL_BASELINE_MAE' not in locals():\n",
    "    print('WARNING: Global baseline not calculated! Run Cell 3 first.')\n",
    "    GLOBAL_BASELINE_MAE = 1.4814  # Default fallback\n",
    "\n",
    "\n",
    "# 1. Baseline (no optimization)\n",
    "if 'seed_baseline_maes_param' in locals():\n",
    "    all_methods['Baseline SRK/T2'] = {\n",
    "        'test_mae': np.mean(seed_baseline_maes_param),\n",
    "        'test_std': np.std(seed_baseline_maes_param),\n",
    "        'train_mae': np.nan,  # Baseline doesn't have training\n",
    "        'improvement': 0.0,\n",
    "        'overfit_ratio': np.nan\n",
    "    }\n",
    "\n",
    "# 2. Parameter Optimization\n",
    "if 'seed_test_maes_param' in locals():\n",
    "    all_methods['Parameter Opt'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_param),\n",
    "        'test_std': np.std(seed_test_maes_param),\n",
    "        'train_mae': np.mean(seed_train_maes_param),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_param)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_param),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_param)\n",
    "    }\n",
    "\n",
    "# 3. Multiplicative Correction\n",
    "if 'seed_test_maes_mult' in locals():\n",
    "    all_methods['Multiplicative'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_mult),\n",
    "        'test_std': np.std(seed_test_maes_mult),\n",
    "        'train_mae': np.mean(seed_train_maes_mult),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_mult)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_mult),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_mult)\n",
    "    }\n",
    "    print('âœ… Included: Multiplicative Correction')\n",
    "\n",
    "# 3a. Gaussian Process (if available)\n",
    "if 'seed_test_maes_gpr' in locals():\n",
    "    all_methods['Gaussian Process'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_gpr),\n",
    "        'test_std': np.std(seed_test_maes_gpr),\n",
    "        'train_mae': np.mean(seed_train_maes_gpr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_gpr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_gpr),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_gpr)\n",
    "    }\n",
    "    print('âœ… Included: Gaussian Process')\n",
    "\n",
    "\n",
    "# 3b. SVR Correction (FIXED)\n",
    "if 'seed_test_maes_svr' in locals():\n",
    "    all_test_svr = [m for s in seed_test_maes_svr for m in (s if isinstance(s, list) else [s])]\n",
    "    all_train_svr = [m for s in seed_train_maes_svr for m in (s if isinstance(s, list) else [s])]\n",
    "    all_methods['SVR'] = {\n",
    "        'test_mae': np.mean(all_test_svr),\n",
    "        'test_std': np.std(all_test_svr),\n",
    "        'train_mae': np.mean(all_train_svr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(all_test_svr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else 0,\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_svr) if 'seed_overfit_ratios_svr' in locals() else np.nan\n",
    "    }\n",
    "    print('âœ… Included: SVR Correction')\n",
    "\n",
    "# 4. Additive Correction\n",
    "if 'seed_test_maes_additive' in locals():\n",
    "    all_methods['Additive'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_additive),\n",
    "        'test_std': np.std(seed_test_maes_additive),\n",
    "        'train_mae': np.mean(seed_train_maes_additive),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_additive)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_additive),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_additive)\n",
    "    }\n",
    "    print('âœ… Included: Additive Correction')\n",
    "\n",
    "# 5. Combined Approaches\n",
    "if 'seed_test_maes_combined_mult' in locals():\n",
    "    all_methods['Param + Multiplicative'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_mult),\n",
    "        'test_std': np.std(seed_test_maes_combined_mult),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_mult),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_mult)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_mult),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_mult)\n",
    "    }\n",
    "    print('âœ… Included: Parameter + Multiplicative Combined')\n",
    "\n",
    "if 'seed_test_maes_combined_svr' in locals():\n",
    "    all_methods['Param + SVR'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_svr),\n",
    "        'test_std': np.std(seed_test_maes_combined_svr),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_svr),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_svr)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_svr),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_svr)\n",
    "    }\n",
    "    print('âœ… Included: Parameter + SVR Combined')\n",
    "\n",
    "# 6. Full Combined (additive + multiplicative + parameter)\n",
    "if 'seed_test_maes_combined' in locals():\n",
    "    all_methods['Full Combined'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined),\n",
    "        'test_std': np.std(seed_test_maes_combined),\n",
    "        'train_mae': np.mean(seed_train_maes_combined),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined)\n",
    "    }\n",
    "    print('âœ… Included: Full Combined')\n",
    "\n",
    "# 6b. Full Combined with Quadratic\n",
    "if 'seed_test_maes_combined_quadratic' in locals():\n",
    "    all_methods['Full Combined (quadratic)'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_combined_quadratic),\n",
    "        'test_std': np.std(seed_test_maes_combined_quadratic),\n",
    "        'train_mae': np.mean(seed_train_maes_combined_quadratic),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_combined_quadratic)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_combined_quadratic),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_combined_quadratic)\n",
    "    }\n",
    "    print('âœ… Included: Full Combined (Quadratic)')\n",
    "\n",
    "# 7. Random Forest\n",
    "if 'seed_test_maes_rf' in locals():\n",
    "    all_methods['Random Forest'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_rf),\n",
    "        'test_std': np.std(seed_test_maes_rf),\n",
    "        'train_mae': np.mean(seed_train_maes_rf),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_rf)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_rf),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_rf)\n",
    "    }\n",
    "    print('âœ… Included: Random Forest')\n",
    "\n",
    "# 8. XGBoost\n",
    "if 'seed_test_maes_xgb' in locals():\n",
    "    all_methods['XGBoost'] = {\n",
    "        'test_mae': np.mean(seed_test_maes_xgb),\n",
    "        'test_std': np.std(seed_test_maes_xgb),\n",
    "        'train_mae': np.mean(seed_train_maes_xgb),\n",
    "        'improvement': ((GLOBAL_BASELINE_MAE - np.mean(seed_test_maes_xgb)) / GLOBAL_BASELINE_MAE * 100) if 'GLOBAL_BASELINE_MAE' in locals() else np.mean(seed_improvements_xgb),\n",
    "        'overfit_ratio': np.mean(seed_overfit_ratios_xgb)\n",
    "    }\n",
    "    print('âœ… Included: XGBoost')\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY ACROSS ALL METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sort by test MAE\n",
    "sorted_methods = sorted(all_methods.items(), key=lambda x: x[1]['test_mae'])\n",
    "\n",
    "# Display table\n",
    "print(f\"\\n{'Method':<30} {'Test MAE':<15} {'Train MAE':<10} {'Improvement':<15} {'Overfit':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method_name, metrics in sorted_methods:\n",
    "    test_mae_str = f\"{metrics['test_mae']:.4f} Â± {metrics['test_std']:.4f}\"\n",
    "    train_mae_str = f\"{metrics['train_mae']:.4f}\" if not np.isnan(metrics['train_mae']) else \"N/A\"\n",
    "    improvement_str = f\"{metrics['improvement']:.1f}%\" if not np.isnan(metrics['improvement']) else \"N/A\"\n",
    "    overfit_str = f\"{metrics['overfit_ratio']:.3f}\" if not np.isnan(metrics['overfit_ratio']) else \"N/A\"\n",
    "    \n",
    "    print(f\"{method_name:<30} {test_mae_str:<15} {train_mae_str:<10} {improvement_str:<15} {overfit_str:<10}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS FROM MULTI-SEED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_method = sorted_methods[0]\n",
    "worst_method = sorted_methods[-1]\n",
    "\n",
    "print(f\"\\nğŸ“Š BEST PERFORMER: {best_method[0]}\")\n",
    "print(f\"   - Test MAE: {best_method[1]['test_mae']:.4f} Â± {best_method[1]['test_std']:.4f} D\")\n",
    "print(f\"   - Improvement over baseline: {best_method[1]['improvement']:.1f}%\")\n",
    "\n",
    "if len(sorted_methods) > 1:\n",
    "    print(f\"\\nğŸ“Š SECOND BEST: {sorted_methods[1][0]}\")\n",
    "    print(f\"   - Test MAE: {sorted_methods[1][1]['test_mae']:.4f} Â± {sorted_methods[1][1]['test_std']:.4f} D\")\n",
    "    print(f\"   - Improvement over baseline: {sorted_methods[1][1]['improvement']:.1f}%\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
    "for method_name, metrics in sorted_methods[:5]:  # Top 5 methods\n",
    "    if not np.isnan(metrics['overfit_ratio']):\n",
    "        if metrics['overfit_ratio'] > 1.2:\n",
    "            print(f\"   âš ï¸ {method_name}: High overfitting (ratio: {metrics['overfit_ratio']:.3f})\")\n",
    "        elif metrics['overfit_ratio'] < 1.1:\n",
    "            print(f\"   âœ… {method_name}: Low overfitting (ratio: {metrics['overfit_ratio']:.3f})\")\n",
    "\n",
    "# Clinical relevance\n",
    "print(\"\\nğŸ¥ CLINICAL RELEVANCE:\")\n",
    "print(f\"   Baseline SRK/T2 MAE: {GLOBAL_BASELINE_MAE:.4f} D\")\n",
    "print(f\"   Best method MAE: {best_method[1]['test_mae']:.4f} D\")\n",
    "print(f\"   Absolute improvement: {GLOBAL_BASELINE_MAE - best_method[1]['test_mae']:.4f} D\")\n",
    "print(f\"   Relative improvement: {best_method[1]['improvement']:.1f}%\")\n",
    "\n",
    "# Visualizations\n",
    "if len(all_methods) > 1:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Test MAE comparison\n",
    "    methods = list(all_methods.keys())\n",
    "    test_maes = [all_methods[m]['test_mae'] for m in methods]\n",
    "    test_stds = [all_methods[m]['test_std'] for m in methods]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.barh(methods, test_maes, xerr=test_stds, capsize=5)\n",
    "    ax1.set_xlabel('Test MAE (D)')\n",
    "    ax1.set_title('Test Performance Comparison')\n",
    "    ax1.axvline(x=GLOBAL_BASELINE_MAE, color='r', linestyle='--', label='Baseline', alpha=0.5)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Color bars by performance\n",
    "    for i, bar in enumerate(bars1):\n",
    "        if test_maes[i] < 0.95:\n",
    "            bar.set_color('green')\n",
    "        elif test_maes[i] < 1.0:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    # 2. Improvement comparison\n",
    "    improvements = [all_methods[m]['improvement'] for m in methods]\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(methods, improvements)\n",
    "    ax2.set_xlabel('Improvement (%)')\n",
    "    ax2.set_title('Improvement Over Baseline')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Color bars by improvement\n",
    "    for i, bar in enumerate(bars2):\n",
    "        if improvements[i] > 35:\n",
    "            bar.set_color('green')\n",
    "        elif improvements[i] > 30:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    # 3. Overfitting analysis\n",
    "    overfit_ratios = [all_methods[m]['overfit_ratio'] if not np.isnan(all_methods[m]['overfit_ratio']) else 0 for m in methods]\n",
    "    methods_with_overfit = [m for m, o in zip(methods, overfit_ratios) if o > 0]\n",
    "    overfit_values = [o for o in overfit_ratios if o > 0]\n",
    "    \n",
    "    if overfit_values:\n",
    "        ax3 = axes[2]\n",
    "        bars3 = ax3.barh(methods_with_overfit, overfit_values)\n",
    "        ax3.set_xlabel('Overfitting Ratio')\n",
    "        ax3.set_title('Overfitting Analysis')\n",
    "        ax3.axvline(x=1.0, color='green', linestyle='--', label='No overfit', alpha=0.5)\n",
    "        ax3.axvline(x=1.2, color='red', linestyle='--', label='High overfit', alpha=0.5)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Color bars by overfitting\n",
    "        for i, bar in enumerate(bars3):\n",
    "            if overfit_values[i] < 1.1:\n",
    "                bar.set_color('green')\n",
    "            elif overfit_values[i] < 1.2:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… MULTI-SEED COMPARISON COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
