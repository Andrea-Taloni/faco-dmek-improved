# COMBINED APPROACH WITH K-FOLD CROSS-VALIDATION - MULTI-SEED
# ========================================================
# PURPOSE: Combine all three methods with nested K-fold CV and multi-seed validation
# Most complex but potentially most accurate approach

print("=" * 80)
print("COMBINED FORMULA WITH K-FOLD CV - MULTI-SEED ANALYSIS")
print("=" * 80)

print("\nğŸ¯ MULTI-SEED NESTED CV FOR COMBINED APPROACH:")
print("-" * 50)
print(f"â€¢ Testing {len(SEEDS)} different random seeds: {SEEDS}")
print("â€¢ Each seed: 75/25 train/test split")
print("â€¢ Inner: 5-fold CV for each method")
print("â€¢ Combine all optimized corrections")

from sklearn.model_selection import train_test_split, KFold
from scipy.optimize import minimize, differential_evolution
import numpy as np

# Store results for each seed
seed_results_combined = []
seed_test_maes_combined = []
seed_train_maes_combined = []
seed_baseline_maes_combined = []
seed_improvements_combined = []
seed_overfit_ratios_combined = []

# Store individual method results
seed_param_results = []
seed_mult_results = []
seed_add_results = []

print("\n" + "="*80)
print("RUNNING MULTI-SEED ANALYSIS")
print("="*80)

for seed_idx, SEED in enumerate(SEEDS, 1):
    print(f"\n{'='*40}")
    print(f"SEED {seed_idx}/{len(SEEDS)}: {SEED}")
    print(f"{'='*40}")
    
    # OUTER SPLIT - consistent across all methods
    X_train_comb, X_test_comb = train_test_split(df, test_size=0.25, random_state=SEED)
    X_train_comb['K_avg'] = (X_train_comb['Bio-Ks'] + X_train_comb['Bio-Kf']) / 2
    X_test_comb['K_avg'] = (X_test_comb['Bio-Ks'] + X_test_comb['Bio-Kf']) / 2
    
    print(f"ğŸ“Š Split: {len(X_train_comb)} train, {len(X_test_comb)} test")
    
    # Calculate baseline for all
    for dataset in [X_train_comb, X_test_comb]:
        dataset['SRKT2_Baseline'] = dataset.apply(
            lambda row: calculate_SRKT2(
                AL=row['Bio-AL'],
                K_avg=row['K_avg'],
                IOL_power=row['IOL Power'],
                A_constant=row['A-Constant']
            ), axis=1
        )
    
    print("\nğŸ“ K-FOLD CV FOR EACH METHOD:")
    print("-" * 40)
    
    # Setup K-fold
    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
    
    # Store fold results for each method
    param_fold_results = []
    mult_fold_results = []
    add_fold_results = []
    combined_fold_maes = []
    
    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train_comb), 1):
        print(f"  Fold {fold_num}/5: ", end="")
        
        fold_train = X_train_comb.iloc[train_idx]
        fold_val = X_train_comb.iloc[val_idx]
        
        # 1. PARAMETER METHOD
        def param_obj(params, df_data):
            nc_base, nc_cct, k_base, k_cct, acd_base, acd_cct = params
            predictions = []
            for _, row in df_data.iterrows():
                cct_norm = (row['CCT'] - 600) / 100
                nc = nc_base + nc_cct * cct_norm
                k_index = k_base + k_cct * cct_norm
                acd_offset = acd_base + acd_cct * cct_norm
                pred = calculate_SRKT2(
                    AL=row['Bio-AL'], K_avg=row['K_avg'],
                    IOL_power=row['IOL Power'],
                    A_constant=row['A-Constant'] + acd_offset,
                    nc=nc, k_index=k_index
                )
                predictions.append(pred)
            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)
        
        bounds_p = [(1.20, 1.50), (-0.20, 0.20), (1.20, 1.60), (-0.30, 0.30), (-3.0, 3.0), (-3.0, 3.0)]
        result_p = differential_evolution(lambda p: param_obj(p, fold_train), bounds_p, 
                                         maxiter=20, seed=SEED+fold_num, disp=False)
        param_fold_results.append(result_p.x)
        
        # 2. MULTIPLICATIVE METHOD
        def mult_obj(params, df_data):
            m0, m1, m2 = params
            predictions = []
            for _, row in df_data.iterrows():
                base_pred = row['SRKT2_Baseline']
                cct_norm = (row['CCT'] - 600) / 100
                cct_ratio = row['CCT'] / row['Bio-AL']
                correction = 1 + m0 + m1 * cct_norm + m2 * cct_ratio
                predictions.append(base_pred * correction)
            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)
        
        result_m = minimize(lambda p: mult_obj(p, fold_train), [0,0,0], 
                           method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)
        mult_fold_results.append(result_m.x)
        
        # 3. ADDITIVE METHOD
        def add_obj(params, df_data):
            a0, a1, a2, a3 = params
            predictions = []
            for _, row in df_data.iterrows():
                base_pred = row['SRKT2_Baseline']
                cct_norm = (row['CCT'] - 600) / 100
                cct_ratio = row['CCT'] / row['Bio-AL']
                correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']
                predictions.append(base_pred + correction)
            return mean_absolute_error(df_data['PostOP Spherical Equivalent'], predictions)
        
        result_a = minimize(lambda p: add_obj(p, fold_train), [0,0,0,0],
                           method='L-BFGS-B', bounds=[(-2,2),(-2,2),(-2,2),(-0.1,0.1)])
        add_fold_results.append(result_a.x)
        
        # VALIDATE COMBINED on fold validation set
        nc_b, nc_c, k_b, k_c, acd_b, acd_c = result_p.x
        m0, m1, m2 = result_m.x
        a0, a1, a2, a3 = result_a.x
        
        combined_preds = []
        for _, row in fold_val.iterrows():
            cct_norm = (row['CCT'] - 600) / 100
            cct_ratio = row['CCT'] / row['Bio-AL']
            
            # Modified SRK/T2
            nc = nc_b + nc_c * cct_norm
            k_index = k_b + k_c * cct_norm
            acd_offset = acd_b + acd_c * cct_norm
            modified = calculate_SRKT2(
                AL=row['Bio-AL'], K_avg=row['K_avg'],
                IOL_power=row['IOL Power'],
                A_constant=row['A-Constant'] + acd_offset,
                nc=nc, k_index=k_index
            )
            
            # Apply multiplicative
            mult_factor = 1 + m0 + m1 * cct_norm + m2 * cct_ratio
            after_mult = modified * mult_factor
            
            # Apply additive
            add_correction = a0 + a1 * cct_norm + a2 * cct_ratio + a3 * row['K_avg']
            final = after_mult + add_correction
            
            combined_preds.append(final)
        
        fold_mae = mean_absolute_error(fold_val['PostOP Spherical Equivalent'], combined_preds)
        combined_fold_maes.append(fold_mae)
        print(f"MAE={fold_mae:.4f} ", end="")
    
    print()  # New line after folds
    
    # Average parameters across folds
    avg_param = np.mean(param_fold_results, axis=0)
    avg_mult = np.mean(mult_fold_results, axis=0)
    avg_add = np.mean(add_fold_results, axis=0)
    avg_combined_mae = np.mean(combined_fold_maes)
    std_combined_mae = np.std(combined_fold_maes)
    
    print(f"  CV MAE: {avg_combined_mae:.4f} Â± {std_combined_mae:.4f} D")
    
    # FINAL RETRAINING on full training set
    print("  Final optimization on full training set...")
    
    result_p_final = differential_evolution(lambda p: param_obj(p, X_train_comb), bounds_p, 
                                           maxiter=50, seed=SEED, disp=False)
    nc_base_c, nc_cct_c, k_base_c, k_cct_c, acd_base_c, acd_cct_c = result_p_final.x
    
    result_m_final = minimize(lambda p: mult_obj(p, X_train_comb), [0,0,0], 
                             method='L-BFGS-B', bounds=[(-0.5,0.5)]*3)
    m0_c, m1_c, m2_c = result_m_final.x
    
    result_a_final = minimize(lambda p: add_obj(p, X_train_comb), [0,0,0,0],
                             method='L-BFGS-B', bounds=[(-2,2),(-2,2),(-2,2),(-0.1,0.1)])
    a0_c, a1_c, a2_c, a3_c = result_a_final.x
    
    # EVALUATE ON TRAINING SET (for overfitting check)
    predictions_combined_train = []
    for _, row in X_train_comb.iterrows():
        cct_norm = (row['CCT'] - 600) / 100
        cct_ratio = row['CCT'] / row['Bio-AL']
        k_avg = row['K_avg']
        
        # Modified SRK/T2
        nc = nc_base_c + nc_cct_c * cct_norm
        k_index = k_base_c + k_cct_c * cct_norm
        acd_offset = acd_base_c + acd_cct_c * cct_norm
        
        modified_srkt2 = calculate_SRKT2(
            AL=row['Bio-AL'], K_avg=k_avg,
            IOL_power=row['IOL Power'],
            A_constant=row['A-Constant'] + acd_offset,
            nc=nc, k_index=k_index
        )
        
        # Apply multiplicative
        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio
        after_mult = modified_srkt2 * mult_factor
        
        # Apply additive
        add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg
        final_combined = after_mult + add_correction
        
        predictions_combined_train.append(final_combined)
    
    mae_train = mean_absolute_error(X_train_comb['PostOP Spherical Equivalent'], predictions_combined_train)
    
    # TEST ON HOLDOUT
    predictions_combined_test = []
    predictions_mult_only = []
    
    for _, row in X_test_comb.iterrows():
        cct_norm = (row['CCT'] - 600) / 100
        cct_ratio = row['CCT'] / row['Bio-AL']
        k_avg = row['K_avg']
        
        # Modified SRK/T2
        nc = nc_base_c + nc_cct_c * cct_norm
        k_index = k_base_c + k_cct_c * cct_norm
        acd_offset = acd_base_c + acd_cct_c * cct_norm
        
        modified_srkt2 = calculate_SRKT2(
            AL=row['Bio-AL'], K_avg=k_avg,
            IOL_power=row['IOL Power'],
            A_constant=row['A-Constant'] + acd_offset,
            nc=nc, k_index=k_index
        )
        
        # Multiplicative only (for comparison)
        mult_factor = 1 + m0_c + m1_c * cct_norm + m2_c * cct_ratio
        mult_only = row['SRKT2_Baseline'] * mult_factor
        predictions_mult_only.append(mult_only)
        
        # Combined: all three
        after_mult = modified_srkt2 * mult_factor
        add_correction = a0_c + a1_c * cct_norm + a2_c * cct_ratio + a3_c * k_avg
        final_combined = after_mult + add_correction
        predictions_combined_test.append(final_combined)
    
    mae_baseline = np.abs(X_test_comb['SRKT2_Baseline'] - X_test_comb['PostOP Spherical Equivalent']).mean()
    mae_optimized = mean_absolute_error(X_test_comb['PostOP Spherical Equivalent'], predictions_combined_test)
    improvement = (mae_baseline - mae_optimized) / mae_baseline * 100
    
    print(f"  Train MAE: {mae_train:.4f}, Test MAE: {mae_optimized:.4f}")
    print(f"  Test: Baseline={mae_baseline:.4f}, Combined={mae_optimized:.4f}")
    print(f"  Improvement: {improvement:.1f}%")
    
    # Check for overfitting
    overfit_ratio = (mae_optimized - mae_train) / mae_train * 100
    if overfit_ratio > 20:
        print(f"  âš ï¸ Overfitting detected: Test {overfit_ratio:.1f}% worse than train")
    elif overfit_ratio > 10:
        print(f"  âš ï¸ Mild overfitting: Test {overfit_ratio:.1f}% worse than train")
    else:
        print(f"  âœ… Good generalization: Test only {overfit_ratio:.1f}% worse than train")
    
    # Store results
    seed_results_combined.append({
        'param': [nc_base_c, nc_cct_c, k_base_c, k_cct_c, acd_base_c, acd_cct_c],
        'mult': [m0_c, m1_c, m2_c],
        'add': [a0_c, a1_c, a2_c, a3_c]
    })
    seed_test_maes_combined.append(mae_optimized)
    seed_train_maes_combined.append(mae_train)
    seed_baseline_maes_combined.append(mae_baseline)
    seed_improvements_combined.append(improvement)
    seed_overfit_ratios_combined.append(overfit_ratio)

# MULTI-SEED SUMMARY
print("\n" + "="*80)
print("COMBINED APPROACH - MULTI-SEED SUMMARY")
print("="*80)

print("\nğŸ“Š TEST PERFORMANCE ACROSS SEEDS:")
print("-" * 50)
for i, seed in enumerate(SEEDS):
    print(f"  Seed {seed:3}: MAE={seed_test_maes_combined[i]:.4f} D, Improvement={seed_improvements_combined[i]:.1f}%")

print("\nğŸ“ˆ STATISTICAL SUMMARY:")
print("-" * 50)
print(f"  Baseline MAE:      {np.mean(seed_baseline_maes_combined):.4f} Â± {np.std(seed_baseline_maes_combined):.4f} D")
print(f"  Train MAE:         {np.mean(seed_train_maes_combined):.4f} Â± {np.std(seed_train_maes_combined):.4f} D")
print(f"  Test MAE:          {np.mean(seed_test_maes_combined):.4f} Â± {np.std(seed_test_maes_combined):.4f} D")
print(f"  Mean Improvement:  {np.mean(seed_improvements_combined):.1f} Â± {np.std(seed_improvements_combined):.1f}%")
print(f"  Best seed:         {SEEDS[np.argmin(seed_test_maes_combined)]} (MAE={min(seed_test_maes_combined):.4f})")
print(f"  Worst seed:        {SEEDS[np.argmax(seed_test_maes_combined)]} (MAE={max(seed_test_maes_combined):.4f})")

# OVERFITTING ANALYSIS
print("\nğŸ” OVERFITTING ANALYSIS:")
print("-" * 50)
print(f"  Mean overfit ratio: {np.mean(seed_overfit_ratios_combined):.1f}%")
print(f"  (Test MAE is {np.mean(seed_overfit_ratios_combined):.1f}% worse than Train MAE on average)")

if np.mean(seed_overfit_ratios_combined) < 10:
    print("  âœ… Excellent generalization - minimal overfitting")
elif np.mean(seed_overfit_ratios_combined) < 20:
    print("  âœ… Good generalization - acceptable overfitting")
else:
    print("  âš ï¸ Significant overfitting - consider regularization")
    print("  Note: Combined approach has more parameters, higher overfitting risk")

# Clinical accuracy
all_errors = []
for i in range(len(SEEDS)):
    errors = np.abs(np.array(seed_test_maes_combined[i]))
    all_errors.append(errors)

mean_mae = np.mean(seed_test_maes_combined)
within_050 = sum(1 for mae in seed_test_maes_combined if mae <= 0.50) / len(seed_test_maes_combined) * 100
within_100 = sum(1 for mae in seed_test_maes_combined if mae <= 1.00) / len(seed_test_maes_combined) * 100

print(f"\nğŸ“ˆ CLINICAL PERFORMANCE (Combined):")
print("-" * 50)
print(f"  Seeds with MAE < 0.50 D: {within_050:.0f}%")
print(f"  Seeds with MAE < 1.00 D: {within_100:.0f}%")

# Store in global results dictionary
multi_seed_results['combined'] = {
    'test_maes': seed_test_maes_combined,
    'train_maes': seed_train_maes_combined,
    'baseline_maes': seed_baseline_maes_combined,
    'improvements': seed_improvements_combined,
    'overfit_ratios': seed_overfit_ratios_combined,
    'mean_mae': np.mean(seed_test_maes_combined),
    'std_mae': np.std(seed_test_maes_combined),
    'mean_improvement': np.mean(seed_improvements_combined)
}

# Extract consensus parameters
all_param_params = [r['param'] for r in seed_results_combined]
all_mult_params = [r['mult'] for r in seed_results_combined]
all_add_params = [r['add'] for r in seed_results_combined]

avg_param_params = np.mean(all_param_params, axis=0)
avg_mult_params = np.mean(all_mult_params, axis=0)
avg_add_params = np.mean(all_add_params, axis=0)

print("\nâœ… CONSENSUS PARAMETERS (averaged across seeds):")
print("-" * 70)
print("1. Modified SRK/T2:")
print(f"   nc = {avg_param_params[0]:.4f} + {avg_param_params[1]:.4f} Ã— CCT_norm")
print(f"   k_index = {avg_param_params[2]:.4f} + {avg_param_params[3]:.4f} Ã— CCT_norm")
print(f"   ACD_offset = {avg_param_params[4]:.4f} + {avg_param_params[5]:.4f} Ã— CCT_norm")
print("2. Multiplicative:")
print(f"   Factor = 1 + {avg_mult_params[0]:.4f} + {avg_mult_params[1]:.4f} Ã— CCT_norm + {avg_mult_params[2]:.4f} Ã— CCT_ratio")
print("3. Additive:")
print(f"   Term = {avg_add_params[0]:.4f} + {avg_add_params[1]:.4f} Ã— CCT_norm + {avg_add_params[2]:.4f} Ã— CCT_ratio + {avg_add_params[3]:.4f} Ã— K_avg")

print("\nğŸ’¡ ROBUSTNESS ANALYSIS:")
print("-" * 50)
mae_cv = np.std(seed_test_maes_combined) / np.mean(seed_test_maes_combined) * 100
if mae_cv < 5:
    print(f"âœ… Excellent stability: CV={mae_cv:.1f}% (very consistent across seeds)")
elif mae_cv < 10:
    print(f"âœ… Good stability: CV={mae_cv:.1f}% (consistent across seeds)")
else:
    print(f"âš ï¸ Moderate stability: CV={mae_cv:.1f}% (some variation across seeds)")

print(f"\nğŸ“Š Range of results: {min(seed_test_maes_combined):.4f} - {max(seed_test_maes_combined):.4f} D")
print(f"   This {max(seed_test_maes_combined)-min(seed_test_maes_combined):.4f} D range shows the impact of data split")

print("\nğŸ’¡ COMBINED APPROACH INSIGHTS:")
print("-" * 50)
if np.mean(seed_overfit_ratios_combined) > np.mean(seed_improvements_combined) * 0.5:
    print("âš ï¸ High complexity may be causing overfitting")
    print("   Consider using simpler approach (multiplicative only)")
else:
    print("âœ… Combined approach balances complexity and performance")
    print("   The three corrections work synergistically")